{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashrafalaghbari/oil-production-forecasting/blob/main/optimal_TCN_single_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "x1LrkoSv4cc7",
        "outputId": "675bbb61-d0d0-4f1e-cd6b-3dbd2fbbe2af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(60000)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autosaving every 60 seconds\n"
          ]
        }
      ],
      "source": [
        "%autosave 60"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tcn --no-dependencies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8tyr5_aWpmN",
        "outputId": "fd7a4362-8773-4f2b-fcbd-55917a00d9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tcn\n",
            "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: keras-tcn\n",
            "Successfully installed keras-tcn-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxzUA1Pg4iEU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation, Dropout\n",
        "from keras.layers import LSTM\n",
        "from tcn import TCN\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ZrWV9NpfPM"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu7vsRejtR_4"
      },
      "outputs": [],
      "source": [
        "# # check if GPU is utilized \n",
        "# device_name = tf.config.experimental.list_physical_devices()[-1][-1]\n",
        "# if device_name != 'GPU':\n",
        "#     raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVMp6qZiT9gO"
      },
      "outputs": [],
      "source": [
        "# Select the best features\n",
        "def select_features(df, target, correlation_type, threshold):\n",
        "    if (threshold < -1 ) | (threshold > 1 ) :\n",
        "            raise SystemError('correlation threshold is out of bounds')\n",
        "    features = df.corr(correlation_type).loc[target].drop(target)\n",
        "    best_features = features.where(abs(features) > threshold).dropna()\n",
        "    df = pd.concat([df[target], df[best_features.index]], axis=1)\n",
        "    return df\n",
        "\n",
        "# convert series to supervised learning\n",
        "def series_to_supervised(data, columns, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('%s(t-%d)' % (columns[j], i)) for j in range(n_vars)]\n",
        "        # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('%s(t)' % (columns[j])) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('%s(t+%d)' % (columns[j], i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "# scale train and test data to new feature range[0, 1]\n",
        "def scale(train, test):\n",
        "\t# fit scaler\n",
        "\tscaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\tscaler = scaler.fit(train)\n",
        "\t# transform train\n",
        "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
        "\ttrain_scaled = scaler.transform(train)\n",
        "\t# transform test\n",
        "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
        "\ttest_scaled = scaler.transform(test)\n",
        "\treturn scaler, train_scaled, test_scaled\n",
        "\n",
        "\n",
        "# inverse differencing\n",
        "def inverse_difference(history, interval=1):\n",
        "\treturn history[-len(test_scaled)-interval:-interval]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX-FCHP3Hy7E"
      },
      "outputs": [],
      "source": [
        "#Evaluation metrics\n",
        "# compute RMSPE\n",
        "def RMSPE(x,y):\n",
        "\tresult=0\n",
        "\tfor i in range(len(x)):\n",
        "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
        "\tresult /= len(x)\n",
        "\tresult = sqrt(result)\n",
        "\tresult *= 100\n",
        "\treturn result\n",
        "\n",
        "# compute MAPE\n",
        "def MAPE(x,y):\n",
        "\tresult=0\n",
        "\tfor i in range(len(x)):\n",
        "\t\tresult += abs((x[i]-y[i])/x[i])\n",
        "\tresult /= len(x)\n",
        "\tresult *= 100\n",
        "\treturn result\n",
        "\n",
        "# compute wMAPE weighted absolute percentage error\n",
        "def wMAPE(actual, predicted): \n",
        "    result_nom = 0\n",
        "    result_deno = 0\n",
        "    for i in range(len(actual)):\n",
        "        result_nom +=  abs(actual[i] - predicted[i])\n",
        "        result_deno +=  abs(actual[i]) \n",
        "    result = result_nom/result_deno\n",
        "    return result *100\n",
        "\n",
        "def SMAPE(actual, predicted): #Symmetric (adjusted) MEAN ABSOLUTE PERCENTAGE ERROR (SMAPE)\n",
        "    result = 0\n",
        "    for i in range(len(actual)):\n",
        "        result += abs(actual[i] - predicted[i])/(abs(actual[i]) + abs(predicted[i]))\n",
        "    result = 2 * result/ len(actual) \n",
        "    return result * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu0tOwl84Xln"
      },
      "outputs": [],
      "source": [
        "#load dataset\n",
        "series = pd.read_csv('/content/drive/MyDrive/volve_production_data/model.csv', \n",
        "                     parse_dates=[\"DATEPRD\"], index_col=\"DATEPRD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRJsglCpLKHo"
      },
      "outputs": [],
      "source": [
        "# select feature based on correlation\n",
        "series = select_features(series, \"BORE_OIL_VOL\", \"spearman\", 0)\n",
        "# select features manually\n",
        "series =series[[\"ON_STREAM_HRS\",'BORE_GAS_VOL', 'BORE_WAT_VOL',\n",
        "                'AVG_CHOKE_SIZE_P','F_4_BORE_WI_VOL','F_5_BORE_WI_VOL',\n",
        "                 \"BORE_OIL_VOL\"]] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd4HfCJBzSDp"
      },
      "outputs": [],
      "source": [
        "series[\"next_choke_size\"] = series['AVG_CHOKE_SIZE_P'].shift(-1)\n",
        "series[\"next_on_stream\"] = series['ON_STREAM_HRS'].shift(-1)\n",
        "series[\"next_BORE_GAS_VOL\"] = series['BORE_GAS_VOL'].shift(-1)\n",
        "series.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEvzyzwpkkLr"
      },
      "outputs": [],
      "source": [
        "column_to_move = series.pop(\"BORE_OIL_VOL\")\n",
        "\n",
        "# insert column with insert(location, column_name, column_value)\n",
        "\n",
        "series.insert(len(series.columns), \"BORE_OIL_VOL\", column_to_move)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLBz9GX14-Wi"
      },
      "outputs": [],
      "source": [
        "# # Data visulaization and disribution plots for well F-14 after including the injectors\n",
        "data = series.columns\n",
        "\n",
        "# Creating figure with two rows and one column\n",
        "fig, axs = plt.subplots(nrows=len(data), figsize=(17, 28))\n",
        "\n",
        "axs = axs.ravel()\n",
        "\n",
        "for id, column in enumerate(data):\n",
        "\n",
        "    axs[id].plot(series[column])\n",
        "    axs[id].grid(True)\n",
        "    axs[id].legend([column], loc='lower left', fontsize=9, handlelength=0, handletextpad=0, frameon=False)\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq2GI1a0NKYT"
      },
      "outputs": [],
      "source": [
        "# # # convert series to stationary \n",
        "series_diff = series.copy()\n",
        "diff_order = 1\n",
        "series_diff['BORE_OIL_VOL'] = series_diff['BORE_OIL_VOL'].diff(diff_order)\n",
        "# # convert the stationary series to supervise learning\n",
        "timesteps = 1 # lag features\n",
        "steps_ahead = 1\n",
        "series_supervised = series_to_supervised(series_diff, series_diff.columns, n_in=timesteps, n_out=steps_ahead, dropnan=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "series_supervised"
      ],
      "metadata": {
        "id": "Lz69JoFCRtXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pKtD-x--GNV"
      },
      "outputs": [],
      "source": [
        "series_supervised.drop(['ON_STREAM_HRS(t)', 'BORE_GAS_VOL(t)',\n",
        "       'BORE_WAT_VOL(t)', 'AVG_CHOKE_SIZE_P(t)', \n",
        "       'next_choke_size(t)', 'next_on_stream(t)',\n",
        "        'next_BORE_GAS_VOL(t)', \n",
        "       'F_4_BORE_WI_VOL(t)',\n",
        "       'F_5_BORE_WI_VOL(t)'\n",
        "       ], axis=1, inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmbQinIh7Gnp",
        "outputId": "b5746fcf-ab72-4a66-f740-52e65ae1a0b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1524, 11) (381, 11)\n"
          ]
        }
      ],
      "source": [
        "# # split into train and test sets\n",
        "series_supervised = series_supervised.values\n",
        "train_size = int(series_supervised.shape[0] * 0.8)\n",
        "test_size = series_supervised.shape[0] - train_size\n",
        "train, test = series_supervised[0:train_size], series_supervised[train_size:]\n",
        "print(train.shape, test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvaAPiiS7y4v",
        "outputId": "8db029ce-5f33-4267-a743-d5ead47a3ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1524, 11) (381, 11)\n"
          ]
        }
      ],
      "source": [
        "# transform the scale of the data\n",
        "scaler, train_scaled, test_scaled = scale(train, test)\n",
        "print(train_scaled.shape, test_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjDT7Ud5l522",
        "outputId": "b4ff4aff-8a6c-4d48-fcd8-5d507c774c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1524, 1, 10) (1524, 1) (381, 1, 10) (381, 1)\n"
          ]
        }
      ],
      "source": [
        "# # reshape input to be 3D [samples, timesteps, features]\n",
        "n_features = len(series.columns) \n",
        "\n",
        "train_X, train_y = train_scaled[:, 0:-steps_ahead], train_scaled[:, -steps_ahead:]\n",
        "train_X = train_X.reshape(train_X.shape[0], timesteps, n_features)\n",
        "test_X, test_y = test_scaled[:, 0:-steps_ahead], test_scaled[:, -steps_ahead:]\n",
        "test_X = test_X.reshape(test_X.shape[0], timesteps, n_features )\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpoQJ7P7lmyb"
      },
      "outputs": [],
      "source": [
        "# Create a new directory in My Drive\n",
        "directory = '/content/drive/My Drive/my_trained_models'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D"
      ],
      "metadata": {
        "id": "aq2_-00BfBO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1EvEswvoTXy5",
        "outputId": "f3f400ff-07cd-4318-88a1-e3f05c6872d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "762/762 - 6s - loss: 0.0263 - val_loss: 4.0579e-04 - 6s/epoch - 8ms/step\n",
            "Epoch 2/700\n",
            "762/762 - 3s - loss: 0.0024 - val_loss: 3.3447e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 3/700\n",
            "762/762 - 3s - loss: 0.0015 - val_loss: 1.9879e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/700\n",
            "762/762 - 3s - loss: 0.0011 - val_loss: 1.6823e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/700\n",
            "762/762 - 3s - loss: 9.0254e-04 - val_loss: 1.6317e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/700\n",
            "762/762 - 3s - loss: 7.6761e-04 - val_loss: 1.5183e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/700\n",
            "762/762 - 3s - loss: 6.6836e-04 - val_loss: 1.3457e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/700\n",
            "762/762 - 3s - loss: 5.9526e-04 - val_loss: 1.1916e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 9/700\n",
            "762/762 - 3s - loss: 5.4108e-04 - val_loss: 1.0781e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 10/700\n",
            "762/762 - 3s - loss: 4.9636e-04 - val_loss: 9.9511e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 11/700\n",
            "762/762 - 3s - loss: 4.5645e-04 - val_loss: 9.3483e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 12/700\n",
            "762/762 - 3s - loss: 4.1977e-04 - val_loss: 8.9269e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 13/700\n",
            "762/762 - 3s - loss: 3.8540e-04 - val_loss: 8.6321e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 14/700\n",
            "762/762 - 5s - loss: 3.5299e-04 - val_loss: 8.3923e-05 - 5s/epoch - 6ms/step\n",
            "Epoch 15/700\n",
            "762/762 - 3s - loss: 3.2267e-04 - val_loss: 8.1295e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 16/700\n",
            "762/762 - 3s - loss: 2.9491e-04 - val_loss: 7.7846e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 17/700\n",
            "762/762 - 3s - loss: 2.7035e-04 - val_loss: 7.3469e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 18/700\n",
            "762/762 - 3s - loss: 2.4938e-04 - val_loss: 6.8431e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 19/700\n",
            "762/762 - 3s - loss: 2.3196e-04 - val_loss: 6.3074e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 20/700\n",
            "762/762 - 3s - loss: 2.1773e-04 - val_loss: 5.7723e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 21/700\n",
            "762/762 - 3s - loss: 2.0625e-04 - val_loss: 5.2707e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 22/700\n",
            "762/762 - 3s - loss: 1.9704e-04 - val_loss: 4.8283e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 23/700\n",
            "762/762 - 3s - loss: 1.8967e-04 - val_loss: 4.4574e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 24/700\n",
            "762/762 - 3s - loss: 1.8373e-04 - val_loss: 4.1567e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 25/700\n",
            "762/762 - 3s - loss: 1.7890e-04 - val_loss: 3.9176e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 26/700\n",
            "762/762 - 3s - loss: 1.7490e-04 - val_loss: 3.7300e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 27/700\n",
            "762/762 - 3s - loss: 1.7153e-04 - val_loss: 3.5836e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 28/700\n",
            "762/762 - 3s - loss: 1.6864e-04 - val_loss: 3.4701e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 29/700\n",
            "762/762 - 3s - loss: 1.6611e-04 - val_loss: 3.3822e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 30/700\n",
            "762/762 - 3s - loss: 1.6387e-04 - val_loss: 3.3141e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 31/700\n",
            "762/762 - 3s - loss: 1.6185e-04 - val_loss: 3.2616e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 32/700\n",
            "762/762 - 3s - loss: 1.6000e-04 - val_loss: 3.2209e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 33/700\n",
            "762/762 - 3s - loss: 1.5829e-04 - val_loss: 3.1897e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 34/700\n",
            "762/762 - 3s - loss: 1.5669e-04 - val_loss: 3.1659e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 35/700\n",
            "762/762 - 3s - loss: 1.5518e-04 - val_loss: 3.1477e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 36/700\n",
            "762/762 - 3s - loss: 1.5376e-04 - val_loss: 3.1337e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 37/700\n",
            "762/762 - 3s - loss: 1.5240e-04 - val_loss: 3.1233e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 38/700\n",
            "762/762 - 3s - loss: 1.5110e-04 - val_loss: 3.1153e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 39/700\n",
            "762/762 - 3s - loss: 1.4986e-04 - val_loss: 3.1093e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 40/700\n",
            "762/762 - 5s - loss: 1.4867e-04 - val_loss: 3.1047e-05 - 5s/epoch - 6ms/step\n",
            "Epoch 41/700\n",
            "762/762 - 4s - loss: 1.4752e-04 - val_loss: 3.1011e-05 - 4s/epoch - 6ms/step\n",
            "Epoch 42/700\n",
            "762/762 - 3s - loss: 1.4642e-04 - val_loss: 3.0984e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 43/700\n",
            "762/762 - 4s - loss: 1.4537e-04 - val_loss: 3.0968e-05 - 4s/epoch - 5ms/step\n",
            "Epoch 44/700\n",
            "762/762 - 3s - loss: 1.4436e-04 - val_loss: 3.0955e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 45/700\n",
            "762/762 - 3s - loss: 1.4339e-04 - val_loss: 3.0941e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 46/700\n",
            "762/762 - 3s - loss: 1.4245e-04 - val_loss: 3.0925e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 47/700\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-4cb43da14e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}/{n_epochs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Train the model for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     history = model.fit(train_X, train_y, callbacks=[early_stopping, mcp_save],\n\u001b[0m\u001b[1;32m     53\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1374\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallbackList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         callbacks = callbacks_module.CallbackList(\n\u001b[0m\u001b[1;32m   1377\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0madd_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, callbacks, add_history, add_progbar, model, **params)\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;34m\"\"\"Container abstracting a list of callbacks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m   def __init__(self,\n\u001b[0m\u001b[1;32m    187\u001b[0m                \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                \u001b[0madd_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "np.random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                        inter_op_parallelism_threads=1)\n",
        "from keras import backend as K\n",
        "tf.random.set_seed(1234)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), \n",
        "                            config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "# fit the model\n",
        "neurons= 22\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters = 46, kernel_size=3, dilation_rate=1, padding='causal',\n",
        "                 input_shape=(timesteps, n_features)))\n",
        "model.add(Conv1D(filters =46, kernel_size=1, dilation_rate=1, padding='causal'))\n",
        "model.add(Conv1D(filters = 25, kernel_size=1, dilation_rate=1, padding='causal'))\n",
        "model.add(Conv1D(filters = 10, kernel_size=1, dilation_rate=1, padding='causal'))\n",
        "model.add(Conv1D(filters = 46, kernel_size=2, dilation_rate=1, padding='causal'))\n",
        "model.add(Conv1D(filters = 125, kernel_size=12, dilation_rate=1, padding='causal'))\n",
        "\n",
        "\n",
        "model.add(LSTM(neurons, activation=\"tanh\",\n",
        "               input_shape=(timesteps, n_features)))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(steps_ahead)) # output layer\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=2,\n",
        "                                restore_best_weights=True, mode='min')\n",
        "# save the best weights if training is interrupted\n",
        "mcp_save = ModelCheckpoint(os.path.join(directory, 'mdl_wts.hdf5'),\n",
        "                            save_best_only=True,\n",
        "                            monitor='val_loss', mode='min') \n",
        "\n",
        "\n",
        "# Set the initial and total number of epochs\n",
        "initial_epoch = 1\n",
        "n_epochs = 700\n",
        "loss_tracking = list()\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(initial_epoch , n_epochs+1):\n",
        "    print(f'Epoch {epoch}/{n_epochs}')\n",
        "    # Train the model for one epoch\n",
        "    history = model.fit(train_X, train_y, callbacks=[early_stopping, mcp_save],\n",
        "                    epochs=1, batch_size=2, validation_data=(test_X, test_y),\n",
        "                     verbose=2,\n",
        "                     shuffle=False)\n",
        "    # to find for which epoch each loss belongs\n",
        "    validation_loss= model.evaluate(test_X, test_y, verbose=0)\n",
        "    loss_tracking.append(validation_loss)\n",
        "    # Save the model every 10 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        # Save the model  in HDF5 foramt with a filename that includes the epoch number\n",
        "        #model.save(f'model_{epoch}Eps.h5')\n",
        "        model.save(os.path.join(directory, f'model_{epoch}Eps.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "np.random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                        inter_op_parallelism_threads=1)\n",
        "from keras import backend as K\n",
        "tf.random.set_seed(1234)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), \n",
        "                            config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "# fit the model\n",
        "neurons= 22\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters = 46, kernel_size=3, dilation_rate=1, padding='causal',\n",
        "                 input_shape=(timesteps, n_features)))\n",
        "model.add(Conv1D(filters =46, kernel_size=1, dilation_rate=1, padding='causal'))\n",
        "model.add(Conv1D(filters = 25, kernel_size=1, dilation_rate=1, padding='causal'))\n",
        "model.add(Conv1D(filters = 10, kernel_size=1, dilation_rate=1, padding='causal'))\n",
        "model.add(Conv1D(filters = 46, kernel_size=2, dilation_rate=1, padding='causal'))\n",
        "model.add(Conv1D(filters = 125, kernel_size=12, dilation_rate=1, padding='causal'))\n",
        "\n",
        "\n",
        "model.add(Dense(steps_ahead)) # output layer\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=2,\n",
        "                                restore_best_weights=True, mode='min')\n",
        "# save the best weights if training is interrupted\n",
        "mcp_save = ModelCheckpoint(os.path.join(directory, 'mdl_wts.hdf5'),\n",
        "                            save_best_only=True,\n",
        "                            monitor='val_loss', mode='min') \n",
        "\n",
        "\n",
        "# Set the initial and total number of epochs\n",
        "initial_epoch = 1\n",
        "n_epochs = 700\n",
        "loss_tracking = list()\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(initial_epoch , n_epochs+1):\n",
        "    print(f'Epoch {epoch}/{n_epochs}')\n",
        "    # Train the model for one epoch\n",
        "    history = model.fit(train_X, train_y, callbacks=[early_stopping, mcp_save],\n",
        "                    epochs=1, batch_size=2, validation_data=(test_X, test_y),\n",
        "                     verbose=2,\n",
        "                     shuffle=False)\n",
        "    # to find for which epoch each loss belongs\n",
        "    validation_loss= model.evaluate(test_X, test_y, verbose=0)\n",
        "    loss_tracking.append(validation_loss)\n",
        "    # Save the model every 10 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        # Save the model  in HDF5 foramt with a filename that includes the epoch number\n",
        "        #model.save(f'model_{epoch}Eps.h5')\n",
        "        model.save(os.path.join(directory, f'model_{epoch}Eps.h5'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ7I0rvzsWSO",
        "outputId": "1e8c724d-43ad-41b8-9532-089a9dbb9116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "762/762 - 5s - loss: 0.0146 - val_loss: 3.9217e-04 - 5s/epoch - 7ms/step\n",
            "Epoch 2/700\n",
            "762/762 - 2s - loss: 0.0015 - val_loss: 1.7227e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/700\n",
            "762/762 - 2s - loss: 9.0010e-04 - val_loss: 1.1180e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/700\n",
            "762/762 - 2s - loss: 6.1101e-04 - val_loss: 8.6051e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 5/700\n",
            "762/762 - 2s - loss: 4.8728e-04 - val_loss: 6.9632e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 6/700\n",
            "762/762 - 2s - loss: 4.3528e-04 - val_loss: 6.1371e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 7/700\n",
            "762/762 - 2s - loss: 4.0019e-04 - val_loss: 5.7662e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 8/700\n",
            "762/762 - 2s - loss: 3.6855e-04 - val_loss: 5.5727e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 9/700\n",
            "762/762 - 2s - loss: 3.4005e-04 - val_loss: 5.4363e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 10/700\n",
            "762/762 - 2s - loss: 3.1508e-04 - val_loss: 5.3047e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 11/700\n",
            "762/762 - 2s - loss: 2.9352e-04 - val_loss: 5.1593e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 12/700\n",
            "762/762 - 2s - loss: 2.7508e-04 - val_loss: 4.9900e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 13/700\n",
            "762/762 - 2s - loss: 2.5944e-04 - val_loss: 4.7904e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 14/700\n",
            "762/762 - 2s - loss: 2.4626e-04 - val_loss: 4.5661e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 15/700\n",
            "762/762 - 2s - loss: 2.3522e-04 - val_loss: 4.3320e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 16/700\n",
            "762/762 - 2s - loss: 2.2599e-04 - val_loss: 4.1051e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 17/700\n",
            "762/762 - 2s - loss: 2.1821e-04 - val_loss: 3.9024e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 18/700\n",
            "762/762 - 2s - loss: 2.1163e-04 - val_loss: 3.7370e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 19/700\n",
            "762/762 - 2s - loss: 2.0602e-04 - val_loss: 3.6088e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 20/700\n",
            "762/762 - 2s - loss: 2.0118e-04 - val_loss: 3.5091e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 21/700\n",
            "762/762 - 2s - loss: 1.9691e-04 - val_loss: 3.4280e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 22/700\n",
            "762/762 - 2s - loss: 1.9309e-04 - val_loss: 3.3582e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 23/700\n",
            "762/762 - 2s - loss: 1.8965e-04 - val_loss: 3.2940e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 24/700\n",
            "762/762 - 2s - loss: 1.8650e-04 - val_loss: 3.2308e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 25/700\n",
            "762/762 - 2s - loss: 1.8359e-04 - val_loss: 3.1656e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 26/700\n",
            "762/762 - 2s - loss: 1.8084e-04 - val_loss: 3.0986e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 27/700\n",
            "762/762 - 2s - loss: 1.7823e-04 - val_loss: 3.0310e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 28/700\n",
            "762/762 - 2s - loss: 1.7580e-04 - val_loss: 2.9639e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 29/700\n",
            "762/762 - 3s - loss: 1.7354e-04 - val_loss: 2.8978e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 30/700\n",
            "762/762 - 2s - loss: 1.7145e-04 - val_loss: 2.8337e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 31/700\n",
            "762/762 - 2s - loss: 1.6952e-04 - val_loss: 2.7720e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 32/700\n",
            "762/762 - 2s - loss: 1.6774e-04 - val_loss: 2.7125e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 33/700\n",
            "762/762 - 2s - loss: 1.6611e-04 - val_loss: 2.6558e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 34/700\n",
            "762/762 - 2s - loss: 1.6461e-04 - val_loss: 2.6023e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 35/700\n",
            "762/762 - 3s - loss: 1.6321e-04 - val_loss: 2.5519e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 36/700\n",
            "762/762 - 3s - loss: 1.6191e-04 - val_loss: 2.5046e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 37/700\n",
            "762/762 - 2s - loss: 1.6068e-04 - val_loss: 2.4597e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 38/700\n",
            "762/762 - 2s - loss: 1.5948e-04 - val_loss: 2.4164e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 39/700\n",
            "762/762 - 2s - loss: 1.5829e-04 - val_loss: 2.3748e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 40/700\n",
            "762/762 - 2s - loss: 1.5712e-04 - val_loss: 2.3351e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 41/700\n",
            "762/762 - 2s - loss: 1.5600e-04 - val_loss: 2.2970e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 42/700\n",
            "762/762 - 2s - loss: 1.5492e-04 - val_loss: 2.2602e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 43/700\n",
            "762/762 - 2s - loss: 1.5389e-04 - val_loss: 2.2247e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 44/700\n",
            "762/762 - 2s - loss: 1.5292e-04 - val_loss: 2.1899e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 45/700\n",
            "762/762 - 2s - loss: 1.5200e-04 - val_loss: 2.1555e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 46/700\n",
            "762/762 - 2s - loss: 1.5114e-04 - val_loss: 2.1216e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 47/700\n",
            "762/762 - 2s - loss: 1.5032e-04 - val_loss: 2.0887e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 48/700\n",
            "762/762 - 2s - loss: 1.4956e-04 - val_loss: 2.0576e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 49/700\n",
            "762/762 - 2s - loss: 1.4883e-04 - val_loss: 2.0288e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 50/700\n",
            "762/762 - 2s - loss: 1.4814e-04 - val_loss: 2.0026e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 51/700\n",
            "762/762 - 2s - loss: 1.4749e-04 - val_loss: 1.9792e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 52/700\n",
            "762/762 - 2s - loss: 1.4687e-04 - val_loss: 1.9591e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 53/700\n",
            "762/762 - 2s - loss: 1.4631e-04 - val_loss: 1.9429e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 54/700\n",
            "762/762 - 2s - loss: 1.4582e-04 - val_loss: 1.9309e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 55/700\n",
            "762/762 - 2s - loss: 1.4541e-04 - val_loss: 1.9235e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 56/700\n",
            "762/762 - 2s - loss: 1.4511e-04 - val_loss: 1.9207e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 57/700\n",
            "762/762 - 2s - loss: 1.4492e-04 - val_loss: 1.9217e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 58/700\n",
            "762/762 - 2s - loss: 1.4482e-04 - val_loss: 1.9264e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 59/700\n",
            "762/762 - 2s - loss: 1.4466e-04 - val_loss: 1.9366e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 60/700\n",
            "762/762 - 2s - loss: 1.4421e-04 - val_loss: 1.9481e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 61/700\n",
            "762/762 - 2s - loss: 1.4333e-04 - val_loss: 1.9417e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 62/700\n",
            "762/762 - 3s - loss: 1.4225e-04 - val_loss: 1.9177e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 63/700\n",
            "762/762 - 2s - loss: 1.4120e-04 - val_loss: 1.8931e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 64/700\n",
            "762/762 - 2s - loss: 1.4027e-04 - val_loss: 1.8737e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 65/700\n",
            "762/762 - 2s - loss: 1.3945e-04 - val_loss: 1.8589e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 66/700\n",
            "762/762 - 2s - loss: 1.3874e-04 - val_loss: 1.8480e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 67/700\n",
            "762/762 - 2s - loss: 1.3812e-04 - val_loss: 1.8413e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 68/700\n",
            "762/762 - 2s - loss: 1.3761e-04 - val_loss: 1.8394e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 69/700\n",
            "762/762 - 2s - loss: 1.3721e-04 - val_loss: 1.8430e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 70/700\n",
            "762/762 - 3s - loss: 1.3694e-04 - val_loss: 1.8519e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 71/700\n",
            "762/762 - 3s - loss: 1.3680e-04 - val_loss: 1.8641e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 72/700\n",
            "762/762 - 2s - loss: 1.3679e-04 - val_loss: 1.8747e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 73/700\n",
            "762/762 - 2s - loss: 1.3685e-04 - val_loss: 1.8767e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 74/700\n",
            "762/762 - 2s - loss: 1.3690e-04 - val_loss: 1.8703e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 75/700\n",
            "762/762 - 2s - loss: 1.3682e-04 - val_loss: 1.8624e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 76/700\n",
            "762/762 - 2s - loss: 1.3658e-04 - val_loss: 1.8568e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 77/700\n",
            "762/762 - 2s - loss: 1.3619e-04 - val_loss: 1.8515e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 78/700\n",
            "762/762 - 2s - loss: 1.3572e-04 - val_loss: 1.8446e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 79/700\n",
            "762/762 - 3s - loss: 1.3522e-04 - val_loss: 1.8368e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 80/700\n",
            "762/762 - 2s - loss: 1.3470e-04 - val_loss: 1.8290e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 81/700\n",
            "762/762 - 2s - loss: 1.3419e-04 - val_loss: 1.8225e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 82/700\n",
            "762/762 - 2s - loss: 1.3368e-04 - val_loss: 1.8176e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 83/700\n",
            "762/762 - 2s - loss: 1.3319e-04 - val_loss: 1.8144e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 84/700\n",
            "762/762 - 2s - loss: 1.3272e-04 - val_loss: 1.8136e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 85/700\n",
            "762/762 - 2s - loss: 1.3229e-04 - val_loss: 1.8160e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 86/700\n",
            "762/762 - 2s - loss: 1.3192e-04 - val_loss: 1.8217e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 87/700\n",
            "762/762 - 2s - loss: 1.3166e-04 - val_loss: 1.8301e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 88/700\n",
            "762/762 - 2s - loss: 1.3152e-04 - val_loss: 1.8389e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 89/700\n",
            "762/762 - 2s - loss: 1.3151e-04 - val_loss: 1.8444e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 90/700\n",
            "762/762 - 2s - loss: 1.3159e-04 - val_loss: 1.8417e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 91/700\n",
            "762/762 - 2s - loss: 1.3171e-04 - val_loss: 1.8308e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 92/700\n",
            "762/762 - 2s - loss: 1.3176e-04 - val_loss: 1.8175e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 93/700\n",
            "762/762 - 3s - loss: 1.3170e-04 - val_loss: 1.8074e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 94/700\n",
            "762/762 - 3s - loss: 1.3153e-04 - val_loss: 1.8022e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 95/700\n",
            "762/762 - 2s - loss: 1.3128e-04 - val_loss: 1.8005e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 96/700\n",
            "762/762 - 2s - loss: 1.3101e-04 - val_loss: 1.8014e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 97/700\n",
            "762/762 - 2s - loss: 1.3071e-04 - val_loss: 1.8046e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 98/700\n",
            "762/762 - 2s - loss: 1.3040e-04 - val_loss: 1.8103e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 99/700\n",
            "762/762 - 2s - loss: 1.3009e-04 - val_loss: 1.8188e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 100/700\n",
            "762/762 - 2s - loss: 1.2977e-04 - val_loss: 1.8302e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 101/700\n",
            "762/762 - 2s - loss: 1.2944e-04 - val_loss: 1.8447e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 102/700\n",
            "762/762 - 2s - loss: 1.2909e-04 - val_loss: 1.8623e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 103/700\n",
            "762/762 - 2s - loss: 1.2874e-04 - val_loss: 1.8834e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 104/700\n",
            "762/762 - 2s - loss: 1.2839e-04 - val_loss: 1.9081e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 105/700\n",
            "762/762 - 2s - loss: 1.2808e-04 - val_loss: 1.9360e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 106/700\n",
            "762/762 - 2s - loss: 1.2784e-04 - val_loss: 1.9635e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 107/700\n",
            "762/762 - 2s - loss: 1.2768e-04 - val_loss: 1.9852e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 108/700\n",
            "762/762 - 2s - loss: 1.2761e-04 - val_loss: 1.9961e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 109/700\n",
            "762/762 - 2s - loss: 1.2760e-04 - val_loss: 1.9932e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 110/700\n",
            "762/762 - 2s - loss: 1.2760e-04 - val_loss: 1.9760e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 111/700\n",
            "762/762 - 2s - loss: 1.2756e-04 - val_loss: 1.9459e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 112/700\n",
            "762/762 - 2s - loss: 1.2748e-04 - val_loss: 1.9072e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 113/700\n",
            "762/762 - 2s - loss: 1.2736e-04 - val_loss: 1.8666e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 114/700\n",
            "762/762 - 2s - loss: 1.2720e-04 - val_loss: 1.8288e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 115/700\n",
            "762/762 - 3s - loss: 1.2699e-04 - val_loss: 1.7947e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 116/700\n",
            "762/762 - 2s - loss: 1.2675e-04 - val_loss: 1.7640e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 117/700\n",
            "762/762 - 2s - loss: 1.2646e-04 - val_loss: 1.7373e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 118/700\n",
            "762/762 - 2s - loss: 1.2613e-04 - val_loss: 1.7158e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 119/700\n",
            "762/762 - 2s - loss: 1.2580e-04 - val_loss: 1.6998e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 120/700\n",
            "762/762 - 2s - loss: 1.2548e-04 - val_loss: 1.6888e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 121/700\n",
            "762/762 - 2s - loss: 1.2521e-04 - val_loss: 1.6808e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 122/700\n",
            "762/762 - 2s - loss: 1.2502e-04 - val_loss: 1.6735e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 123/700\n",
            "762/762 - 2s - loss: 1.2491e-04 - val_loss: 1.6649e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 124/700\n",
            "762/762 - 2s - loss: 1.2488e-04 - val_loss: 1.6551e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 125/700\n",
            "762/762 - 2s - loss: 1.2490e-04 - val_loss: 1.6460e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 126/700\n",
            "762/762 - 2s - loss: 1.2494e-04 - val_loss: 1.6397e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 127/700\n",
            "762/762 - 2s - loss: 1.2497e-04 - val_loss: 1.6374e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 128/700\n",
            "762/762 - 2s - loss: 1.2498e-04 - val_loss: 1.6391e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 129/700\n",
            "762/762 - 3s - loss: 1.2498e-04 - val_loss: 1.6449e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 130/700\n",
            "762/762 - 2s - loss: 1.2496e-04 - val_loss: 1.6551e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 131/700\n",
            "762/762 - 2s - loss: 1.2490e-04 - val_loss: 1.6698e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 132/700\n",
            "762/762 - 2s - loss: 1.2481e-04 - val_loss: 1.6888e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 133/700\n",
            "762/762 - 2s - loss: 1.2468e-04 - val_loss: 1.7112e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 134/700\n",
            "762/762 - 2s - loss: 1.2453e-04 - val_loss: 1.7352e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 135/700\n",
            "762/762 - 2s - loss: 1.2441e-04 - val_loss: 1.7582e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 136/700\n",
            "762/762 - 2s - loss: 1.2434e-04 - val_loss: 1.7771e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 137/700\n",
            "762/762 - 2s - loss: 1.2431e-04 - val_loss: 1.7905e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 138/700\n",
            "762/762 - 2s - loss: 1.2431e-04 - val_loss: 1.7976e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 139/700\n",
            "762/762 - 2s - loss: 1.2432e-04 - val_loss: 1.7987e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 140/700\n",
            "762/762 - 2s - loss: 1.2430e-04 - val_loss: 1.7944e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 141/700\n",
            "762/762 - 2s - loss: 1.2426e-04 - val_loss: 1.7857e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 142/700\n",
            "762/762 - 2s - loss: 1.2423e-04 - val_loss: 1.7759e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 143/700\n",
            "762/762 - 2s - loss: 1.2417e-04 - val_loss: 1.7686e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 144/700\n",
            "762/762 - 2s - loss: 1.2407e-04 - val_loss: 1.7663e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 145/700\n",
            "762/762 - 2s - loss: 1.2390e-04 - val_loss: 1.7709e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 146/700\n",
            "762/762 - 2s - loss: 1.2365e-04 - val_loss: 1.7836e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 147/700\n",
            "762/762 - 2s - loss: 1.2331e-04 - val_loss: 1.8060e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 148/700\n",
            "762/762 - 2s - loss: 1.2290e-04 - val_loss: 1.8392e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 149/700\n",
            "762/762 - 2s - loss: 1.2245e-04 - val_loss: 1.8827e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 150/700\n",
            "762/762 - 2s - loss: 1.2206e-04 - val_loss: 1.9311e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 151/700\n",
            "762/762 - 2s - loss: 1.2177e-04 - val_loss: 1.9747e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 152/700\n",
            "762/762 - 2s - loss: 1.2163e-04 - val_loss: 1.9972e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 153/700\n",
            "762/762 - 2s - loss: 1.2161e-04 - val_loss: 1.9891e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 154/700\n",
            "762/762 - 2s - loss: 1.2165e-04 - val_loss: 1.9547e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 155/700\n",
            "762/762 - 2s - loss: 1.2163e-04 - val_loss: 1.9080e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 156/700\n",
            "762/762 - 3s - loss: 1.2155e-04 - val_loss: 1.8655e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 157/700\n",
            "762/762 - 2s - loss: 1.2145e-04 - val_loss: 1.8365e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 158/700\n",
            "762/762 - 2s - loss: 1.2132e-04 - val_loss: 1.8179e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 159/700\n",
            "762/762 - 2s - loss: 1.2119e-04 - val_loss: 1.8017e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 160/700\n",
            "762/762 - 2s - loss: 1.2106e-04 - val_loss: 1.7826e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 161/700\n",
            "762/762 - 2s - loss: 1.2094e-04 - val_loss: 1.7597e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 162/700\n",
            "762/762 - 2s - loss: 1.2082e-04 - val_loss: 1.7352e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 163/700\n",
            "762/762 - 2s - loss: 1.2071e-04 - val_loss: 1.7094e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 164/700\n",
            "762/762 - 2s - loss: 1.2062e-04 - val_loss: 1.6879e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 165/700\n",
            "762/762 - 2s - loss: 1.2053e-04 - val_loss: 1.6713e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 166/700\n",
            "762/762 - 2s - loss: 1.2045e-04 - val_loss: 1.6567e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 167/700\n",
            "762/762 - 2s - loss: 1.2036e-04 - val_loss: 1.6379e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 168/700\n",
            "762/762 - 3s - loss: 1.2026e-04 - val_loss: 1.6210e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 169/700\n",
            "762/762 - 2s - loss: 1.2016e-04 - val_loss: 1.6136e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 170/700\n",
            "762/762 - 2s - loss: 1.2010e-04 - val_loss: 1.6139e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 171/700\n",
            "762/762 - 2s - loss: 1.2010e-04 - val_loss: 1.6146e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 172/700\n",
            "762/762 - 2s - loss: 1.2011e-04 - val_loss: 1.6120e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 173/700\n",
            "762/762 - 2s - loss: 1.2013e-04 - val_loss: 1.6075e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 174/700\n",
            "762/762 - 2s - loss: 1.2016e-04 - val_loss: 1.6009e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 175/700\n",
            "762/762 - 2s - loss: 1.2018e-04 - val_loss: 1.5948e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 176/700\n",
            "762/762 - 2s - loss: 1.2017e-04 - val_loss: 1.5925e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 177/700\n",
            "762/762 - 2s - loss: 1.2015e-04 - val_loss: 1.5938e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 178/700\n",
            "762/762 - 2s - loss: 1.2013e-04 - val_loss: 1.5964e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 179/700\n",
            "762/762 - 2s - loss: 1.2012e-04 - val_loss: 1.5996e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 180/700\n",
            "762/762 - 2s - loss: 1.2006e-04 - val_loss: 1.6022e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 181/700\n",
            "762/762 - 2s - loss: 1.2000e-04 - val_loss: 1.6027e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 182/700\n",
            "762/762 - 2s - loss: 1.1995e-04 - val_loss: 1.5988e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 183/700\n",
            "762/762 - 2s - loss: 1.1992e-04 - val_loss: 1.5936e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 184/700\n",
            "762/762 - 2s - loss: 1.1983e-04 - val_loss: 1.5993e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 185/700\n",
            "762/762 - 2s - loss: 1.1972e-04 - val_loss: 1.6090e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 186/700\n",
            "762/762 - 2s - loss: 1.1952e-04 - val_loss: 1.6311e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 187/700\n",
            "762/762 - 2s - loss: 1.1938e-04 - val_loss: 1.6500e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 188/700\n",
            "762/762 - 2s - loss: 1.1921e-04 - val_loss: 1.6668e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 189/700\n",
            "762/762 - 2s - loss: 1.1913e-04 - val_loss: 1.6740e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 190/700\n",
            "762/762 - 2s - loss: 1.1903e-04 - val_loss: 1.6768e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 191/700\n",
            "762/762 - 2s - loss: 1.1898e-04 - val_loss: 1.6750e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 192/700\n",
            "762/762 - 3s - loss: 1.1893e-04 - val_loss: 1.6599e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 193/700\n",
            "762/762 - 2s - loss: 1.1888e-04 - val_loss: 1.6324e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 194/700\n",
            "762/762 - 2s - loss: 1.1880e-04 - val_loss: 1.6064e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 195/700\n",
            "762/762 - 3s - loss: 1.1874e-04 - val_loss: 1.5884e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 196/700\n",
            "762/762 - 2s - loss: 1.1868e-04 - val_loss: 1.5626e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 197/700\n",
            "762/762 - 2s - loss: 1.1861e-04 - val_loss: 1.5284e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 198/700\n",
            "762/762 - 2s - loss: 1.1849e-04 - val_loss: 1.5028e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 199/700\n",
            "762/762 - 2s - loss: 1.1833e-04 - val_loss: 1.4921e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 200/700\n",
            "762/762 - 2s - loss: 1.1817e-04 - val_loss: 1.4978e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 201/700\n",
            "762/762 - 2s - loss: 1.1809e-04 - val_loss: 1.4946e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 202/700\n",
            "762/762 - 2s - loss: 1.1797e-04 - val_loss: 1.5131e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 203/700\n",
            "762/762 - 2s - loss: 1.1791e-04 - val_loss: 1.5088e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 204/700\n",
            "762/762 - 2s - loss: 1.1767e-04 - val_loss: 1.5371e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 205/700\n",
            "762/762 - 2s - loss: 1.1776e-04 - val_loss: 1.5191e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 206/700\n",
            "762/762 - 3s - loss: 1.1770e-04 - val_loss: 1.4918e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 207/700\n",
            "762/762 - 2s - loss: 1.1769e-04 - val_loss: 1.4536e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 208/700\n",
            "762/762 - 2s - loss: 1.1761e-04 - val_loss: 1.4356e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 209/700\n",
            "762/762 - 2s - loss: 1.1752e-04 - val_loss: 1.4229e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 210/700\n",
            "762/762 - 2s - loss: 1.1744e-04 - val_loss: 1.4205e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 211/700\n",
            "762/762 - 2s - loss: 1.1738e-04 - val_loss: 1.4173e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 212/700\n",
            "762/762 - 2s - loss: 1.1732e-04 - val_loss: 1.4193e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 213/700\n",
            "762/762 - 2s - loss: 1.1722e-04 - val_loss: 1.4284e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 214/700\n",
            "762/762 - 2s - loss: 1.1719e-04 - val_loss: 1.4392e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 215/700\n",
            "762/762 - 2s - loss: 1.1710e-04 - val_loss: 1.4603e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 216/700\n",
            "762/762 - 2s - loss: 1.1718e-04 - val_loss: 1.4598e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 217/700\n",
            "762/762 - 2s - loss: 1.1717e-04 - val_loss: 1.4542e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 218/700\n",
            "762/762 - 3s - loss: 1.1721e-04 - val_loss: 1.4357e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 219/700\n",
            "762/762 - 2s - loss: 1.1718e-04 - val_loss: 1.4270e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 220/700\n",
            "762/762 - 2s - loss: 1.1720e-04 - val_loss: 1.4213e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 221/700\n",
            "762/762 - 2s - loss: 1.1711e-04 - val_loss: 1.4336e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 222/700\n",
            "762/762 - 2s - loss: 1.1705e-04 - val_loss: 1.4436e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 223/700\n",
            "762/762 - 2s - loss: 1.1684e-04 - val_loss: 1.4813e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 224/700\n",
            "762/762 - 2s - loss: 1.1674e-04 - val_loss: 1.5059e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 225/700\n",
            "762/762 - 2s - loss: 1.1657e-04 - val_loss: 1.5320e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 226/700\n",
            "762/762 - 2s - loss: 1.1651e-04 - val_loss: 1.5290e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 227/700\n",
            "762/762 - 2s - loss: 1.1640e-04 - val_loss: 1.5185e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 228/700\n",
            "762/762 - 2s - loss: 1.1630e-04 - val_loss: 1.5060e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 229/700\n",
            "762/762 - 2s - loss: 1.1621e-04 - val_loss: 1.4939e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 230/700\n",
            "762/762 - 2s - loss: 1.1615e-04 - val_loss: 1.4759e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 231/700\n",
            "762/762 - 2s - loss: 1.1609e-04 - val_loss: 1.4557e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 232/700\n",
            "762/762 - 2s - loss: 1.1605e-04 - val_loss: 1.4379e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 233/700\n",
            "762/762 - 2s - loss: 1.1602e-04 - val_loss: 1.4214e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 234/700\n",
            "762/762 - 2s - loss: 1.1599e-04 - val_loss: 1.4032e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 235/700\n",
            "762/762 - 2s - loss: 1.1597e-04 - val_loss: 1.3843e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 236/700\n",
            "762/762 - 2s - loss: 1.1595e-04 - val_loss: 1.3735e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 237/700\n",
            "762/762 - 2s - loss: 1.1594e-04 - val_loss: 1.3634e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 238/700\n",
            "762/762 - 2s - loss: 1.1595e-04 - val_loss: 1.3532e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 239/700\n",
            "762/762 - 2s - loss: 1.1594e-04 - val_loss: 1.3418e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 240/700\n",
            "762/762 - 2s - loss: 1.1587e-04 - val_loss: 1.3444e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 241/700\n",
            "762/762 - 2s - loss: 1.1586e-04 - val_loss: 1.3457e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 242/700\n",
            "762/762 - 2s - loss: 1.1571e-04 - val_loss: 1.3702e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 243/700\n",
            "762/762 - 2s - loss: 1.1572e-04 - val_loss: 1.3888e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 244/700\n",
            "762/762 - 2s - loss: 1.1560e-04 - val_loss: 1.4208e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 245/700\n",
            "762/762 - 2s - loss: 1.1565e-04 - val_loss: 1.4371e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 246/700\n",
            "762/762 - 2s - loss: 1.1565e-04 - val_loss: 1.4212e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 247/700\n",
            "762/762 - 2s - loss: 1.1561e-04 - val_loss: 1.4061e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 248/700\n",
            "762/762 - 2s - loss: 1.1559e-04 - val_loss: 1.3810e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 249/700\n",
            "762/762 - 2s - loss: 1.1550e-04 - val_loss: 1.3653e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 250/700\n",
            "762/762 - 2s - loss: 1.1548e-04 - val_loss: 1.3529e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 251/700\n",
            "762/762 - 2s - loss: 1.1536e-04 - val_loss: 1.3543e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 252/700\n",
            "762/762 - 2s - loss: 1.1531e-04 - val_loss: 1.3597e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 253/700\n",
            "762/762 - 2s - loss: 1.1518e-04 - val_loss: 1.3797e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 254/700\n",
            "762/762 - 2s - loss: 1.1516e-04 - val_loss: 1.3970e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 255/700\n",
            "762/762 - 2s - loss: 1.1507e-04 - val_loss: 1.4182e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 256/700\n",
            "762/762 - 3s - loss: 1.1511e-04 - val_loss: 1.4298e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 257/700\n",
            "762/762 - 2s - loss: 1.1506e-04 - val_loss: 1.4365e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 258/700\n",
            "762/762 - 2s - loss: 1.1511e-04 - val_loss: 1.4367e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 259/700\n",
            "762/762 - 2s - loss: 1.1501e-04 - val_loss: 1.4386e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 260/700\n",
            "762/762 - 2s - loss: 1.1512e-04 - val_loss: 1.4278e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 261/700\n",
            "762/762 - 2s - loss: 1.1498e-04 - val_loss: 1.4185e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 262/700\n",
            "762/762 - 2s - loss: 1.1507e-04 - val_loss: 1.4000e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 263/700\n",
            "762/762 - 2s - loss: 1.1490e-04 - val_loss: 1.3873e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 264/700\n",
            "762/762 - 2s - loss: 1.1498e-04 - val_loss: 1.3800e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 265/700\n",
            "762/762 - 2s - loss: 1.1482e-04 - val_loss: 1.3718e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 266/700\n",
            "762/762 - 2s - loss: 1.1490e-04 - val_loss: 1.3729e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 267/700\n",
            "762/762 - 2s - loss: 1.1479e-04 - val_loss: 1.3641e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 268/700\n",
            "762/762 - 2s - loss: 1.1485e-04 - val_loss: 1.3628e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 269/700\n",
            "762/762 - 2s - loss: 1.1476e-04 - val_loss: 1.3530e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 270/700\n",
            "762/762 - 2s - loss: 1.1481e-04 - val_loss: 1.3571e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 271/700\n",
            "762/762 - 2s - loss: 1.1475e-04 - val_loss: 1.3502e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 272/700\n",
            "762/762 - 2s - loss: 1.1478e-04 - val_loss: 1.3595e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 273/700\n",
            "762/762 - 2s - loss: 1.1473e-04 - val_loss: 1.3595e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 274/700\n",
            "762/762 - 2s - loss: 1.1475e-04 - val_loss: 1.3768e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 275/700\n",
            "762/762 - 2s - loss: 1.1472e-04 - val_loss: 1.3771e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 276/700\n",
            "762/762 - 2s - loss: 1.1470e-04 - val_loss: 1.3914e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 277/700\n",
            "762/762 - 2s - loss: 1.1468e-04 - val_loss: 1.3827e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 278/700\n",
            "762/762 - 2s - loss: 1.1466e-04 - val_loss: 1.3850e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 279/700\n",
            "762/762 - 2s - loss: 1.1458e-04 - val_loss: 1.3775e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 280/700\n",
            "762/762 - 3s - loss: 1.1458e-04 - val_loss: 1.3724e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 281/700\n",
            "762/762 - 3s - loss: 1.1440e-04 - val_loss: 1.3779e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 282/700\n",
            "762/762 - 2s - loss: 1.1444e-04 - val_loss: 1.3760e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 283/700\n",
            "762/762 - 2s - loss: 1.1419e-04 - val_loss: 1.3939e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 284/700\n",
            "762/762 - 2s - loss: 1.1432e-04 - val_loss: 1.3839e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 285/700\n",
            "762/762 - 2s - loss: 1.1403e-04 - val_loss: 1.4007e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 286/700\n",
            "762/762 - 2s - loss: 1.1419e-04 - val_loss: 1.3891e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 287/700\n",
            "762/762 - 2s - loss: 1.1390e-04 - val_loss: 1.4133e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 288/700\n",
            "762/762 - 2s - loss: 1.1401e-04 - val_loss: 1.4081e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 289/700\n",
            "762/762 - 2s - loss: 1.1379e-04 - val_loss: 1.4208e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 290/700\n",
            "762/762 - 2s - loss: 1.1387e-04 - val_loss: 1.4065e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 291/700\n",
            "762/762 - 2s - loss: 1.1379e-04 - val_loss: 1.3887e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 292/700\n",
            "762/762 - 2s - loss: 1.1383e-04 - val_loss: 1.3616e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 293/700\n",
            "762/762 - 3s - loss: 1.1374e-04 - val_loss: 1.3379e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 294/700\n",
            "762/762 - 2s - loss: 1.1374e-04 - val_loss: 1.3206e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 295/700\n",
            "762/762 - 2s - loss: 1.1367e-04 - val_loss: 1.3182e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 296/700\n",
            "762/762 - 2s - loss: 1.1373e-04 - val_loss: 1.3271e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 297/700\n",
            "762/762 - 2s - loss: 1.1372e-04 - val_loss: 1.3482e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 298/700\n",
            "762/762 - 2s - loss: 1.1380e-04 - val_loss: 1.3839e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 299/700\n",
            "762/762 - 2s - loss: 1.1382e-04 - val_loss: 1.4119e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 300/700\n",
            "762/762 - 2s - loss: 1.1384e-04 - val_loss: 1.4409e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 301/700\n",
            "762/762 - 2s - loss: 1.1383e-04 - val_loss: 1.4385e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 302/700\n",
            "762/762 - 2s - loss: 1.1377e-04 - val_loss: 1.4166e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 303/700\n",
            "762/762 - 2s - loss: 1.1364e-04 - val_loss: 1.3870e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 304/700\n",
            "762/762 - 2s - loss: 1.1357e-04 - val_loss: 1.3665e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 305/700\n",
            "762/762 - 2s - loss: 1.1345e-04 - val_loss: 1.3515e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 306/700\n",
            "762/762 - 2s - loss: 1.1346e-04 - val_loss: 1.3495e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 307/700\n",
            "762/762 - 2s - loss: 1.1338e-04 - val_loss: 1.3464e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 308/700\n",
            "762/762 - 2s - loss: 1.1343e-04 - val_loss: 1.3667e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 309/700\n",
            "762/762 - 2s - loss: 1.1337e-04 - val_loss: 1.3727e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 310/700\n",
            "762/762 - 2s - loss: 1.1338e-04 - val_loss: 1.3995e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 311/700\n",
            "762/762 - 2s - loss: 1.1335e-04 - val_loss: 1.4046e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 312/700\n",
            "762/762 - 2s - loss: 1.1334e-04 - val_loss: 1.3977e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 313/700\n",
            "762/762 - 2s - loss: 1.1323e-04 - val_loss: 1.3782e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 314/700\n",
            "762/762 - 2s - loss: 1.1321e-04 - val_loss: 1.3540e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 315/700\n",
            "762/762 - 2s - loss: 1.1310e-04 - val_loss: 1.3399e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 316/700\n",
            "762/762 - 2s - loss: 1.1310e-04 - val_loss: 1.3324e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 317/700\n",
            "762/762 - 3s - loss: 1.1305e-04 - val_loss: 1.3451e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 318/700\n",
            "762/762 - 3s - loss: 1.1310e-04 - val_loss: 1.3680e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 319/700\n",
            "762/762 - 2s - loss: 1.1306e-04 - val_loss: 1.4010e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 320/700\n",
            "762/762 - 2s - loss: 1.1310e-04 - val_loss: 1.4419e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 321/700\n",
            "762/762 - 2s - loss: 1.1305e-04 - val_loss: 1.4891e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 322/700\n",
            "762/762 - 2s - loss: 1.1314e-04 - val_loss: 1.5117e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 323/700\n",
            "762/762 - 2s - loss: 1.1308e-04 - val_loss: 1.5243e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 324/700\n",
            "762/762 - 2s - loss: 1.1316e-04 - val_loss: 1.4885e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 325/700\n",
            "762/762 - 2s - loss: 1.1298e-04 - val_loss: 1.4568e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 326/700\n",
            "762/762 - 2s - loss: 1.1299e-04 - val_loss: 1.4077e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 327/700\n",
            "762/762 - 2s - loss: 1.1274e-04 - val_loss: 1.3786e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 328/700\n",
            "762/762 - 2s - loss: 1.1277e-04 - val_loss: 1.3453e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 329/700\n",
            "762/762 - 2s - loss: 1.1251e-04 - val_loss: 1.3276e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 330/700\n",
            "762/762 - 3s - loss: 1.1254e-04 - val_loss: 1.3118e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 331/700\n",
            "762/762 - 2s - loss: 1.1235e-04 - val_loss: 1.3088e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 332/700\n",
            "762/762 - 2s - loss: 1.1248e-04 - val_loss: 1.3107e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 333/700\n",
            "762/762 - 2s - loss: 1.1235e-04 - val_loss: 1.3219e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 334/700\n",
            "762/762 - 2s - loss: 1.1251e-04 - val_loss: 1.3280e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 335/700\n",
            "762/762 - 2s - loss: 1.1243e-04 - val_loss: 1.3579e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 336/700\n",
            "762/762 - 2s - loss: 1.1257e-04 - val_loss: 1.3462e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 337/700\n",
            "762/762 - 2s - loss: 1.1246e-04 - val_loss: 1.3469e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 338/700\n",
            "762/762 - 2s - loss: 1.1249e-04 - val_loss: 1.3250e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 339/700\n",
            "762/762 - 2s - loss: 1.1232e-04 - val_loss: 1.3094e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 340/700\n",
            "762/762 - 3s - loss: 1.1231e-04 - val_loss: 1.2979e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 341/700\n",
            "762/762 - 2s - loss: 1.1217e-04 - val_loss: 1.2960e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 342/700\n",
            "762/762 - 2s - loss: 1.1220e-04 - val_loss: 1.3060e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 343/700\n",
            "762/762 - 2s - loss: 1.1211e-04 - val_loss: 1.3327e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 344/700\n",
            "762/762 - 2s - loss: 1.1222e-04 - val_loss: 1.3779e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 345/700\n",
            "762/762 - 2s - loss: 1.1222e-04 - val_loss: 1.4133e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 346/700\n",
            "762/762 - 2s - loss: 1.1230e-04 - val_loss: 1.4434e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 347/700\n",
            "762/762 - 2s - loss: 1.1227e-04 - val_loss: 1.4398e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 348/700\n",
            "762/762 - 2s - loss: 1.1223e-04 - val_loss: 1.4367e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 349/700\n",
            "762/762 - 2s - loss: 1.1211e-04 - val_loss: 1.4018e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 350/700\n",
            "762/762 - 2s - loss: 1.1202e-04 - val_loss: 1.3743e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 351/700\n",
            "762/762 - 2s - loss: 1.1188e-04 - val_loss: 1.3379e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 352/700\n",
            "762/762 - 2s - loss: 1.1180e-04 - val_loss: 1.3160e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 353/700\n",
            "762/762 - 3s - loss: 1.1169e-04 - val_loss: 1.2892e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 354/700\n",
            "762/762 - 2s - loss: 1.1166e-04 - val_loss: 1.2779e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 355/700\n",
            "762/762 - 2s - loss: 1.1162e-04 - val_loss: 1.2624e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 356/700\n",
            "762/762 - 2s - loss: 1.1164e-04 - val_loss: 1.2660e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 357/700\n",
            "762/762 - 2s - loss: 1.1168e-04 - val_loss: 1.2619e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 358/700\n",
            "762/762 - 2s - loss: 1.1174e-04 - val_loss: 1.2906e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 359/700\n",
            "762/762 - 2s - loss: 1.1180e-04 - val_loss: 1.3003e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 360/700\n",
            "762/762 - 2s - loss: 1.1189e-04 - val_loss: 1.3521e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 361/700\n",
            "762/762 - 2s - loss: 1.1193e-04 - val_loss: 1.3691e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 362/700\n",
            "762/762 - 2s - loss: 1.1199e-04 - val_loss: 1.3861e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 363/700\n",
            "762/762 - 2s - loss: 1.1197e-04 - val_loss: 1.3820e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 364/700\n",
            "762/762 - 2s - loss: 1.1193e-04 - val_loss: 1.3639e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 365/700\n",
            "762/762 - 2s - loss: 1.1185e-04 - val_loss: 1.3572e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 366/700\n",
            "762/762 - 2s - loss: 1.1183e-04 - val_loss: 1.3360e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 367/700\n",
            "762/762 - 2s - loss: 1.1167e-04 - val_loss: 1.3172e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 368/700\n",
            "762/762 - 2s - loss: 1.1161e-04 - val_loss: 1.2914e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 369/700\n",
            "762/762 - 2s - loss: 1.1144e-04 - val_loss: 1.2796e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 370/700\n",
            "762/762 - 2s - loss: 1.1143e-04 - val_loss: 1.2679e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 371/700\n",
            "762/762 - 2s - loss: 1.1132e-04 - val_loss: 1.2650e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 372/700\n",
            "762/762 - 2s - loss: 1.1136e-04 - val_loss: 1.2681e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 373/700\n",
            "762/762 - 2s - loss: 1.1129e-04 - val_loss: 1.2640e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 374/700\n",
            "762/762 - 2s - loss: 1.1137e-04 - val_loss: 1.2772e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 375/700\n",
            "762/762 - 2s - loss: 1.1128e-04 - val_loss: 1.2682e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 376/700\n",
            "762/762 - 2s - loss: 1.1135e-04 - val_loss: 1.2789e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 377/700\n",
            "762/762 - 3s - loss: 1.1128e-04 - val_loss: 1.2686e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 378/700\n",
            "762/762 - 2s - loss: 1.1129e-04 - val_loss: 1.2842e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 379/700\n",
            "762/762 - 2s - loss: 1.1127e-04 - val_loss: 1.2768e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 380/700\n",
            "762/762 - 2s - loss: 1.1129e-04 - val_loss: 1.3108e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 381/700\n",
            "762/762 - 2s - loss: 1.1133e-04 - val_loss: 1.3078e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 382/700\n",
            "762/762 - 2s - loss: 1.1133e-04 - val_loss: 1.3490e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 383/700\n",
            "762/762 - 2s - loss: 1.1140e-04 - val_loss: 1.3446e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 384/700\n",
            "762/762 - 2s - loss: 1.1134e-04 - val_loss: 1.3770e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 385/700\n",
            "762/762 - 2s - loss: 1.1139e-04 - val_loss: 1.3730e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 386/700\n",
            "762/762 - 2s - loss: 1.1131e-04 - val_loss: 1.3764e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 387/700\n",
            "762/762 - 2s - loss: 1.1128e-04 - val_loss: 1.3536e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 388/700\n",
            "762/762 - 2s - loss: 1.1114e-04 - val_loss: 1.3284e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 389/700\n",
            "762/762 - 2s - loss: 1.1105e-04 - val_loss: 1.3094e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 390/700\n",
            "762/762 - 2s - loss: 1.1089e-04 - val_loss: 1.2861e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 391/700\n",
            "762/762 - 2s - loss: 1.1082e-04 - val_loss: 1.2867e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 392/700\n",
            "762/762 - 2s - loss: 1.1074e-04 - val_loss: 1.2733e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 393/700\n",
            "762/762 - 2s - loss: 1.1074e-04 - val_loss: 1.2748e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 394/700\n",
            "762/762 - 3s - loss: 1.1066e-04 - val_loss: 1.2548e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 395/700\n",
            "762/762 - 2s - loss: 1.1063e-04 - val_loss: 1.2486e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 396/700\n",
            "762/762 - 2s - loss: 1.1057e-04 - val_loss: 1.2357e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 397/700\n",
            "762/762 - 2s - loss: 1.1059e-04 - val_loss: 1.2407e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 398/700\n",
            "762/762 - 2s - loss: 1.1062e-04 - val_loss: 1.2392e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 399/700\n",
            "762/762 - 2s - loss: 1.1067e-04 - val_loss: 1.2557e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 400/700\n",
            "762/762 - 3s - loss: 1.1072e-04 - val_loss: 1.2653e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 401/700\n",
            "762/762 - 2s - loss: 1.1076e-04 - val_loss: 1.2789e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 402/700\n",
            "762/762 - 2s - loss: 1.1079e-04 - val_loss: 1.2949e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 403/700\n",
            "762/762 - 2s - loss: 1.1080e-04 - val_loss: 1.2977e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 404/700\n",
            "762/762 - 2s - loss: 1.1078e-04 - val_loss: 1.3036e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 405/700\n",
            "762/762 - 2s - loss: 1.1073e-04 - val_loss: 1.2875e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 406/700\n",
            "762/762 - 2s - loss: 1.1061e-04 - val_loss: 1.2802e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 407/700\n",
            "762/762 - 2s - loss: 1.1053e-04 - val_loss: 1.2639e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 408/700\n",
            "762/762 - 2s - loss: 1.1048e-04 - val_loss: 1.2855e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 409/700\n",
            "762/762 - 2s - loss: 1.1051e-04 - val_loss: 1.2855e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 410/700\n",
            "762/762 - 2s - loss: 1.1054e-04 - val_loss: 1.3035e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 411/700\n",
            "762/762 - 2s - loss: 1.1055e-04 - val_loss: 1.2876e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 412/700\n",
            "762/762 - 2s - loss: 1.1047e-04 - val_loss: 1.2743e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 413/700\n",
            "762/762 - 2s - loss: 1.1040e-04 - val_loss: 1.2577e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 414/700\n",
            "762/762 - 2s - loss: 1.1029e-04 - val_loss: 1.2442e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 415/700\n",
            "762/762 - 2s - loss: 1.1020e-04 - val_loss: 1.2260e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 416/700\n",
            "762/762 - 2s - loss: 1.1007e-04 - val_loss: 1.2143e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 417/700\n",
            "762/762 - 2s - loss: 1.1002e-04 - val_loss: 1.2021e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 418/700\n",
            "762/762 - 2s - loss: 1.0997e-04 - val_loss: 1.1957e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 419/700\n",
            "762/762 - 2s - loss: 1.1000e-04 - val_loss: 1.1944e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 420/700\n",
            "762/762 - 2s - loss: 1.1004e-04 - val_loss: 1.1962e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 421/700\n",
            "762/762 - 2s - loss: 1.1014e-04 - val_loss: 1.2092e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 422/700\n",
            "762/762 - 2s - loss: 1.1024e-04 - val_loss: 1.2282e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 423/700\n",
            "762/762 - 2s - loss: 1.1037e-04 - val_loss: 1.2775e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 424/700\n",
            "762/762 - 2s - loss: 1.1040e-04 - val_loss: 1.3090e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 425/700\n",
            "762/762 - 2s - loss: 1.1053e-04 - val_loss: 1.3523e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 426/700\n",
            "762/762 - 2s - loss: 1.1049e-04 - val_loss: 1.3338e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 427/700\n",
            "762/762 - 2s - loss: 1.1039e-04 - val_loss: 1.3152e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 428/700\n",
            "762/762 - 2s - loss: 1.1020e-04 - val_loss: 1.2862e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 429/700\n",
            "762/762 - 2s - loss: 1.1008e-04 - val_loss: 1.2684e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 430/700\n",
            "762/762 - 2s - loss: 1.0993e-04 - val_loss: 1.2588e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 431/700\n",
            "762/762 - 2s - loss: 1.0986e-04 - val_loss: 1.2419e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 432/700\n",
            "762/762 - 2s - loss: 1.0976e-04 - val_loss: 1.2392e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 433/700\n",
            "762/762 - 2s - loss: 1.0967e-04 - val_loss: 1.2217e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 434/700\n",
            "762/762 - 2s - loss: 1.0964e-04 - val_loss: 1.2285e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 435/700\n",
            "762/762 - 2s - loss: 1.0957e-04 - val_loss: 1.2167e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 436/700\n",
            "762/762 - 2s - loss: 1.0963e-04 - val_loss: 1.2307e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 437/700\n",
            "762/762 - 3s - loss: 1.0961e-04 - val_loss: 1.2284e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 438/700\n",
            "762/762 - 2s - loss: 1.0969e-04 - val_loss: 1.2409e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 439/700\n",
            "762/762 - 2s - loss: 1.0972e-04 - val_loss: 1.2483e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 440/700\n",
            "762/762 - 2s - loss: 1.0981e-04 - val_loss: 1.2571e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 441/700\n",
            "762/762 - 2s - loss: 1.0987e-04 - val_loss: 1.2743e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 442/700\n",
            "762/762 - 2s - loss: 1.1000e-04 - val_loss: 1.2778e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 443/700\n",
            "762/762 - 2s - loss: 1.1007e-04 - val_loss: 1.2887e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 444/700\n",
            "762/762 - 2s - loss: 1.1009e-04 - val_loss: 1.2728e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 445/700\n",
            "762/762 - 2s - loss: 1.1006e-04 - val_loss: 1.2613e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 446/700\n",
            "762/762 - 2s - loss: 1.0994e-04 - val_loss: 1.2450e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 447/700\n",
            "762/762 - 2s - loss: 1.0992e-04 - val_loss: 1.2392e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 448/700\n",
            "762/762 - 2s - loss: 1.0975e-04 - val_loss: 1.2288e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 449/700\n",
            "762/762 - 2s - loss: 1.0979e-04 - val_loss: 1.2326e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 450/700\n",
            "762/762 - 2s - loss: 1.0960e-04 - val_loss: 1.2194e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 451/700\n",
            "762/762 - 2s - loss: 1.0962e-04 - val_loss: 1.2190e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 452/700\n",
            "762/762 - 2s - loss: 1.0946e-04 - val_loss: 1.2069e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 453/700\n",
            "762/762 - 2s - loss: 1.0949e-04 - val_loss: 1.2112e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 454/700\n",
            "762/762 - 2s - loss: 1.0936e-04 - val_loss: 1.2019e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 455/700\n",
            "762/762 - 2s - loss: 1.0943e-04 - val_loss: 1.2087e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 456/700\n",
            "762/762 - 2s - loss: 1.0936e-04 - val_loss: 1.2034e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 457/700\n",
            "762/762 - 2s - loss: 1.0936e-04 - val_loss: 1.2012e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 458/700\n",
            "762/762 - 2s - loss: 1.0934e-04 - val_loss: 1.2036e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 459/700\n",
            "762/762 - 2s - loss: 1.0928e-04 - val_loss: 1.1978e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 460/700\n",
            "762/762 - 2s - loss: 1.0933e-04 - val_loss: 1.2096e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 461/700\n",
            "762/762 - 2s - loss: 1.0927e-04 - val_loss: 1.2084e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 462/700\n",
            "762/762 - 3s - loss: 1.0935e-04 - val_loss: 1.2421e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 463/700\n",
            "762/762 - 2s - loss: 1.0936e-04 - val_loss: 1.2645e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 464/700\n",
            "762/762 - 2s - loss: 1.0946e-04 - val_loss: 1.3048e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 465/700\n",
            "762/762 - 2s - loss: 1.0958e-04 - val_loss: 1.3210e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 466/700\n",
            "762/762 - 2s - loss: 1.0960e-04 - val_loss: 1.3005e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 467/700\n",
            "762/762 - 2s - loss: 1.0955e-04 - val_loss: 1.2773e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 468/700\n",
            "762/762 - 2s - loss: 1.0942e-04 - val_loss: 1.2520e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 469/700\n",
            "762/762 - 2s - loss: 1.0933e-04 - val_loss: 1.2308e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 470/700\n",
            "762/762 - 2s - loss: 1.0916e-04 - val_loss: 1.2121e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 471/700\n",
            "762/762 - 2s - loss: 1.0906e-04 - val_loss: 1.1946e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 472/700\n",
            "762/762 - 3s - loss: 1.0896e-04 - val_loss: 1.1852e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 473/700\n",
            "762/762 - 2s - loss: 1.0888e-04 - val_loss: 1.1718e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 474/700\n",
            "762/762 - 2s - loss: 1.0881e-04 - val_loss: 1.1695e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 475/700\n",
            "762/762 - 2s - loss: 1.0881e-04 - val_loss: 1.1617e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 476/700\n",
            "762/762 - 2s - loss: 1.0881e-04 - val_loss: 1.1677e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 477/700\n",
            "762/762 - 2s - loss: 1.0888e-04 - val_loss: 1.1690e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 478/700\n",
            "762/762 - 2s - loss: 1.0887e-04 - val_loss: 1.1797e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 479/700\n",
            "762/762 - 2s - loss: 1.0894e-04 - val_loss: 1.1888e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 480/700\n",
            "762/762 - 2s - loss: 1.0891e-04 - val_loss: 1.1961e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 481/700\n",
            "762/762 - 2s - loss: 1.0895e-04 - val_loss: 1.2016e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 482/700\n",
            "762/762 - 2s - loss: 1.0889e-04 - val_loss: 1.2016e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 483/700\n",
            "762/762 - 2s - loss: 1.0893e-04 - val_loss: 1.2172e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 484/700\n",
            "762/762 - 2s - loss: 1.0890e-04 - val_loss: 1.2143e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 485/700\n",
            "762/762 - 2s - loss: 1.0900e-04 - val_loss: 1.2418e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 486/700\n",
            "762/762 - 2s - loss: 1.0895e-04 - val_loss: 1.2290e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 487/700\n",
            "762/762 - 2s - loss: 1.0907e-04 - val_loss: 1.2328e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 488/700\n",
            "762/762 - 2s - loss: 1.0887e-04 - val_loss: 1.2115e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 489/700\n",
            "762/762 - 2s - loss: 1.0892e-04 - val_loss: 1.2049e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 490/700\n",
            "762/762 - 2s - loss: 1.0870e-04 - val_loss: 1.1887e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 491/700\n",
            "762/762 - 2s - loss: 1.0871e-04 - val_loss: 1.1806e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 492/700\n",
            "762/762 - 2s - loss: 1.0859e-04 - val_loss: 1.1723e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 493/700\n",
            "762/762 - 2s - loss: 1.0857e-04 - val_loss: 1.1671e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 494/700\n",
            "762/762 - 2s - loss: 1.0861e-04 - val_loss: 1.1695e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 495/700\n",
            "762/762 - 2s - loss: 1.0859e-04 - val_loss: 1.1671e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 496/700\n",
            "762/762 - 2s - loss: 1.0873e-04 - val_loss: 1.1836e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 497/700\n",
            "762/762 - 2s - loss: 1.0874e-04 - val_loss: 1.1838e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 498/700\n",
            "762/762 - 3s - loss: 1.0883e-04 - val_loss: 1.2033e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 499/700\n",
            "762/762 - 3s - loss: 1.0878e-04 - val_loss: 1.1955e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 500/700\n",
            "762/762 - 2s - loss: 1.0873e-04 - val_loss: 1.1984e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 501/700\n",
            "762/762 - 2s - loss: 1.0864e-04 - val_loss: 1.1879e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 502/700\n",
            "762/762 - 2s - loss: 1.0855e-04 - val_loss: 1.1872e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 503/700\n",
            "762/762 - 2s - loss: 1.0851e-04 - val_loss: 1.1792e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 504/700\n",
            "762/762 - 2s - loss: 1.0841e-04 - val_loss: 1.1801e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 505/700\n",
            "762/762 - 2s - loss: 1.0842e-04 - val_loss: 1.1749e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 506/700\n",
            "762/762 - 2s - loss: 1.0836e-04 - val_loss: 1.1791e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 507/700\n",
            "762/762 - 2s - loss: 1.0839e-04 - val_loss: 1.1817e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 508/700\n",
            "762/762 - 2s - loss: 1.0832e-04 - val_loss: 1.1783e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 509/700\n",
            "762/762 - 2s - loss: 1.0837e-04 - val_loss: 1.1857e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 510/700\n",
            "762/762 - 2s - loss: 1.0825e-04 - val_loss: 1.1766e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 511/700\n",
            "762/762 - 2s - loss: 1.0831e-04 - val_loss: 1.1822e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 512/700\n",
            "762/762 - 2s - loss: 1.0820e-04 - val_loss: 1.1739e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 513/700\n",
            "762/762 - 2s - loss: 1.0819e-04 - val_loss: 1.1754e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 514/700\n",
            "762/762 - 2s - loss: 1.0814e-04 - val_loss: 1.1699e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 515/700\n",
            "762/762 - 2s - loss: 1.0812e-04 - val_loss: 1.1692e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 516/700\n",
            "762/762 - 2s - loss: 1.0811e-04 - val_loss: 1.1653e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 517/700\n",
            "762/762 - 2s - loss: 1.0810e-04 - val_loss: 1.1643e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 518/700\n",
            "762/762 - 2s - loss: 1.0814e-04 - val_loss: 1.1646e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 519/700\n",
            "762/762 - 2s - loss: 1.0813e-04 - val_loss: 1.1650e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 520/700\n",
            "762/762 - 2s - loss: 1.0823e-04 - val_loss: 1.1709e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 521/700\n",
            "762/762 - 2s - loss: 1.0823e-04 - val_loss: 1.1742e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 522/700\n",
            "762/762 - 2s - loss: 1.0827e-04 - val_loss: 1.1821e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 523/700\n",
            "762/762 - 3s - loss: 1.0828e-04 - val_loss: 1.1858e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 524/700\n",
            "762/762 - 2s - loss: 1.0822e-04 - val_loss: 1.1843e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 525/700\n",
            "762/762 - 2s - loss: 1.0817e-04 - val_loss: 1.1817e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 526/700\n",
            "762/762 - 2s - loss: 1.0806e-04 - val_loss: 1.1762e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 527/700\n",
            "762/762 - 2s - loss: 1.0800e-04 - val_loss: 1.1715e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 528/700\n",
            "762/762 - 2s - loss: 1.0790e-04 - val_loss: 1.1669e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 529/700\n",
            "762/762 - 2s - loss: 1.0785e-04 - val_loss: 1.1618e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 530/700\n",
            "762/762 - 3s - loss: 1.0778e-04 - val_loss: 1.1601e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 531/700\n",
            "762/762 - 2s - loss: 1.0774e-04 - val_loss: 1.1571e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 532/700\n",
            "762/762 - 2s - loss: 1.0773e-04 - val_loss: 1.1581e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 533/700\n",
            "762/762 - 2s - loss: 1.0768e-04 - val_loss: 1.1580e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 534/700\n",
            "762/762 - 2s - loss: 1.0770e-04 - val_loss: 1.1599e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 535/700\n",
            "762/762 - 2s - loss: 1.0767e-04 - val_loss: 1.1643e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 536/700\n",
            "762/762 - 2s - loss: 1.0766e-04 - val_loss: 1.1643e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 537/700\n",
            "762/762 - 2s - loss: 1.0770e-04 - val_loss: 1.1756e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 538/700\n",
            "762/762 - 2s - loss: 1.0762e-04 - val_loss: 1.1696e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 539/700\n",
            "762/762 - 2s - loss: 1.0765e-04 - val_loss: 1.1778e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 540/700\n",
            "762/762 - 2s - loss: 1.0762e-04 - val_loss: 1.1706e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 541/700\n",
            "762/762 - 2s - loss: 1.0760e-04 - val_loss: 1.1747e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 542/700\n",
            "762/762 - 2s - loss: 1.0758e-04 - val_loss: 1.1673e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 543/700\n",
            "762/762 - 2s - loss: 1.0760e-04 - val_loss: 1.1725e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 544/700\n",
            "762/762 - 2s - loss: 1.0755e-04 - val_loss: 1.1675e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 545/700\n",
            "762/762 - 2s - loss: 1.0761e-04 - val_loss: 1.1734e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 546/700\n",
            "762/762 - 2s - loss: 1.0758e-04 - val_loss: 1.1684e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 547/700\n",
            "762/762 - 2s - loss: 1.0767e-04 - val_loss: 1.1764e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 548/700\n",
            "762/762 - 2s - loss: 1.0758e-04 - val_loss: 1.1711e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 549/700\n",
            "762/762 - 2s - loss: 1.0773e-04 - val_loss: 1.1866e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 550/700\n",
            "762/762 - 2s - loss: 1.0762e-04 - val_loss: 1.1816e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 551/700\n",
            "762/762 - 2s - loss: 1.0777e-04 - val_loss: 1.1916e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 552/700\n",
            "762/762 - 2s - loss: 1.0762e-04 - val_loss: 1.1786e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 553/700\n",
            "762/762 - 2s - loss: 1.0757e-04 - val_loss: 1.1766e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 554/700\n",
            "762/762 - 2s - loss: 1.0747e-04 - val_loss: 1.1668e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 555/700\n",
            "762/762 - 2s - loss: 1.0743e-04 - val_loss: 1.1649e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 556/700\n",
            "762/762 - 2s - loss: 1.0737e-04 - val_loss: 1.1576e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 557/700\n",
            "762/762 - 2s - loss: 1.0735e-04 - val_loss: 1.1573e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 558/700\n",
            "762/762 - 3s - loss: 1.0734e-04 - val_loss: 1.1528e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 559/700\n",
            "762/762 - 3s - loss: 1.0733e-04 - val_loss: 1.1531e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 560/700\n",
            "762/762 - 2s - loss: 1.0742e-04 - val_loss: 1.1546e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 561/700\n",
            "762/762 - 2s - loss: 1.0739e-04 - val_loss: 1.1552e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 562/700\n",
            "762/762 - 2s - loss: 1.0748e-04 - val_loss: 1.1649e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 563/700\n",
            "762/762 - 2s - loss: 1.0745e-04 - val_loss: 1.1621e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 564/700\n",
            "762/762 - 2s - loss: 1.0752e-04 - val_loss: 1.1817e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 565/700\n",
            "762/762 - 2s - loss: 1.0743e-04 - val_loss: 1.1683e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 566/700\n",
            "762/762 - 2s - loss: 1.0750e-04 - val_loss: 1.1802e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 567/700\n",
            "762/762 - 2s - loss: 1.0730e-04 - val_loss: 1.1690e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 568/700\n",
            "762/762 - 2s - loss: 1.0728e-04 - val_loss: 1.1730e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 569/700\n",
            "762/762 - 2s - loss: 1.0727e-04 - val_loss: 1.1682e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 570/700\n",
            "762/762 - 2s - loss: 1.0723e-04 - val_loss: 1.1703e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 571/700\n",
            "762/762 - 2s - loss: 1.0726e-04 - val_loss: 1.1714e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 572/700\n",
            "762/762 - 2s - loss: 1.0719e-04 - val_loss: 1.1697e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 573/700\n",
            "762/762 - 2s - loss: 1.0723e-04 - val_loss: 1.1707e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 574/700\n",
            "762/762 - 2s - loss: 1.0718e-04 - val_loss: 1.1669e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 575/700\n",
            "762/762 - 2s - loss: 1.0718e-04 - val_loss: 1.1692e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 576/700\n",
            "762/762 - 2s - loss: 1.0710e-04 - val_loss: 1.1648e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 577/700\n",
            "762/762 - 2s - loss: 1.0713e-04 - val_loss: 1.1718e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 578/700\n",
            "762/762 - 2s - loss: 1.0714e-04 - val_loss: 1.1733e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 579/700\n",
            "762/762 - 2s - loss: 1.0718e-04 - val_loss: 1.1802e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 580/700\n",
            "762/762 - 2s - loss: 1.0719e-04 - val_loss: 1.1786e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 581/700\n",
            "762/762 - 2s - loss: 1.0708e-04 - val_loss: 1.1737e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 582/700\n",
            "762/762 - 2s - loss: 1.0707e-04 - val_loss: 1.1704e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 583/700\n",
            "762/762 - 2s - loss: 1.0698e-04 - val_loss: 1.1656e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 584/700\n",
            "762/762 - 2s - loss: 1.0693e-04 - val_loss: 1.1623e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 585/700\n",
            "762/762 - 2s - loss: 1.0689e-04 - val_loss: 1.1586e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 586/700\n",
            "762/762 - 2s - loss: 1.0685e-04 - val_loss: 1.1558e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 587/700\n",
            "762/762 - 2s - loss: 1.0685e-04 - val_loss: 1.1544e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 588/700\n",
            "762/762 - 3s - loss: 1.0680e-04 - val_loss: 1.1512e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 589/700\n",
            "762/762 - 2s - loss: 1.0685e-04 - val_loss: 1.1524e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 590/700\n",
            "762/762 - 3s - loss: 1.0680e-04 - val_loss: 1.1500e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 591/700\n",
            "762/762 - 2s - loss: 1.0696e-04 - val_loss: 1.1583e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 592/700\n",
            "762/762 - 2s - loss: 1.0686e-04 - val_loss: 1.1554e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 593/700\n",
            "762/762 - 2s - loss: 1.0699e-04 - val_loss: 1.1657e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 594/700\n",
            "762/762 - 2s - loss: 1.0691e-04 - val_loss: 1.1619e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 595/700\n",
            "762/762 - 2s - loss: 1.0701e-04 - val_loss: 1.1680e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 596/700\n",
            "762/762 - 2s - loss: 1.0692e-04 - val_loss: 1.1635e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 597/700\n",
            "762/762 - 2s - loss: 1.0690e-04 - val_loss: 1.1661e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 598/700\n",
            "762/762 - 2s - loss: 1.0681e-04 - val_loss: 1.1602e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 599/700\n",
            "762/762 - 2s - loss: 1.0682e-04 - val_loss: 1.1653e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 600/700\n",
            "762/762 - 2s - loss: 1.0677e-04 - val_loss: 1.1620e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 601/700\n",
            "762/762 - 2s - loss: 1.0675e-04 - val_loss: 1.1656e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 602/700\n",
            "762/762 - 2s - loss: 1.0679e-04 - val_loss: 1.1664e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 603/700\n",
            "762/762 - 2s - loss: 1.0672e-04 - val_loss: 1.1681e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 604/700\n",
            "762/762 - 2s - loss: 1.0674e-04 - val_loss: 1.1718e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 605/700\n",
            "762/762 - 2s - loss: 1.0671e-04 - val_loss: 1.1721e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 606/700\n",
            "762/762 - 2s - loss: 1.0674e-04 - val_loss: 1.1736e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 607/700\n",
            "762/762 - 2s - loss: 1.0667e-04 - val_loss: 1.1689e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 608/700\n",
            "762/762 - 2s - loss: 1.0662e-04 - val_loss: 1.1644e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 609/700\n",
            "762/762 - 2s - loss: 1.0654e-04 - val_loss: 1.1599e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 610/700\n",
            "762/762 - 2s - loss: 1.0655e-04 - val_loss: 1.1579e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 611/700\n",
            "762/762 - 2s - loss: 1.0655e-04 - val_loss: 1.1563e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 612/700\n",
            "762/762 - 2s - loss: 1.0654e-04 - val_loss: 1.1547e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 613/700\n",
            "762/762 - 2s - loss: 1.0670e-04 - val_loss: 1.1577e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 614/700\n",
            "762/762 - 2s - loss: 1.0654e-04 - val_loss: 1.1542e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 615/700\n",
            "762/762 - 2s - loss: 1.0671e-04 - val_loss: 1.1576e-05 - 2s/epoch - 2ms/step\n",
            "Epoch 616/700\n",
            "762/762 - 2s - loss: 1.0654e-04 - val_loss: 1.1539e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 617/700\n",
            "762/762 - 2s - loss: 1.0673e-04 - val_loss: 1.1581e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 618/700\n",
            "762/762 - 2s - loss: 1.0657e-04 - val_loss: 1.1534e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 619/700\n",
            "762/762 - 2s - loss: 1.0666e-04 - val_loss: 1.1572e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 620/700\n",
            "762/762 - 2s - loss: 1.0656e-04 - val_loss: 1.1524e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 621/700\n",
            "762/762 - 3s - loss: 1.0659e-04 - val_loss: 1.1557e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 622/700\n",
            "762/762 - 2s - loss: 1.0656e-04 - val_loss: 1.1523e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 623/700\n",
            "762/762 - 2s - loss: 1.0651e-04 - val_loss: 1.1535e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 624/700\n",
            "762/762 - 2s - loss: 1.0653e-04 - val_loss: 1.1523e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 625/700\n",
            "762/762 - 2s - loss: 1.0647e-04 - val_loss: 1.1531e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 626/700\n",
            "762/762 - 2s - loss: 1.0650e-04 - val_loss: 1.1543e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 627/700\n",
            "762/762 - 2s - loss: 1.0642e-04 - val_loss: 1.1543e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 628/700\n",
            "762/762 - 2s - loss: 1.0646e-04 - val_loss: 1.1576e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 629/700\n",
            "762/762 - 2s - loss: 1.0633e-04 - val_loss: 1.1569e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 630/700\n",
            "762/762 - 2s - loss: 1.0639e-04 - val_loss: 1.1575e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 631/700\n",
            "762/762 - 2s - loss: 1.0630e-04 - val_loss: 1.1596e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 632/700\n",
            "762/762 - 2s - loss: 1.0628e-04 - val_loss: 1.1579e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 633/700\n",
            "762/762 - 2s - loss: 1.0626e-04 - val_loss: 1.1599e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 634/700\n",
            "762/762 - 2s - loss: 1.0621e-04 - val_loss: 1.1587e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 635/700\n",
            "762/762 - 2s - loss: 1.0619e-04 - val_loss: 1.1591e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 636/700\n",
            "762/762 - 2s - loss: 1.0615e-04 - val_loss: 1.1559e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 637/700\n",
            "762/762 - 2s - loss: 1.0615e-04 - val_loss: 1.1544e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 638/700\n",
            "762/762 - 2s - loss: 1.0612e-04 - val_loss: 1.1524e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 639/700\n",
            "762/762 - 2s - loss: 1.0610e-04 - val_loss: 1.1509e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 640/700\n",
            "762/762 - 3s - loss: 1.0609e-04 - val_loss: 1.1498e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 641/700\n",
            "762/762 - 2s - loss: 1.0609e-04 - val_loss: 1.1485e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 642/700\n",
            "762/762 - 2s - loss: 1.0609e-04 - val_loss: 1.1481e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 643/700\n",
            "762/762 - 2s - loss: 1.0611e-04 - val_loss: 1.1483e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 644/700\n",
            "762/762 - 2s - loss: 1.0608e-04 - val_loss: 1.1482e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 645/700\n",
            "762/762 - 2s - loss: 1.0614e-04 - val_loss: 1.1497e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 646/700\n",
            "762/762 - 2s - loss: 1.0605e-04 - val_loss: 1.1494e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 647/700\n",
            "762/762 - 2s - loss: 1.0616e-04 - val_loss: 1.1525e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 648/700\n",
            "762/762 - 2s - loss: 1.0610e-04 - val_loss: 1.1526e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 649/700\n",
            "762/762 - 3s - loss: 1.0615e-04 - val_loss: 1.1538e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 650/700\n",
            "762/762 - 2s - loss: 1.0605e-04 - val_loss: 1.1508e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 651/700\n",
            "762/762 - 2s - loss: 1.0613e-04 - val_loss: 1.1539e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 652/700\n",
            "762/762 - 2s - loss: 1.0607e-04 - val_loss: 1.1514e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 653/700\n",
            "762/762 - 2s - loss: 1.0608e-04 - val_loss: 1.1532e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 654/700\n",
            "762/762 - 2s - loss: 1.0603e-04 - val_loss: 1.1499e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 655/700\n",
            "762/762 - 2s - loss: 1.0608e-04 - val_loss: 1.1533e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 656/700\n",
            "762/762 - 2s - loss: 1.0603e-04 - val_loss: 1.1506e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 657/700\n",
            "762/762 - 2s - loss: 1.0610e-04 - val_loss: 1.1547e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 658/700\n",
            "762/762 - 2s - loss: 1.0599e-04 - val_loss: 1.1512e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 659/700\n",
            "762/762 - 2s - loss: 1.0609e-04 - val_loss: 1.1575e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 660/700\n",
            "762/762 - 2s - loss: 1.0591e-04 - val_loss: 1.1517e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 661/700\n",
            "762/762 - 2s - loss: 1.0603e-04 - val_loss: 1.1578e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 662/700\n",
            "762/762 - 2s - loss: 1.0587e-04 - val_loss: 1.1508e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 663/700\n",
            "762/762 - 2s - loss: 1.0588e-04 - val_loss: 1.1547e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 664/700\n",
            "762/762 - 2s - loss: 1.0585e-04 - val_loss: 1.1511e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 665/700\n",
            "762/762 - 2s - loss: 1.0582e-04 - val_loss: 1.1531e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 666/700\n",
            "762/762 - 2s - loss: 1.0585e-04 - val_loss: 1.1515e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 667/700\n",
            "762/762 - 2s - loss: 1.0575e-04 - val_loss: 1.1522e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 668/700\n",
            "762/762 - 2s - loss: 1.0581e-04 - val_loss: 1.1526e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 669/700\n",
            "762/762 - 2s - loss: 1.0571e-04 - val_loss: 1.1522e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 670/700\n",
            "762/762 - 2s - loss: 1.0582e-04 - val_loss: 1.1554e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 671/700\n",
            "762/762 - 2s - loss: 1.0568e-04 - val_loss: 1.1532e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 672/700\n",
            "762/762 - 2s - loss: 1.0576e-04 - val_loss: 1.1569e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 673/700\n",
            "762/762 - 2s - loss: 1.0563e-04 - val_loss: 1.1532e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 674/700\n",
            "762/762 - 2s - loss: 1.0573e-04 - val_loss: 1.1572e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 675/700\n",
            "762/762 - 2s - loss: 1.0559e-04 - val_loss: 1.1531e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 676/700\n",
            "762/762 - 2s - loss: 1.0564e-04 - val_loss: 1.1553e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 677/700\n",
            "762/762 - 2s - loss: 1.0557e-04 - val_loss: 1.1530e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 678/700\n",
            "762/762 - 2s - loss: 1.0559e-04 - val_loss: 1.1524e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 679/700\n",
            "762/762 - 2s - loss: 1.0556e-04 - val_loss: 1.1502e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 680/700\n",
            "762/762 - 2s - loss: 1.0555e-04 - val_loss: 1.1489e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 681/700\n",
            "762/762 - 3s - loss: 1.0554e-04 - val_loss: 1.1483e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 682/700\n",
            "762/762 - 3s - loss: 1.0553e-04 - val_loss: 1.1468e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 683/700\n",
            "762/762 - 2s - loss: 1.0550e-04 - val_loss: 1.1457e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 684/700\n",
            "762/762 - 2s - loss: 1.0558e-04 - val_loss: 1.1457e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 685/700\n",
            "762/762 - 2s - loss: 1.0550e-04 - val_loss: 1.1461e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 686/700\n",
            "762/762 - 2s - loss: 1.0558e-04 - val_loss: 1.1480e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 687/700\n",
            "762/762 - 2s - loss: 1.0555e-04 - val_loss: 1.1488e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 688/700\n",
            "762/762 - 2s - loss: 1.0560e-04 - val_loss: 1.1519e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 689/700\n",
            "762/762 - 2s - loss: 1.0553e-04 - val_loss: 1.1495e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 690/700\n",
            "762/762 - 2s - loss: 1.0545e-04 - val_loss: 1.1490e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 691/700\n",
            "762/762 - 2s - loss: 1.0544e-04 - val_loss: 1.1473e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 692/700\n",
            "762/762 - 3s - loss: 1.0538e-04 - val_loss: 1.1460e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 693/700\n",
            "762/762 - 3s - loss: 1.0543e-04 - val_loss: 1.1468e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 694/700\n",
            "762/762 - 3s - loss: 1.0537e-04 - val_loss: 1.1458e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 695/700\n",
            "762/762 - 3s - loss: 1.0545e-04 - val_loss: 1.1472e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 696/700\n",
            "762/762 - 3s - loss: 1.0537e-04 - val_loss: 1.1468e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 697/700\n",
            "762/762 - 3s - loss: 1.0542e-04 - val_loss: 1.1476e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 698/700\n",
            "762/762 - 4s - loss: 1.0533e-04 - val_loss: 1.1476e-05 - 4s/epoch - 5ms/step\n",
            "Epoch 699/700\n",
            "762/762 - 2s - loss: 1.0538e-04 - val_loss: 1.1484e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 700/700\n",
            "762/762 - 2s - loss: 1.0537e-04 - val_loss: 1.1492e-05 - 2s/epoch - 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  best tcn architecture is the one used in the papaer"
      ],
      "metadata": {
        "id": "Vi-k3p80zLmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyrxlI_zh4_P"
      },
      "outputs": [],
      "source": [
        "# load the trained saved model\n",
        "model_saved = tf.keras.models.load_model('/content/drive/MyDrive/my_trained_models/optimal_lstm_model_200Eps.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tInM8k2Aa_V1"
      },
      "outputs": [],
      "source": [
        "# Load the best weights\n",
        "model_saved.load_weights('/content/drive/MyDrive/my_trained_models/optimal_lstm_wts.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "615tFlHjehTK"
      },
      "outputs": [],
      "source": [
        "# Load the best weights\n",
        "model.load_weights(os.path.join(directory, 'mdl_wts.hdf5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qycqaUhnK5QM",
        "outputId": "b96811c0-3be1-49c2-afdc-a07ee03fc0e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4128404473012779e-05"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model_saved.evaluate(test_X, test_y, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUQ9MQO4e0uQ",
        "outputId": "e17eab6b-5331-4627-a29e-1e5e4111974c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:700\n",
            "Validation loss: 1.1491964869492222e-05\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(test_X, test_y, verbose=0)\n",
        "best_epoch = loss_tracking.index(score) + 1\n",
        "# validation loss and corresponding epoch for the saved model\n",
        "print(f'Epoch:{best_epoch}\\nValidation loss: {score}') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfmgjN3OhMYp"
      },
      "outputs": [],
      "source": [
        "# # continue if training is interrupted \n",
        "# model.compile(loss='mean_squared_error',\n",
        "#               optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5))\n",
        "# early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=2,\n",
        "#                                 restore_best_weights=True, mode='min')\n",
        "# # save the best weights if training is interrupted\n",
        "# mcp_save = ModelCheckpoint(os.path.join(directory, 'mdl_wts.hdf5'),\n",
        "#                             save_best_only=True,\n",
        "#                             monitor='val_loss', mode='min') \n",
        "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n",
        "\n",
        "# # Set the initial and total number of epochs\n",
        "# initial_epoch = 1\n",
        "# n_epochs = 2000\n",
        "\n",
        "# # Run the training loop\n",
        "# for epoch in range(initial_epoch , n_epochs+1):\n",
        "#     print(f'Epoch {epoch}/{n_epochs}')\n",
        "#     # Train the model for one epoch\n",
        "#     history = model.fit(train_X, train_y, callbacks=[early_stopping, mcp_save, reduce_lr_loss],\n",
        "#                     epochs=1, batch_size=2, validation_data=(test_X, test_y),\n",
        "#                      verbose=2,\n",
        "#                      shuffle=False)\n",
        "#     # to find for which epoch each loss belongs\n",
        "#     validation_loss= model.evaluate(test_X, test_y, verbose=0)\n",
        "#     loss_tracking.append(validation_loss)\n",
        "#     # Save the model every 10 epochs\n",
        "#     if epoch % 50 == 0:\n",
        "#         # Save the model  in HDF5 foramt with a filename that includes the epoch number\n",
        "#         #model.save(f'model_{epoch}Eps.h5')\n",
        "#         model.save(os.path.join(directory, f'model_{epoch}Eps.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "Om2PkqgiVjri",
        "outputId": "8d75ecc0-91a9-49f5-f5a9-47da9b0cd8f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-5ae6c544b3e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultipleLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.plot(history.history['loss'], label='train')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mloss_tracking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_tracking' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(18,6)) \n",
        "ax.xaxis.set_major_locator(plt.MultipleLocator(5))\n",
        "#plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(loss_tracking, label='test')\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHohbk8uYRIh"
      },
      "source": [
        "**MinMax Scaler equation**\n",
        "$$x' = \\frac{(x - min)}{(max - min)} \\times (new\\ max\\ value - new\\ min\\  value) + new\\ min\\ value$$<br/>\n",
        "\n",
        "\n",
        "$$x = \\frac{(max - min)\\times (new\\ min\\ value + x') + (new\\ max\\ value - new\\ min\\ value)\\times min}{new\\ max\\ value - new\\ min\\ value}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$x$ is the inverse scaled value\n",
        "$x'$ is the scaled value\n",
        "$min$ is the minimum value of the original data\n",
        "$max$ is the maximum value of the original data.<br/>\n",
        "$new\\ max\\ value$ and $new\\ min\\ value$ is the new range that we want to scale the data to. For example: $(1,0)\\ or\\ (1,-1)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ3IIvIhvHaz",
        "outputId": "2a99c626-e774-417c-f00f-9660bbe52238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# make a prediction \n",
        "# select the number of obersvtions for prediction\n",
        "n_obs = len(test)\n",
        "yhat = model.predict(test_X[-n_obs:], verbose=1)\n",
        "\n",
        "\n",
        "# invert scaling \n",
        "scaled_y = pd.DataFrame(test_y)\n",
        "scaled_yhat = pd.DataFrame(yhat.ravel()) ## ravel () converting into 1D array\n",
        "#obtain the min and max from the training set\n",
        "unscaled_train = pd.DataFrame(series_supervised[:len(train)])\n",
        "#new feature range\n",
        "new_max_value = 1 \n",
        "new_min_value= 0\n",
        "feature_range = new_max_value - new_min_value\n",
        "\n",
        "def transform_column(column):\n",
        "    min_value = min(unscaled_train.iloc[:, -steps_ahead + column.name])\n",
        "    max_value = max(unscaled_train.iloc[:, -steps_ahead + column.name])\n",
        "    return ((max_value - min_value) * (new_min_value + column) + (feature_range  * min_value)) / feature_range \n",
        "    \n",
        "# invert scaling for actual\n",
        "inv_scale_y = scaled_y.apply(transform_column, axis=0)\n",
        "inv_scale_y = inv_scale_y.values.ravel() \n",
        "# invert scaling for forecast\n",
        "inv_scale_yhat = scaled_yhat.apply(transform_column, axis=0)\n",
        "inv_scale_yhat = inv_scale_yhat.values.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqftbPYxvEE5"
      },
      "outputs": [],
      "source": [
        "# Invert the Differencing for actual \n",
        "df = pd.DataFrame(series.iloc[-len(test)-steps_ahead:,-1])\n",
        "n_vars = df.shape[1]\n",
        "columns = df.columns\n",
        "cols, names = list(), list()\n",
        "for i in range(0, steps_ahead):\n",
        "    cols.append(df.shift(-i))\n",
        "    if i == 0:\n",
        "        names += [('%s(t)' % (columns[j])) for j in range(n_vars)]\n",
        "    else:\n",
        "        names += [('%s(t+%d)' % (columns[j], i)) for j in range(n_vars)]\n",
        "# put it all together\n",
        "agg = pd.concat(cols, axis=1)\n",
        "agg.columns = names\n",
        "agg.dropna(inplace=True)\n",
        "agg = agg.iloc[:-1,0]\n",
        "#drop all the variables that we don't want to predict\n",
        "#agg.drop(columns=vars_to_drop, inplace=True)\n",
        "agg = agg.to_numpy()\n",
        "inv_y = np.add(inv_scale_y,agg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHQK0b0wTCAw",
        "outputId": "6c64b299-1305-4ea7-aab3-9df186a1536e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DATEPRD\n",
              "2014-03-11    3333.536720\n",
              "2014-03-12    3312.214262\n",
              "2014-03-13    3421.594068\n",
              "2014-03-14    3415.241360\n",
              "2014-03-15    3419.141042\n",
              "                 ...     \n",
              "2015-03-18    1659.440731\n",
              "2015-03-19    1662.711432\n",
              "2015-03-20    1707.494884\n",
              "2015-03-21    1725.420844\n",
              "2015-03-22    1620.632599\n",
              "Name: BORE_OIL_VOL, Length: 377, dtype: float64"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "series.iloc[-len(test)-steps_ahead:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWAqc4iTkxvn"
      },
      "source": [
        "* To invert the differencing of time series for multistep prediction:<br/>\n",
        "The equation is given by $$\n",
        "\\hat x_{t+h|t}=x_t+(\\widehat{\\Delta x_{t+1}}+\\dots+\\widehat{\\Delta x_{t+h}}).\n",
        "$$ <br/>\n",
        "where: <br/>\n",
        "$\\hat x_{t+h|t}$ is the predicted value of the time series x at time $t$+h, given the value of the time series at time $t$.<br/>\n",
        "$x_t$ is the value of the time series $x$ at time t.<br/>\n",
        "${\\Delta x_{t+1}}$ is the difference between the value of the time series $x$ at time $t+1$ and the value of the time series at time t.<br/>\n",
        "${\\Delta x_{t+2}}$ is the difference between the value of the time series $x$ at time $t+2$ and the value of the time series at time $t+1$.<br/>\n",
        "${\\Delta x_{t+h}}$ is the difference between the value of the time series $x$ at time $t+h$ and the value of the time series at time $t+h-1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "DSJErK0WDkX4",
        "outputId": "cd4fddbf-5e0e-4ae1-df63-49e1c3bb0cbb"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-169-8f51947b0c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#drop all the variables that we don't want to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvars_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginalSeries_supervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msteps_ahead\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvars_to_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars_y\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars_name_to_drop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0moriginalSeries_supervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvars_to_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moriginalSeries_xt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginalSeries_supervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msteps_ahead\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-169-8f51947b0c24>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#drop all the variables that we don't want to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvars_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginalSeries_supervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msteps_ahead\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvars_to_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars_y\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars_name_to_drop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0moriginalSeries_supervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvars_to_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moriginalSeries_xt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginalSeries_supervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msteps_ahead\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vars_name_to_drop' is not defined"
          ]
        }
      ],
      "source": [
        "# Invert the Differencing for forecast\n",
        "# to invert the diffrenced predicted values,the the predicted differenced value is added\n",
        "# to previous predicted diffenced values and last available observation in test set(Xt) as explained above\n",
        "originalSeries_supervised = series_to_supervised(series, series.columns, n_in=timesteps, n_out=steps_ahead, dropnan=True)\n",
        "\n",
        "current_timestep = 1\n",
        "# actual value of oil rate at current time step\n",
        "# steps_ahead = 4\n",
        "#drop all the variables that we don't want to predict\n",
        "vars_y = originalSeries_supervised.columns[-steps_ahead*len(series.columns):]\n",
        "vars_to_drop = [col for col in vars_y if col.startswith(vars_name_to_drop[0])]\n",
        "originalSeries_supervised.drop(columns=vars_to_drop, inplace=True)\n",
        "originalSeries_xt = originalSeries_supervised.iloc[-len(test):,-steps_ahead-2]\n",
        "\n",
        "\n",
        "# A predicted value at any given step ahead is a result of the previous cumulative differnced predicted values and current time step\n",
        "col = []\n",
        "#inv_yhat_cum = np.cumsum(inv_scale_yhat, axis=1)\n",
        "inv_yhat_cum = inv_scale_yhat\n",
        "\n",
        "for i in range(n_obs):\n",
        "    #.ravel() flattens the series into a one-dimensional array\n",
        "    inverted_diff_yhat = originalSeries_xt[-n_obs:].ravel()[i] + inv_yhat_cum[i]\n",
        "    col.append(inverted_diff_yhat)\n",
        "#col = pd.DataFrame.from_records(col) # creates a DataFrame from a list of records\n",
        "col = pd.DataFrame(col)\n",
        "#col.columns = pd.RangeIndex(start=1, stop=steps_ahead+1, step=1)\n",
        "inv_yhat = col.values.ravel() # convert df to NumpyArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si8_sP7AeBNV"
      },
      "outputs": [],
      "source": [
        "inv_yhat = np.add(inv_scale_yhat,agg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsMdUkqbt5W_",
        "outputId": "e935514b-1ee2-49bc-e860-ae1e28938af9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.81898940e-12, -3.83693077e-13, -1.81898940e-12, -1.81898940e-12,\n",
              "       -1.81898940e-12, -1.81898940e-12])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "inv_y[np.where(inv_y <0)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_yhat[np.where(inv_y <0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmkOOERvSI9O",
        "outputId": "c733cc36-2b70-4ecf-d94e-90ffd737aa0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-229.85074056, -201.1940488 , -129.91894531, -129.54980469,\n",
              "       -129.54980469, -129.54980469])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_yhat[np.where(inv_y <0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWR9qU5LPtZg",
        "outputId": "25f4c3c0-4f26-4d4b-ffc5-1a1597fffd98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-163.72281087,  -25.71943942,  -20.23535156,  -19.48730469,\n",
              "        -19.48730469,  -19.48730469])"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjwHlOmtZus4",
        "outputId": "7b9c1267-4726-4510-c7b9-068d15fb9d5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-193.72964681,  -66.68428317,  -19.41894531,  -18.77441406,\n",
              "        -18.77441406,  -18.77441406])"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ],
      "source": [
        "inv_yhat[np.where(inv_y <0)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_yhat[np.where(inv_y <0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo_sUPxwReC4",
        "outputId": "c9d3378d-949e-4514-c93c-4f53466d7664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-255.16617025, -167.07588473,  -37.01660156,  -41.84667969,\n",
              "        -41.84667969,  -41.84667969])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_yhat[np.where(inv_y <0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK2_Ux0yNTHw",
        "outputId": "2e040b1e-6ac1-4063-c3a5-9a3f9ba67c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-193.81265462,  -66.7252988 ,  -19.41699219,  -18.77148438,\n",
              "        -18.77148438,  -18.77148438])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_yhat[np.where(inv_y <0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzeRUbVzJn0o",
        "outputId": "53f58b8b-6b40-4c01-8f87-7b5b721db792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-370.86734212, -266.39229098, -111.45996094,  -61.03417969,\n",
              "        -61.15429688,  -61.15429688])"
            ]
          },
          "metadata": {},
          "execution_count": 437
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_yhat[np.where(inv_y <0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS1_cAiI4lVX",
        "outputId": "e03c9691-2780-4510-bd28-8f94bb1c2dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-184.43960775, -117.37275973,   31.31445312,   32.32421875,\n",
              "         32.32421875,   32.32421875])"
            ]
          },
          "metadata": {},
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(inv_y,inv_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkTqNvIGrasw",
        "outputId": "9e742c3e-a15c-434e-928a-b4d42007b330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9825556356039782"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(inv_y,inv_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1ri_DlnKPjl",
        "outputId": "037f37f6-18d2-4960-da09-8144eee84c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.943113222152864"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(inv_y,inv_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygd2GdFLrdOA",
        "outputId": "1f1823e5-3076-4ae2-91df-c5de59e3ba68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9678677413182221"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaXlbRINdKzi",
        "outputId": "777f558f-5fb3-44d9-f1a0-23805873aff8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9785535819551673"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "r2_score(inv_y,inv_yhat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAPE(inv_y[np.where(~(inv_y <0))],inv_yhat[np.where(~(inv_y <0))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmGtJTgnrjnh",
        "outputId": "60a07854-322f-4e72-ce95-96fa2734da20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.554805617459637"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wMAPE(inv_y,inv_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcD7s6oCKRiT",
        "outputId": "d71fa61d-2219-49f1-dd17-1fda14291928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2230121923686776"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UT3jqDxPF9e",
        "outputId": "2797fa5d-2b38-4af8-c003-56940c31c038"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.075756149494592"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "wMAPE(inv_y,inv_yhat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xw8O7CoPQAG",
        "outputId": "e2f4c065-c83d-4ff0-e6f3-1f96b1abc63d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((6,), (6,))"
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inv_y.shape, inv_yhat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqXWUX41YtDY",
        "outputId": "5addeb7f-08ae-4dbb-b192-afba3e83c4ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 97.92466\n",
            "Test RMSPE: 2858157198299995.50000\n",
            "Test MAE: 50.85937\n",
            "Test MAPE: 245619343228612.25000\n",
            "Test r2: 0.98256\n",
            "Test wMAPE: 2.22301 \n",
            "Test SMAPE: 7.10244 \n"
          ]
        }
      ],
      "source": [
        "# Performance evaluation\n",
        "\n",
        "rmse_test = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "print('Test RMSE: %.5f' % rmse_test)\n",
        "#report performance using RMSPE\n",
        "RMSPE_test = RMSPE(inv_y, inv_yhat)\n",
        "print('Test RMSPE: %.5f' % RMSPE_test)\n",
        "MAE_test = mean_absolute_error(inv_y, inv_yhat)\n",
        "print('Test MAE: %.5f' % MAE_test)\n",
        "MAPE_test = MAPE(inv_y, inv_yhat)\n",
        "print('Test MAPE: %.5f' % MAPE_test)\n",
        "r2 = r2_score(inv_y, inv_yhat)\n",
        "print('Test r2: %.5f' % r2)\n",
        "wMAPE_test = wMAPE(inv_y, inv_yhat)\n",
        "print('Test wMAPE: %.5f ' % wMAPE_test)\n",
        "SMAPE_test = SMAPE(inv_y, inv_yhat)\n",
        "print('Test SMAPE: %.5f ' % SMAPE_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Test RMSE: 108.57802\n",
        "Test RMSPE: 1050251866795420.12500\n",
        "Test MAE: 59.25265\n",
        "Test MAPE: 84532810027035.62500\n",
        "Test r2: 0.97855\n",
        "Test wMAPE: 2.58987 \n",
        "Test SMAPE: 7.66815"
      ],
      "metadata": {
        "id": "bBgiDQ9MzrkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "51SnY7n5e0nx",
        "outputId": "1f038602-a51a-4b26-8c5f-0bfb86cabda4"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-bef913473e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresult_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_yhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mresult_RMSPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_yhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresult_MAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_yhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        }
      ],
      "source": [
        "# Performance evaluation\n",
        "rmse_test, RMSPE_test, MAE_test, MAPE_test, r2_test, wMAPE_test, SMAPE_test  = [], [], [], [], [], [], []\n",
        "# calculate the score for each day\n",
        "\n",
        "for i in range(test_y.shape[1]):\n",
        "    result_rmse = sqrt(mean_squared_error(inv_y[:,i], inv_yhat[:,i]))\n",
        "    result_RMSPE = RMSPE(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_MAE = mean_absolute_error(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_MAPE = MAPE(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_r2 = r2_score(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_wMAPE = wMAPE(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_SMAPE = SMAPE(inv_y[:,i], inv_yhat[:,i])\n",
        "\n",
        "    rmse_test.append(result_rmse)\n",
        "    RMSPE_test.append(result_RMSPE)\n",
        "    MAE_test.append(result_MAE)\n",
        "    MAPE_test.append(result_MAPE)\n",
        "    r2_test.append(result_r2)\n",
        "    wMAPE_test.append(result_wMAPE)\n",
        "    SMAPE_test.append(result_SMAPE)\n",
        "    \n",
        "## calculate overall score\n",
        "print(\"The Average scores for the vector output {} steps ahead:\\n\".format(steps_ahead))\n",
        "print('Test RMSE: %.5f' % np.mean(rmse_test))\n",
        "#print('Test RMSPE: %.5f' % np.mean(RMSPE_test)) because of that the denominator (actual) has some zero values\n",
        "print('Test MAE: %.5f' % np.mean(MAE_test))\n",
        "#print('Test MAPE: %.5f' % np.mean(MAPE_test)) because of that the denominator (actual) has some zero values\n",
        "print('Test r2: %.5f' % np.mean(r2_test))\n",
        "print('Test wMAPE: %.5f ' % np.mean(wMAPE_test))\n",
        "print('Test SMAPE: %.5f ' % np.mean(SMAPE_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef--j4gGOE6F"
      },
      "outputs": [],
      "source": [
        "Test RMSE: 555.56135\n",
        "Test MAE: 277.11301\n",
        "Test r2: 0.39641\n",
        "Test wMAPE: 12.32902 \n",
        "Test SMAPE: 17.45901"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4KkEyXsJvDW"
      },
      "outputs": [],
      "source": [
        "# plot the scores for each time step of the multi-step forecast\n",
        "scores = pd.DataFrame({\"rmse_test\":rmse_test, \"MAE_test\":MAE_test, \"R-squared_test\":r2_test, \"wMAPE_test\":wMAPE_test, \"SMAPE_test\":SMAPE_test})\n",
        "\n",
        "# Reset the index, keeping the old index as a column\n",
        "scores = scores.reset_index(drop=False)\n",
        "\n",
        "# Set the 'index' column as the new index\n",
        "scores.index = scores['index'] + 1\n",
        "\n",
        "# Drop the old 'index' column\n",
        "scores = scores.drop(columns='index')\n",
        "\n",
        "data = scores.columns\n",
        "\n",
        "# Creating figure with two rows and one column\n",
        "fig, axs = plt.subplots(nrows=len(data), figsize=(17, 15))\n",
        "\n",
        "axs = axs.ravel()\n",
        "\n",
        "for id, column in enumerate(data):\n",
        "    # Set the x-axis limits\n",
        "    #axs[id].set_xlim(xmin=1, xmax= steps_ahead)\n",
        "    #print the name of the test on plot\n",
        "    axs[id].plot(scores[column])\n",
        "    # Add a title to the x-axis\n",
        "    axs[id].set_xlabel('Steps ahead',fontsize=10, labelpad=0.1)\n",
        "    axs[id].grid(True)\n",
        "    # Remove the horizontal grid lines\n",
        "    axs[id].grid(which='both', axis='y')\n",
        "    axs[id].xaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "    axs[id].legend([column], loc='upper left', fontsize=15, handlelength=0, handletextpad=0, frameon=False)\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-CDANaINC-c"
      },
      "outputs": [],
      "source": [
        "inv_yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHKnDMnFNdmw"
      },
      "outputs": [],
      "source": [
        "inv_yhat[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1S1CN4BEWUe"
      },
      "outputs": [],
      "source": [
        "plt.plot(inv_yhat[-1], label = \"predicted\")\n",
        "plt.plot(inv_y[-1], label = \"actual\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2CGD3H6NZoj"
      },
      "outputs": [],
      "source": [
        "plt.plot(inv_yhat.flatten(), label = \"predicted\")\n",
        "plt.plot(inv_y.flatten(), label = \"actual\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GHomhZ5E3jc"
      },
      "outputs": [],
      "source": [
        "plt.plot(inv_yhat.flatten(), label = \"predicted\")\n",
        "plt.plot(inv_y.flatten(), label = \"actual\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doFyZoCCZ-ht"
      },
      "outputs": [],
      "source": [
        "# plot the last forecasted values on test set\n",
        "fig, ax = plt.subplots(1, 1, figsize=(18,6)) \n",
        "# Set the major locator for the x-axis\n",
        "x = list(range(1, len(inv_yhat[-1])+1))\n",
        "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "ax.plot(x, inv_yhat[-1], label = \"predicted\")\n",
        "ax.plot(x, inv_y[-1], label = \"actual\")\n",
        "ax.set_ylabel('Oil Rate', fontsize=15)\n",
        "ax.set_xlabel('Steps Ahead (Days)',fontsize=15)\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZcZ-O1kL3Q5"
      },
      "outputs": [],
      "source": [
        "#comparing predictions and actual values\n",
        "act_pred = pd.DataFrame({\"actual\":inv_y.flatten(), \"prediction\":inv_yhat.flatten()})\n",
        "act_pred.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7APsQRBRvRbx"
      },
      "outputs": [],
      "source": [
        "series_to_supervised(series, series.columns, n_in=2, n_out=steps_ahead, dropnan=True).iloc[-len(test):-len(test)+5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kx12S2F506y"
      },
      "outputs": [],
      "source": [
        "r2_score(act_pred.iloc[:5,0],act_pred.iloc[:5,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS4cX0QOuinK"
      },
      "outputs": [],
      "source": [
        "wMAPE(act_pred.iloc[:5,0],act_pred.iloc[:5,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4g3tcrW5lIB"
      },
      "outputs": [],
      "source": [
        "MAPE(act_pred.iloc[:5,0],act_pred.iloc[:5,1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wwKJr1Q3FXimTKbuiFQ7g-CxNgJqVCdf",
      "authorship_tag": "ABX9TyMFEdic+Uzp0vowE8F1GbEG",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}