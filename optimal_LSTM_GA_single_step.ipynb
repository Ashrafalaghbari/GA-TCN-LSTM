{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashrafalaghbari/oil-production-forecasting/blob/main/optimal_LSTM_GA_single_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "x1LrkoSv4cc7",
        "outputId": "6cfa3831-1585-4e71-eb4d-f5bd7b8a773d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(60000)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autosaving every 60 seconds\n"
          ]
        }
      ],
      "source": [
        "%autosave 60"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tcn --no-dependencies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8tyr5_aWpmN",
        "outputId": "01cac0e3-14ad-4b0c-8225-b28c9361c9dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tcn\n",
            "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: keras-tcn\n",
            "Successfully installed keras-tcn-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XxzUA1Pg4iEU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import re\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation, Dropout\n",
        "from keras.layers import LSTM, Conv1D\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import math "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6ZrWV9NpfPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417e3aae-4c6d-4e04-e5b8-80c4b1cfb0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu7vsRejtR_4"
      },
      "outputs": [],
      "source": [
        "# # check if GPU is utilized \n",
        "# device_name = tf.config.experimental.list_physical_devices()[-1][-1]\n",
        "# if device_name != 'GPU':\n",
        "#     raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lVMp6qZiT9gO"
      },
      "outputs": [],
      "source": [
        "# Select the best features\n",
        "def select_features(df, target, correlation_type, threshold):\n",
        "    if (threshold < -1 ) | (threshold > 1 ) :\n",
        "            raise SystemError('correlation threshold is out of bounds')\n",
        "    features = df.corr(correlation_type).loc[target].drop(target)\n",
        "    best_features = features.where(abs(features) > threshold).dropna()\n",
        "    df = pd.concat([df[target], df[best_features.index]], axis=1)\n",
        "    return df\n",
        "\n",
        "# convert series to supervised learning using a sliding  window approach\n",
        "def series_to_supervised(data, columns, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('%s(t-%d)' % (columns[j], i)) for j in range(n_vars)]\n",
        "        # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('%s(t)' % (columns[j])) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('%s(t+%d)' % (columns[j], i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "# scale train and test data to new feature range[0, 1]\n",
        "def scale(train, test):\n",
        "\t# fit scaler\n",
        "\tscaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\tscaler = scaler.fit(train)\n",
        "\t# transform train\n",
        "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
        "\ttrain_scaled = scaler.transform(train)\n",
        "\t# transform test\n",
        "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
        "\ttest_scaled = scaler.transform(test)\n",
        "\treturn scaler, train_scaled, test_scaled\n",
        "\n",
        "\n",
        "# inverse differencing\n",
        "def inverse_difference(history, interval=1):\n",
        "\treturn history[-len(test_scaled)-interval:-interval]\n",
        "\n",
        "#Evaluation metrics\n",
        "# compute RMSPE\n",
        "def RMSPE(x,y):\n",
        "\tresult=0\n",
        "\tfor i in range(len(x)):\n",
        "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
        "\tresult /= len(x)\n",
        "\tresult = sqrt(result)\n",
        "\tresult *= 100\n",
        "\treturn result\n",
        "\n",
        "# compute MAPE\n",
        "def MAPE(x,y):\n",
        "\tresult=0\n",
        "\tfor i in range(len(x)):\n",
        "\t\tresult += abs((x[i]-y[i])/x[i])\n",
        "\tresult /= len(x)\n",
        "\tresult *= 100\n",
        "\treturn result\n",
        "\n",
        "# compute wMAPE weighted absolute percentage error\n",
        "def wMAPE(actual, predicted): \n",
        "    result_nom = 0\n",
        "    result_deno = 0\n",
        "    for i in range(len(actual)):\n",
        "        result_nom +=  abs(actual[i] - predicted[i])\n",
        "        result_deno +=  abs(actual[i]) \n",
        "    result = result_nom/result_deno\n",
        "    return result *100\n",
        "\n",
        "def SMAPE(actual, predicted): #Symmetric (adjusted) MEAN ABSOLUTE PERCENTAGE ERROR (SMAPE)\n",
        "    result = 0\n",
        "    for i in range(len(actual)):\n",
        "        result += abs(actual[i] - predicted[i])/(abs(actual[i]) + abs(predicted[i]))\n",
        "    result = 2 * result/ len(actual) \n",
        "    return result * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tRJsglCpLKHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aed4624-df55-4b4d-f04f-8d167d129d77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1907, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#load dataset\n",
        "series = pd.read_csv('/content/drive/MyDrive/volve_production_data/model.csv', \n",
        "                     parse_dates=[\"DATEPRD\"], index_col=\"DATEPRD\")\n",
        "days = pd.Series(range(len(series),0, -1 ), index=series.index)\n",
        "series.insert(0, 'days', days)\n",
        "series['days1'] = series['days'].shift(-1)\n",
        "series[\"AVG_CHOKE_SIZE_P1\"] = series['AVG_CHOKE_SIZE_P'].shift(-1)\n",
        "series[\"ON_STREAM_HRS1\"] = series['ON_STREAM_HRS'].shift(-1)\n",
        "\n",
        "\n",
        "series['interaction_effect_onNext_oil1'] = series[\"AVG_CHOKE_SIZE_P1\"]  * series[\"ON_STREAM_HRS1\"] * series['days']\n",
        "\n",
        "series.dropna(inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # # # select feature based on correlation\n",
        "# # series = select_features(series, \"BORE_OIL_VOL\", \"spearman\", 0.2)\n",
        "# # select features manually\n",
        "series =series[[\n",
        "                'interaction_effect_onNext_oil1',\n",
        "                \"BORE_GAS_VOL\", \n",
        "                \"BORE_OIL_VOL\"\n",
        "                ]] \n",
        "series.shape             "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "series"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "I3ti_3XeWC0I",
        "outputId": "526747b2-ae1b-417a-f1f6-c70fcd511454"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            interaction_effect_onNext_oil1  BORE_GAS_VOL  BORE_OIL_VOL\n",
              "DATEPRD                                                               \n",
              "2010-01-01                    2.321410e+06  1.462166e+07  18593.749401\n",
              "2010-01-02                    2.181563e+06  1.469266e+07  18701.242265\n",
              "2010-01-03                    2.045042e+06  1.400904e+07  17799.912406\n",
              "2010-01-04                    2.091405e+06  1.341015e+07  17002.616014\n",
              "2010-01-05                    2.104483e+06  1.361768e+07  17270.939334\n",
              "...                                    ...           ...           ...\n",
              "2015-03-18                    1.440000e+04  1.354500e+06   1659.440731\n",
              "2015-03-19                    1.200000e+04  1.366424e+06   1662.711432\n",
              "2015-03-20                    9.600000e+03  1.397308e+06   1707.494884\n",
              "2015-03-21                    7.200000e+03  1.408435e+06   1725.420844\n",
              "2015-03-22                    4.799716e+03  1.379366e+06   1620.632599\n",
              "\n",
              "[1907 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45b9907c-f43d-4eae-adbf-5d826333e48d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>interaction_effect_onNext_oil1</th>\n",
              "      <th>BORE_GAS_VOL</th>\n",
              "      <th>BORE_OIL_VOL</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATEPRD</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-01</th>\n",
              "      <td>2.321410e+06</td>\n",
              "      <td>1.462166e+07</td>\n",
              "      <td>18593.749401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-02</th>\n",
              "      <td>2.181563e+06</td>\n",
              "      <td>1.469266e+07</td>\n",
              "      <td>18701.242265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-03</th>\n",
              "      <td>2.045042e+06</td>\n",
              "      <td>1.400904e+07</td>\n",
              "      <td>17799.912406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-04</th>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>1.341015e+07</td>\n",
              "      <td>17002.616014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>1.361768e+07</td>\n",
              "      <td>17270.939334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-18</th>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>1.354500e+06</td>\n",
              "      <td>1659.440731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-19</th>\n",
              "      <td>1.200000e+04</td>\n",
              "      <td>1.366424e+06</td>\n",
              "      <td>1662.711432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-20</th>\n",
              "      <td>9.600000e+03</td>\n",
              "      <td>1.397308e+06</td>\n",
              "      <td>1707.494884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-21</th>\n",
              "      <td>7.200000e+03</td>\n",
              "      <td>1.408435e+06</td>\n",
              "      <td>1725.420844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-22</th>\n",
              "      <td>4.799716e+03</td>\n",
              "      <td>1.379366e+06</td>\n",
              "      <td>1620.632599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1907 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45b9907c-f43d-4eae-adbf-5d826333e48d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-45b9907c-f43d-4eae-adbf-5d826333e48d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-45b9907c-f43d-4eae-adbf-5d826333e48d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KpoQJ7P7lmyb"
      },
      "outputs": [],
      "source": [
        "# Create a new directory in My Drive\n",
        "directory = '/content/drive/My Drive/my_trained_models'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # convert series to stationary \n",
        "series_diff = series.copy()\n",
        "# diff_order = 1\n",
        "# series_diff['BORE_OIL_VOL'] = series_diff['BORE_OIL_VOL'].diff(diff_order)\n",
        "\n",
        "# Define window size and number of the steps ahead for forecasting\n",
        "window_size = 4\n",
        "steps_ahead = 1\n",
        "\n",
        "# # convert the stationary series to supervise learning using sliding window approach\n",
        "series_supervised = series_to_supervised(series_diff, series_diff.columns, n_in= window_size, n_out= steps_ahead, dropnan=True)   \n",
        "\n",
        "# drop columns we don't want to predict\n",
        "pattern = re.compile(r\"(t-)|^BORE_OIL_VOL.*\")\n",
        "\n",
        "# Extract the column names that match the pattern\n",
        "matching_columns = [col for col in series_supervised.columns if re.search(pattern, col)]\n",
        "series_supervised = series_supervised[matching_columns]"
      ],
      "metadata": {
        "id": "6b5TKp5GhQE-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series_supervised"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "gHS94TLUbavd",
        "outputId": "b0a42833-f446-4459-c862-f54e9ca8e33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            interaction_effect_onNext_oil1(t-4)  BORE_OIL_VOL(t-4)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-05                         2.321410e+06       18593.749401   \n",
              "2010-01-06                         2.181563e+06       18701.242265   \n",
              "2010-01-07                         2.045042e+06       17799.912406   \n",
              "2010-01-08                         2.091405e+06       17002.616014   \n",
              "2010-01-09                         2.104483e+06       17270.939334   \n",
              "...                                         ...                ...   \n",
              "2015-03-18                         2.400000e+04        1627.740085   \n",
              "2015-03-19                         2.160000e+04        1613.713808   \n",
              "2015-03-20                         1.920000e+04        1630.318908   \n",
              "2015-03-21                         1.680000e+04        1649.125441   \n",
              "2015-03-22                         1.440000e+04        1659.440731   \n",
              "\n",
              "            interaction_effect_onNext_oil1(t-3)  BORE_OIL_VOL(t-3)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-05                         2.181563e+06       18701.242265   \n",
              "2010-01-06                         2.045042e+06       17799.912406   \n",
              "2010-01-07                         2.091405e+06       17002.616014   \n",
              "2010-01-08                         2.104483e+06       17270.939334   \n",
              "2010-01-09                         2.043370e+06       17331.761803   \n",
              "...                                         ...                ...   \n",
              "2015-03-18                         2.160000e+04        1613.713808   \n",
              "2015-03-19                         1.920000e+04        1630.318908   \n",
              "2015-03-20                         1.680000e+04        1649.125441   \n",
              "2015-03-21                         1.440000e+04        1659.440731   \n",
              "2015-03-22                         1.200000e+04        1662.711432   \n",
              "\n",
              "            interaction_effect_onNext_oil1(t-2)  BORE_OIL_VOL(t-2)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-05                         2.045042e+06       17799.912406   \n",
              "2010-01-06                         2.091405e+06       17002.616014   \n",
              "2010-01-07                         2.104483e+06       17270.939334   \n",
              "2010-01-08                         2.043370e+06       17331.761803   \n",
              "2010-01-09                         2.045269e+06       17138.601719   \n",
              "...                                         ...                ...   \n",
              "2015-03-18                         1.920000e+04        1630.318908   \n",
              "2015-03-19                         1.680000e+04        1649.125441   \n",
              "2015-03-20                         1.440000e+04        1659.440731   \n",
              "2015-03-21                         1.200000e+04        1662.711432   \n",
              "2015-03-22                         9.600000e+03        1707.494884   \n",
              "\n",
              "            interaction_effect_onNext_oil1(t-1)  BORE_OIL_VOL(t-1)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-05                         2.091405e+06       17002.616014   \n",
              "2010-01-06                         2.104483e+06       17270.939334   \n",
              "2010-01-07                         2.043370e+06       17331.761803   \n",
              "2010-01-08                         2.045269e+06       17138.601719   \n",
              "2010-01-09                         1.724474e+06       17127.657449   \n",
              "...                                         ...                ...   \n",
              "2015-03-18                         1.680000e+04        1649.125441   \n",
              "2015-03-19                         1.440000e+04        1659.440731   \n",
              "2015-03-20                         1.200000e+04        1662.711432   \n",
              "2015-03-21                         9.600000e+03        1707.494884   \n",
              "2015-03-22                         7.200000e+03        1725.420844   \n",
              "\n",
              "            BORE_OIL_VOL(t)  \n",
              "DATEPRD                      \n",
              "2010-01-05     17270.939334  \n",
              "2010-01-06     17331.761803  \n",
              "2010-01-07     17138.601719  \n",
              "2010-01-08     17127.657449  \n",
              "2010-01-09     14477.823141  \n",
              "...                     ...  \n",
              "2015-03-18      1659.440731  \n",
              "2015-03-19      1662.711432  \n",
              "2015-03-20      1707.494884  \n",
              "2015-03-21      1725.420844  \n",
              "2015-03-22      1620.632599  \n",
              "\n",
              "[1903 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f468f6e4-f282-4966-b526-0027d466e62e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>interaction_effect_onNext_oil1(t-4)</th>\n",
              "      <th>BORE_OIL_VOL(t-4)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t-3)</th>\n",
              "      <th>BORE_OIL_VOL(t-3)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t-2)</th>\n",
              "      <th>BORE_OIL_VOL(t-2)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t-1)</th>\n",
              "      <th>BORE_OIL_VOL(t-1)</th>\n",
              "      <th>BORE_OIL_VOL(t)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATEPRD</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>2.321410e+06</td>\n",
              "      <td>18593.749401</td>\n",
              "      <td>2.181563e+06</td>\n",
              "      <td>18701.242265</td>\n",
              "      <td>2.045042e+06</td>\n",
              "      <td>17799.912406</td>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>17002.616014</td>\n",
              "      <td>17270.939334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>2.181563e+06</td>\n",
              "      <td>18701.242265</td>\n",
              "      <td>2.045042e+06</td>\n",
              "      <td>17799.912406</td>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>17002.616014</td>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>17270.939334</td>\n",
              "      <td>17331.761803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>2.045042e+06</td>\n",
              "      <td>17799.912406</td>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>17002.616014</td>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>17270.939334</td>\n",
              "      <td>2.043370e+06</td>\n",
              "      <td>17331.761803</td>\n",
              "      <td>17138.601719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>17002.616014</td>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>17270.939334</td>\n",
              "      <td>2.043370e+06</td>\n",
              "      <td>17331.761803</td>\n",
              "      <td>2.045269e+06</td>\n",
              "      <td>17138.601719</td>\n",
              "      <td>17127.657449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-09</th>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>17270.939334</td>\n",
              "      <td>2.043370e+06</td>\n",
              "      <td>17331.761803</td>\n",
              "      <td>2.045269e+06</td>\n",
              "      <td>17138.601719</td>\n",
              "      <td>1.724474e+06</td>\n",
              "      <td>17127.657449</td>\n",
              "      <td>14477.823141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-18</th>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>1627.740085</td>\n",
              "      <td>2.160000e+04</td>\n",
              "      <td>1613.713808</td>\n",
              "      <td>1.920000e+04</td>\n",
              "      <td>1630.318908</td>\n",
              "      <td>1.680000e+04</td>\n",
              "      <td>1649.125441</td>\n",
              "      <td>1659.440731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-19</th>\n",
              "      <td>2.160000e+04</td>\n",
              "      <td>1613.713808</td>\n",
              "      <td>1.920000e+04</td>\n",
              "      <td>1630.318908</td>\n",
              "      <td>1.680000e+04</td>\n",
              "      <td>1649.125441</td>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>1659.440731</td>\n",
              "      <td>1662.711432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-20</th>\n",
              "      <td>1.920000e+04</td>\n",
              "      <td>1630.318908</td>\n",
              "      <td>1.680000e+04</td>\n",
              "      <td>1649.125441</td>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>1659.440731</td>\n",
              "      <td>1.200000e+04</td>\n",
              "      <td>1662.711432</td>\n",
              "      <td>1707.494884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-21</th>\n",
              "      <td>1.680000e+04</td>\n",
              "      <td>1649.125441</td>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>1659.440731</td>\n",
              "      <td>1.200000e+04</td>\n",
              "      <td>1662.711432</td>\n",
              "      <td>9.600000e+03</td>\n",
              "      <td>1707.494884</td>\n",
              "      <td>1725.420844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-22</th>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>1659.440731</td>\n",
              "      <td>1.200000e+04</td>\n",
              "      <td>1662.711432</td>\n",
              "      <td>9.600000e+03</td>\n",
              "      <td>1707.494884</td>\n",
              "      <td>7.200000e+03</td>\n",
              "      <td>1725.420844</td>\n",
              "      <td>1620.632599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1903 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f468f6e4-f282-4966-b526-0027d466e62e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f468f6e4-f282-4966-b526-0027d466e62e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f468f6e4-f282-4966-b526-0027d466e62e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 331
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # split into train and test sets\n",
        "n_features = int((len(series_supervised.columns) -steps_ahead)/window_size)\n",
        "series_supervised = series_supervised.values\n",
        "train_size = int(series_supervised.shape[0] * 0.8)\n",
        "test_size = series_supervised.shape[0] - train_size\n",
        "train, test = series_supervised[0:train_size], series_supervised[train_size:]\n",
        "print(\"train.shape:\",train.shape, \"test.shape:\",test.shape)\n",
        "\n",
        "# scale  the data to a feature range(0,1)\n",
        "scaler, train_scaled, test_scaled = scale(train, test)\n",
        "print('train_scaled.shape:',train_scaled.shape, \"test_scaled.shape:\",test_scaled.shape)\n",
        "\n",
        "# # reshape input to be 3D [samples, window_size, features]\n",
        "train_X, train_y = train_scaled[:, 0:-steps_ahead], train_scaled[:, -steps_ahead:]\n",
        "train_X = train_X.reshape(train_X.shape[0], window_size, n_features)\n",
        "test_X, test_y = test_scaled[:, 0:-steps_ahead], test_scaled[:, -steps_ahead:]\n",
        "test_X = test_X.reshape(test_X.shape[0], window_size, n_features )\n",
        "print(\"train_X.shape:\",train_X.shape, \"train_y.shape:\",train_y.shape,\n",
        "        \"test_X.shape:\", test_X.shape,\"test_y.shape:\",test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsv-ScobJY3w",
        "outputId": "c0e2f5d3-06ff-41b8-f1c4-fc8ef29ce31e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.shape: (1522, 13) test.shape: (381, 13)\n",
            "train_scaled.shape: (1522, 13) test_scaled.shape: (381, 13)\n",
            "train_X.shape: (1522, 4, 3) train_y.shape: (1522, 1) test_X.shape: (381, 4, 3) test_y.shape: (381, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#grid search \n",
        "def get_hyper_param(n_epochs, num_hidden_layers, num_neurons, batch_size, window_size):\n",
        "    \"\"\" \n",
        "    This is a grid search function that generates all possible comibinations\n",
        "    from the search space \n",
        "\n",
        "    Args:\n",
        "    n_epochs: number of epochs\n",
        "    num_hidden_layers: number of hidden layers\n",
        "    num_neurons: number of neurons same for both input and hidden layers \n",
        "    batch_size: number of batch size \n",
        "    window_size: historical timesteps in the sliding window\n",
        "    \n",
        "    \"\"\"\n",
        "    hyper_param = []\n",
        "    for current_params in itertools.product(n_epochs, num_hidden_layers, num_neurons, batch_size, window_size):\n",
        "        hyper_param.append(list(current_params))\n",
        "    return hyper_param\n",
        "\n",
        "# Seacrh space\n",
        "n_epochs = [500]\n",
        "num_hidden_layers = [1, 2]\n",
        "num_neurons = [4, 8, 16, 32, 64]\n",
        "batch_size = [2, 4]\n",
        "window_size = [2, 3, 4, 5, 6]\n",
        "\n",
        "hyper_param = get_hyper_param(n_epochs, num_hidden_layers, num_neurons, batch_size, window_size)\n",
        "len(hyper_param)# print the number of combinations\n",
        "hyper_param = hyper_param[:2]"
      ],
      "metadata": {
        "id": "D0EVoWtzzMsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparamter tuning\n",
        "def fit_lstm(steps_ahead = 1):\n",
        "    \n",
        "        # Create a new directory in My Drive\n",
        "    directory = '/content/drive/My Drive/my_trained_models'\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # setting the session configurations for reproducibility.\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    np.random.seed(42)\n",
        "    np.random.seed(12345)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                            inter_op_parallelism_threads=1)\n",
        "    from keras import backend as K\n",
        "    tf.random.set_seed(1234)\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), \n",
        "                                config=session_conf)\n",
        "    K.set_session(sess)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    min_val_loss = math.inf \n",
        "    for n_epochs, num_hidden_layers, num_neurons, batch_size, window_size in hyper_param:\n",
        "        print('n_epochs', n_epochs, 'num_hidden_layers', num_hidden_layers, 'num_neurons',\n",
        "              num_neurons, \"batch_size\", batch_size, 'window_size', window_size)\n",
        "        combinations = [n_epochs, num_hidden_layers, num_neurons, batch_size, window_size]\n",
        "\n",
        "        #feature engineering\n",
        "        # # convert the stationary series to supervise learning\n",
        "        series_supervised = series_to_supervised(series_diff, series_diff.columns, n_in= window_size, n_out= steps_ahead, dropnan=True)   \n",
        "\n",
        "        # drop columns we don't want to predict\n",
        "        pattern = re.compile(r\"(t-)|^BORE_OIL_VOL.*\")\n",
        "        # Extract the column names that match the pattern\n",
        "        matching_columns = [col for col in series_supervised.columns if re.search(pattern, col)]\n",
        "        series_supervised = series_supervised[matching_columns]\n",
        "\n",
        "        # # split into train and test sets\n",
        "        series_supervised = series_supervised.values\n",
        "        train_size = int(series_supervised.shape[0] * 0.8)\n",
        "        test_size = series_supervised.shape[0] - train_size\n",
        "        train, test = series_supervised[0:train_size], series_supervised[train_size:]\n",
        "        print(\"train.shape:\",train.shape, \"test.shape:\",test.shape)\n",
        "\n",
        "        # scale  the data to a feature range(0,1)\n",
        "        scaler, train_scaled, test_scaled = scale(train, test)\n",
        "        print('train_scaled.shape:',train_scaled.shape, \"test_scaled.shape:\",test_scaled.shape)\n",
        "\n",
        "        # reshape input to be 3D [samples, window_size, features]\n",
        "        n_features = len(series.columns)\n",
        "        train_X, train_y = train_scaled[:, 0:-steps_ahead], train_scaled[:, -steps_ahead:]\n",
        "        train_X = train_X.reshape(train_X.shape[0], window_size, n_features)\n",
        "        test_X, test_y = test_scaled[:, 0:-steps_ahead], test_scaled[:, -steps_ahead:]\n",
        "        test_X = test_X.reshape(test_X.shape[0], window_size, n_features )\n",
        "        print(\"train_X.shape:\",train_X.shape, \"train_y.shape:\",train_y.shape,\n",
        "              \"test_X.shape:\", test_X.shape,\"test_y.shape:\",test_y.shape)\n",
        "        \n",
        "        # instantiate the LSTM model\n",
        "        model = Sequential()\n",
        "\n",
        "        if num_hidden_layers != 1:\n",
        "  \n",
        "            for num in range(num_hidden_layers-1):\n",
        "                model.add(LSTM(num_neurons, input_shape=(window_size, n_features), return_sequences=True))\n",
        "            model.add(LSTM(num_neurons))\n",
        "\n",
        "        else:\n",
        "            model.add(LSTM(num_neurons, input_shape=(window_size, n_features)))\n",
        "        model.add(Dense(steps_ahead)) # output layer\n",
        "        model.compile(loss='mean_squared_error',\n",
        "                    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "\n",
        "        \n",
        "        #prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=2,\n",
        "                                restore_best_weights=True, mode='min')\n",
        "        \n",
        "        # save the best weights\n",
        "        mcp_save = ModelCheckpoint(os.path.join(directory, f'{combinations}_weights.hdf5'),\n",
        "                            save_best_only=True,\n",
        "                            monitor='val_loss', mode='min') \n",
        "        # fit model\n",
        "        lstm_model = model.fit(train_X, train_y, epochs=n_epochs,\n",
        "                            callbacks=[early_stopping, mcp_save],\n",
        "                            batch_size=batch_size,\n",
        "                            validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "        \n",
        "        # Save the model  in HDF5 foramt with a filename that includes the hyperparamters\n",
        "        model.save(os.path.join(directory, f'{combinations}_model.h5'))\n",
        "        # Load the best weights\n",
        "        model.load_weights(os.path.join(directory, f'{combinations}_weights.hdf5'))\n",
        "        current_val_loss = model.evaluate(test_X, test_y, verbose=0) #lstm_model.history['val_loss'][-1]\n",
        "        if current_val_loss < min_val_loss:\n",
        "            min_val_loss = current_val_loss\n",
        "            best_params = [n_epochs,  num_hidden_layers, num_neurons, batch_size, window_size]\n",
        "\n",
        "        \n",
        "    print('final best params',\"n_epochs:\",best_params[0],\"num_hidden_layers:\",\n",
        "          best_params[1], \"num_neurons:\", best_params[2], \"batch_size:\", best_params[3],\n",
        "          \"window_size:\",best_params[4]) \n",
        "    return {\"best_params\": str(best_params) , \"MSE\": min_val_loss}#, lstm_model"
      ],
      "metadata": {
        "id": "WWzSZIJ36xtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # run the model and repeat the evaluation to reduce the certainty asscoicated with the random initialization of model weights\n",
        "# def run_model(n_repeats = 1):\n",
        "#     scores = [fit_lstm_random() for _ in range(n_repeats)]\n",
        "#     result = pd.DataFrame(scores)\n",
        "#     result = result.groupby(\"best_params\").mean()\n",
        "#     return result"
      ],
      "metadata": {
        "id": "Zpnlrc0t52h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit_lstm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QpKzZlx53Qd",
        "outputId": "59a13f43-49f9-4a21-f6b0-894d0c3513e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_epochs 500 num_hidden_layers 1 num_neurons 4 batch_size 2 window_size 2\n",
            "train.shape: (1524, 7) test.shape: (381, 7)\n",
            "train_scaled.shape: (1524, 7) test_scaled.shape: (381, 7)\n",
            "train_X.shape: (1524, 2, 3) train_y.shape: (1524, 1) test_X.shape: (381, 2, 3) test_y.shape: (381, 1)\n",
            "Epoch 1/500\n",
            "762/762 - 4s - loss: 0.0650 - val_loss: 4.9336e-04 - 4s/epoch - 6ms/step\n",
            "Epoch 2/500\n",
            "762/762 - 2s - loss: 0.0241 - val_loss: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "762/762 - 1s - loss: 0.0143 - val_loss: 0.0026 - 1s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "762/762 - 1s - loss: 0.0122 - val_loss: 0.0017 - 1s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "762/762 - 2s - loss: 0.0112 - val_loss: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "762/762 - 2s - loss: 0.0104 - val_loss: 7.2967e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "762/762 - 1s - loss: 0.0099 - val_loss: 5.4199e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "762/762 - 2s - loss: 0.0095 - val_loss: 4.4359e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "762/762 - 2s - loss: 0.0092 - val_loss: 3.9415e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "762/762 - 2s - loss: 0.0089 - val_loss: 3.7046e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "762/762 - 2s - loss: 0.0087 - val_loss: 3.5962e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "762/762 - 2s - loss: 0.0086 - val_loss: 3.5467e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "762/762 - 2s - loss: 0.0084 - val_loss: 3.5217e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "762/762 - 2s - loss: 0.0083 - val_loss: 3.5055e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "762/762 - 1s - loss: 0.0082 - val_loss: 3.4910e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "762/762 - 2s - loss: 0.0081 - val_loss: 3.4753e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "762/762 - 2s - loss: 0.0079 - val_loss: 3.4574e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "762/762 - 2s - loss: 0.0078 - val_loss: 3.4370e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "762/762 - 2s - loss: 0.0077 - val_loss: 3.4137e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "762/762 - 2s - loss: 0.0076 - val_loss: 3.3874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "762/762 - 2s - loss: 0.0075 - val_loss: 3.3584e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "762/762 - 2s - loss: 0.0074 - val_loss: 3.3266e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "762/762 - 2s - loss: 0.0073 - val_loss: 3.2925e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "762/762 - 2s - loss: 0.0071 - val_loss: 3.2563e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "762/762 - 2s - loss: 0.0070 - val_loss: 3.2186e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "762/762 - 2s - loss: 0.0069 - val_loss: 3.1799e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "762/762 - 2s - loss: 0.0067 - val_loss: 3.1407e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "762/762 - 2s - loss: 0.0066 - val_loss: 3.1015e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "762/762 - 2s - loss: 0.0065 - val_loss: 3.0628e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "762/762 - 1s - loss: 0.0063 - val_loss: 3.0250e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "762/762 - 2s - loss: 0.0062 - val_loss: 2.9887e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 32/500\n",
            "762/762 - 2s - loss: 0.0060 - val_loss: 2.9540e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 33/500\n",
            "762/762 - 2s - loss: 0.0059 - val_loss: 2.9215e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 34/500\n",
            "762/762 - 2s - loss: 0.0057 - val_loss: 2.8914e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 35/500\n",
            "762/762 - 1s - loss: 0.0055 - val_loss: 2.8640e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "762/762 - 1s - loss: 0.0054 - val_loss: 2.8397e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "762/762 - 2s - loss: 0.0052 - val_loss: 2.8187e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "762/762 - 1s - loss: 0.0050 - val_loss: 2.8013e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "762/762 - 2s - loss: 0.0048 - val_loss: 2.7878e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "762/762 - 2s - loss: 0.0047 - val_loss: 2.7786e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "762/762 - 2s - loss: 0.0045 - val_loss: 2.7740e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "762/762 - 2s - loss: 0.0043 - val_loss: 2.7743e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "762/762 - 2s - loss: 0.0041 - val_loss: 2.7796e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 44/500\n",
            "762/762 - 1s - loss: 0.0039 - val_loss: 2.7903e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "762/762 - 2s - loss: 0.0038 - val_loss: 2.8065e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 46/500\n",
            "762/762 - 2s - loss: 0.0036 - val_loss: 2.8281e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 47/500\n",
            "762/762 - 2s - loss: 0.0034 - val_loss: 2.8552e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 48/500\n",
            "762/762 - 2s - loss: 0.0033 - val_loss: 2.8875e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 49/500\n",
            "762/762 - 2s - loss: 0.0031 - val_loss: 2.9242e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 50/500\n",
            "762/762 - 3s - loss: 0.0030 - val_loss: 2.9643e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 51/500\n",
            "762/762 - 2s - loss: 0.0029 - val_loss: 3.0066e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 52/500\n",
            "762/762 - 2s - loss: 0.0028 - val_loss: 3.0497e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 53/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 3.0920e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 54/500\n",
            "762/762 - 1s - loss: 0.0026 - val_loss: 3.1320e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "762/762 - 2s - loss: 0.0025 - val_loss: 3.1686e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 56/500\n",
            "762/762 - 1s - loss: 0.0025 - val_loss: 3.2005e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 57/500\n",
            "762/762 - 1s - loss: 0.0024 - val_loss: 3.2270e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 58/500\n",
            "762/762 - 2s - loss: 0.0024 - val_loss: 3.2474e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 59/500\n",
            "762/762 - 1s - loss: 0.0024 - val_loss: 3.2615e-04 - 1s/epoch - 2ms/step\n",
            "Epoch 60/500\n",
            "762/762 - 2s - loss: 0.0023 - val_loss: 3.2693e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 61/500\n",
            "Restoring model weights from the end of the best epoch: 41.\n",
            "762/762 - 2s - loss: 0.0023 - val_loss: 3.2710e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 61: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 4 batch_size 2 window_size 3\n",
            "train.shape: (1523, 10) test.shape: (381, 10)\n",
            "train_scaled.shape: (1523, 10) test_scaled.shape: (381, 10)\n",
            "train_X.shape: (1523, 3, 3) train_y.shape: (1523, 1) test_X.shape: (381, 3, 3) test_y.shape: (381, 1)\n",
            "Epoch 1/500\n",
            "762/762 - 59s - loss: 0.1119 - val_loss: 0.0027 - 59s/epoch - 77ms/step\n",
            "Epoch 2/500\n",
            "762/762 - 2s - loss: 0.0424 - val_loss: 0.0104 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "762/762 - 2s - loss: 0.0242 - val_loss: 0.0081 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "762/762 - 2s - loss: 0.0191 - val_loss: 0.0047 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "762/762 - 2s - loss: 0.0162 - val_loss: 0.0025 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "762/762 - 2s - loss: 0.0142 - val_loss: 0.0014 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "762/762 - 2s - loss: 0.0129 - val_loss: 8.7635e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "762/762 - 2s - loss: 0.0121 - val_loss: 6.4639e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "762/762 - 2s - loss: 0.0116 - val_loss: 5.5319e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "762/762 - 2s - loss: 0.0112 - val_loss: 5.1659e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 11/500\n",
            "762/762 - 2s - loss: 0.0110 - val_loss: 5.0138e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 12/500\n",
            "762/762 - 2s - loss: 0.0108 - val_loss: 4.9341e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 13/500\n",
            "762/762 - 2s - loss: 0.0106 - val_loss: 4.8752e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 14/500\n",
            "762/762 - 2s - loss: 0.0105 - val_loss: 4.8212e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 15/500\n",
            "762/762 - 2s - loss: 0.0103 - val_loss: 4.7691e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 16/500\n",
            "762/762 - 2s - loss: 0.0102 - val_loss: 4.7195e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 17/500\n",
            "762/762 - 2s - loss: 0.0101 - val_loss: 4.6737e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 18/500\n",
            "762/762 - 2s - loss: 0.0100 - val_loss: 4.6327e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 19/500\n",
            "762/762 - 2s - loss: 0.0099 - val_loss: 4.5966e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 20/500\n",
            "762/762 - 2s - loss: 0.0098 - val_loss: 4.5655e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 21/500\n",
            "762/762 - 2s - loss: 0.0097 - val_loss: 4.5388e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 22/500\n",
            "762/762 - 2s - loss: 0.0096 - val_loss: 4.5161e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 23/500\n",
            "762/762 - 2s - loss: 0.0095 - val_loss: 4.4965e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 24/500\n",
            "762/762 - 2s - loss: 0.0094 - val_loss: 4.4793e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 25/500\n",
            "762/762 - 2s - loss: 0.0093 - val_loss: 4.4637e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 26/500\n",
            "762/762 - 2s - loss: 0.0092 - val_loss: 4.4489e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 27/500\n",
            "762/762 - 2s - loss: 0.0091 - val_loss: 4.4340e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 28/500\n",
            "762/762 - 2s - loss: 0.0090 - val_loss: 4.4182e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 29/500\n",
            "762/762 - 2s - loss: 0.0089 - val_loss: 4.4009e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 30/500\n",
            "762/762 - 2s - loss: 0.0088 - val_loss: 4.3812e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 31/500\n",
            "762/762 - 2s - loss: 0.0087 - val_loss: 4.3584e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 32/500\n",
            "762/762 - 2s - loss: 0.0085 - val_loss: 4.3319e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 33/500\n",
            "762/762 - 2s - loss: 0.0084 - val_loss: 4.3013e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 34/500\n",
            "762/762 - 2s - loss: 0.0083 - val_loss: 4.2660e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 35/500\n",
            "762/762 - 2s - loss: 0.0082 - val_loss: 4.2258e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 36/500\n",
            "762/762 - 2s - loss: 0.0080 - val_loss: 4.1804e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 37/500\n",
            "762/762 - 2s - loss: 0.0079 - val_loss: 4.1299e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 38/500\n",
            "762/762 - 2s - loss: 0.0077 - val_loss: 4.0742e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 39/500\n",
            "762/762 - 2s - loss: 0.0076 - val_loss: 4.0134e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 40/500\n",
            "762/762 - 2s - loss: 0.0074 - val_loss: 3.9480e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 41/500\n",
            "762/762 - 2s - loss: 0.0073 - val_loss: 3.8782e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 42/500\n",
            "762/762 - 2s - loss: 0.0071 - val_loss: 3.8044e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 43/500\n",
            "762/762 - 2s - loss: 0.0069 - val_loss: 3.7271e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 44/500\n",
            "762/762 - 2s - loss: 0.0068 - val_loss: 3.6467e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 45/500\n",
            "762/762 - 2s - loss: 0.0066 - val_loss: 3.5637e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 46/500\n",
            "762/762 - 2s - loss: 0.0064 - val_loss: 3.4786e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 47/500\n",
            "762/762 - 2s - loss: 0.0062 - val_loss: 3.3920e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 48/500\n",
            "762/762 - 2s - loss: 0.0060 - val_loss: 3.3042e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 49/500\n",
            "762/762 - 2s - loss: 0.0058 - val_loss: 3.2159e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 50/500\n",
            "762/762 - 2s - loss: 0.0056 - val_loss: 3.1275e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 51/500\n",
            "762/762 - 2s - loss: 0.0055 - val_loss: 3.0397e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 52/500\n",
            "762/762 - 2s - loss: 0.0053 - val_loss: 2.9528e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 53/500\n",
            "762/762 - 2s - loss: 0.0051 - val_loss: 2.8675e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 54/500\n",
            "762/762 - 2s - loss: 0.0049 - val_loss: 2.7843e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 55/500\n",
            "762/762 - 2s - loss: 0.0047 - val_loss: 2.7037e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 56/500\n",
            "762/762 - 2s - loss: 0.0045 - val_loss: 2.6263e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 57/500\n",
            "762/762 - 2s - loss: 0.0044 - val_loss: 2.5526e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 58/500\n",
            "762/762 - 2s - loss: 0.0042 - val_loss: 2.4830e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 59/500\n",
            "762/762 - 2s - loss: 0.0041 - val_loss: 2.4179e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 60/500\n",
            "762/762 - 2s - loss: 0.0040 - val_loss: 2.3577e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 61/500\n",
            "762/762 - 2s - loss: 0.0038 - val_loss: 2.3026e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 62/500\n",
            "762/762 - 2s - loss: 0.0037 - val_loss: 2.2527e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 63/500\n",
            "762/762 - 2s - loss: 0.0036 - val_loss: 2.2080e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 64/500\n",
            "762/762 - 2s - loss: 0.0035 - val_loss: 2.1686e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 65/500\n",
            "762/762 - 2s - loss: 0.0034 - val_loss: 2.1342e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 66/500\n",
            "762/762 - 2s - loss: 0.0034 - val_loss: 2.1047e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 67/500\n",
            "762/762 - 2s - loss: 0.0033 - val_loss: 2.0798e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 68/500\n",
            "762/762 - 2s - loss: 0.0032 - val_loss: 2.0591e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 69/500\n",
            "762/762 - 2s - loss: 0.0032 - val_loss: 2.0422e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 70/500\n",
            "762/762 - 2s - loss: 0.0031 - val_loss: 2.0288e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 71/500\n",
            "762/762 - 2s - loss: 0.0031 - val_loss: 2.0184e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 72/500\n",
            "762/762 - 3s - loss: 0.0031 - val_loss: 2.0108e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 73/500\n",
            "762/762 - 2s - loss: 0.0030 - val_loss: 2.0054e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 74/500\n",
            "762/762 - 2s - loss: 0.0030 - val_loss: 2.0020e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 75/500\n",
            "762/762 - 2s - loss: 0.0030 - val_loss: 2.0003e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 76/500\n",
            "762/762 - 2s - loss: 0.0029 - val_loss: 1.9998e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 77/500\n",
            "762/762 - 2s - loss: 0.0029 - val_loss: 2.0004e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 78/500\n",
            "762/762 - 2s - loss: 0.0029 - val_loss: 2.0017e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 79/500\n",
            "762/762 - 2s - loss: 0.0029 - val_loss: 2.0037e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 80/500\n",
            "762/762 - 3s - loss: 0.0029 - val_loss: 2.0061e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 81/500\n",
            "762/762 - 4s - loss: 0.0028 - val_loss: 2.0088e-04 - 4s/epoch - 5ms/step\n",
            "Epoch 82/500\n",
            "762/762 - 3s - loss: 0.0028 - val_loss: 2.0116e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 83/500\n",
            "762/762 - 2s - loss: 0.0028 - val_loss: 2.0143e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 84/500\n",
            "762/762 - 2s - loss: 0.0028 - val_loss: 2.0171e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 85/500\n",
            "762/762 - 2s - loss: 0.0028 - val_loss: 2.0196e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 86/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.0219e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 87/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.0240e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 88/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.0258e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 89/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.0272e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 90/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.0284e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 91/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.0291e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 92/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.0296e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 93/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.0297e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 94/500\n",
            "762/762 - 2s - loss: 0.0026 - val_loss: 2.0294e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 95/500\n",
            "762/762 - 2s - loss: 0.0026 - val_loss: 2.0289e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 96/500\n",
            "Restoring model weights from the end of the best epoch: 76.\n",
            "762/762 - 2s - loss: 0.0026 - val_loss: 2.0281e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 96: early stopping\n",
            "final best params n_epochs: 500 num_hidden_layers: 1 num_neurons: 4 batch_size: 2 window_size: 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_params': '[500, 1, 4, 2, 3]', 'MSE': 0.00019997876370325685}"
            ]
          },
          "metadata": {},
          "execution_count": 393
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import randint, rand"
      ],
      "metadata": {
        "id": "l7agEK2HvLKn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initial population of random bitstring\n",
        "n_bits = 8 # number of bits for each individual\n",
        "n_pop= 2 # number of experiments\n",
        "pop = [randint(0, 2, n_bits) for _ in range(n_pop)]\n",
        "# define range for input\n",
        "bounds = [[2, 32]]\n",
        "\n",
        "# initial population of random bitstring\n",
        "pop = [randint(0, 2, n_bits*len(bounds)).tolist() for _ in range(n_pop)]\n",
        "pop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO28Zsn3_5DO",
        "outputId": "e6c630c9-8421-4191-975b-15043c7573f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 0, 1, 0, 0, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# decode population\n",
        "decoded = [decode(bounds, n_bits, p) for p in pop]\n",
        "decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwh7hICuBXex",
        "outputId": "e648b92c-39ab-47a2-abe4-5cb0104ffbf0"
      },
      "execution_count": 570,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[27], [17]]"
            ]
          },
          "metadata": {},
          "execution_count": 570
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode bitstring to numbers\n",
        "def decode(bounds, n_bits, bitstring):\n",
        "    decoded = list()\n",
        "    largest = 2**n_bits\n",
        "    for i in range(len(bounds)):\n",
        "        # extract the substring\n",
        "        start, end = i * n_bits, (i * n_bits)+n_bits\n",
        "        substring = bitstring[start:end]\n",
        "        # convert bitstring to a string of chars\n",
        "        chars = ''.join([str(s) for s in substring])\n",
        "        # convert string to integer\n",
        "        integer = int(chars, 2)\n",
        "        # scale integer to desired range\n",
        "        value = bounds[i][0] + (integer/largest) * (bounds[i][1] - bounds[i][0])\n",
        "        value = np.round(value)\n",
        "        value = int(value)\n",
        "        # store\n",
        "        decoded.append(value)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "cVok8ZpzBIcj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate all candidates in the population\n",
        "scores = [evaluate(d) for d in decoded]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dndiGGngE6S-",
        "outputId": "c4a592e7-ed0a-407a-960c-fc73bfa3b2d1"
      },
      "execution_count": 563,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "381/381 - 3s - loss: 0.1639 - val_loss: 8.4715e-04 - 3s/epoch - 9ms/step\n",
            "Epoch 2/10\n",
            "381/381 - 1s - loss: 0.0519 - val_loss: 0.0071 - 1s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "381/381 - 1s - loss: 0.0217 - val_loss: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "381/381 - 1s - loss: 0.0175 - val_loss: 0.0053 - 979ms/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "381/381 - 1s - loss: 0.0156 - val_loss: 0.0037 - 1s/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "381/381 - 1s - loss: 0.0142 - val_loss: 0.0027 - 985ms/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "381/381 - 1s - loss: 0.0131 - val_loss: 0.0020 - 996ms/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "381/381 - 1s - loss: 0.0122 - val_loss: 0.0015 - 1s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "381/381 - 1s - loss: 0.0116 - val_loss: 0.0012 - 1s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "381/381 - 1s - loss: 0.0111 - val_loss: 0.0010 - 1s/epoch - 3ms/step\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 1/10\n",
            "381/381 - 3s - loss: 0.1973 - val_loss: 0.0039 - 3s/epoch - 8ms/step\n",
            "Epoch 2/10\n",
            "381/381 - 1s - loss: 0.0967 - val_loss: 0.0018 - 1s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "381/381 - 1s - loss: 0.0410 - val_loss: 0.0068 - 989ms/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "381/381 - 1s - loss: 0.0233 - val_loss: 0.0089 - 1s/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "381/381 - 1s - loss: 0.0189 - val_loss: 0.0080 - 975ms/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "381/381 - 1s - loss: 0.0172 - val_loss: 0.0064 - 994ms/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "381/381 - 1s - loss: 0.0161 - val_loss: 0.0050 - 965ms/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "381/381 - 1s - loss: 0.0152 - val_loss: 0.0039 - 1s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "381/381 - 1s - loss: 0.0144 - val_loss: 0.0030 - 984ms/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "381/381 - 1s - loss: 0.0137 - val_loss: 0.0023 - 973ms/epoch - 3ms/step\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRnMyl69B6OX",
        "outputId": "3996e103-dbc4-4539-9fc5-04b25b63f34d"
      },
      "execution_count": 564,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0010407392401248217, 0.002341017359867692]"
            ]
          },
          "metadata": {},
          "execution_count": 564
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keep track of best solution\n",
        "best, best_eval = 0, evaluate(pop[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw71vfzDIMp-",
        "outputId": "d51b4787-9598-4b3d-e704-1faa05bcd9cc"
      },
      "execution_count": 567,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "381/381 - 4s - loss: 0.1177 - val_loss: 0.0022 - 4s/epoch - 11ms/step\n",
            "Epoch 2/10\n",
            "381/381 - 1s - loss: 0.0839 - val_loss: 4.8181e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 3/10\n",
            "381/381 - 1s - loss: 0.0565 - val_loss: 0.0023 - 994ms/epoch - 3ms/step\n",
            "Epoch 4/10\n",
            "381/381 - 1s - loss: 0.0395 - val_loss: 0.0055 - 991ms/epoch - 3ms/step\n",
            "Epoch 5/10\n",
            "381/381 - 1s - loss: 0.0304 - val_loss: 0.0081 - 954ms/epoch - 3ms/step\n",
            "Epoch 6/10\n",
            "381/381 - 1s - loss: 0.0259 - val_loss: 0.0093 - 992ms/epoch - 3ms/step\n",
            "Epoch 7/10\n",
            "381/381 - 1s - loss: 0.0235 - val_loss: 0.0093 - 1s/epoch - 3ms/step\n",
            "Epoch 8/10\n",
            "381/381 - 1s - loss: 0.0220 - val_loss: 0.0086 - 1s/epoch - 3ms/step\n",
            "Epoch 9/10\n",
            "381/381 - 1s - loss: 0.0209 - val_loss: 0.0075 - 1s/epoch - 3ms/step\n",
            "Epoch 10/10\n",
            "381/381 - 1s - loss: 0.0199 - val_loss: 0.0064 - 1s/epoch - 3ms/step\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.0064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2-1):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQzqqXJDIbMF",
        "outputId": "fc22c443-5ae6-4385-9835-608cc45e08ca"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "5K75dSDCEmA9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seacrh space\n",
        "n_epochs = [500]\n",
        "num_hidden_layers = [1, 2]\n",
        "num_neurons = [4, 8, 16, 32, 64]\n",
        "batch_size = [2, 4]\n",
        "window_size = [2, 3, 4, 5, 6]"
      ],
      "metadata": {
        "id": "9hqj1BYPb5I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    #feature engineering\n",
        "    window_size=7\n",
        "    steps_ahead=1\n",
        "    # # convert the stationary series to supervise learning\n",
        "    series_supervised = series_to_supervised(series, series.columns, n_in= window_size, n_out= steps_ahead, dropnan=True) \n",
        "    series_supervised"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "XoimAPHDoflC",
        "outputId": "e39fcd68-e1ba-4e98-a480-cb5f5556eda3"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            interaction_effect_onNext_oil1(t-7)  BORE_GAS_VOL(t-7)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-08                         2.321410e+06       1.462166e+07   \n",
              "2010-01-09                         2.181563e+06       1.469266e+07   \n",
              "2010-01-10                         2.045042e+06       1.400904e+07   \n",
              "2010-01-11                         2.091405e+06       1.341015e+07   \n",
              "2010-01-12                         2.104483e+06       1.361768e+07   \n",
              "...                                         ...                ...   \n",
              "2015-03-18                         3.120000e+04       1.366879e+06   \n",
              "2015-03-19                         2.880000e+04       1.365665e+06   \n",
              "2015-03-20                         2.640000e+04       1.363799e+06   \n",
              "2015-03-21                         2.400000e+04       1.362575e+06   \n",
              "2015-03-22                         2.160000e+04       1.337302e+06   \n",
              "\n",
              "            BORE_OIL_VOL(t-7)  interaction_effect_onNext_oil1(t-6)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-08       18593.749401                         2.181563e+06   \n",
              "2010-01-09       18701.242265                         2.045042e+06   \n",
              "2010-01-10       17799.912406                         2.091405e+06   \n",
              "2010-01-11       17002.616014                         2.104483e+06   \n",
              "2010-01-12       17270.939334                         2.043370e+06   \n",
              "...                       ...                                  ...   \n",
              "2015-03-18        1643.024325                         2.880000e+04   \n",
              "2015-03-19        1639.439133                         2.640000e+04   \n",
              "2015-03-20        1618.116675                         2.400000e+04   \n",
              "2015-03-21        1627.740085                         2.160000e+04   \n",
              "2015-03-22        1613.713808                         1.920000e+04   \n",
              "\n",
              "            BORE_GAS_VOL(t-6)  BORE_OIL_VOL(t-6)  \\\n",
              "DATEPRD                                            \n",
              "2010-01-08       1.469266e+07       18701.242265   \n",
              "2010-01-09       1.400904e+07       17799.912406   \n",
              "2010-01-10       1.341015e+07       17002.616014   \n",
              "2010-01-11       1.361768e+07       17270.939334   \n",
              "2010-01-12       1.364834e+07       17331.761803   \n",
              "...                       ...                ...   \n",
              "2015-03-18       1.365665e+06        1639.439133   \n",
              "2015-03-19       1.363799e+06        1618.116675   \n",
              "2015-03-20       1.362575e+06        1627.740085   \n",
              "2015-03-21       1.337302e+06        1613.713808   \n",
              "2015-03-22       1.344981e+06        1630.318908   \n",
              "\n",
              "            interaction_effect_onNext_oil1(t-5)  BORE_GAS_VOL(t-5)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-08                         2.045042e+06       1.400904e+07   \n",
              "2010-01-09                         2.091405e+06       1.341015e+07   \n",
              "2010-01-10                         2.104483e+06       1.361768e+07   \n",
              "2010-01-11                         2.043370e+06       1.364834e+07   \n",
              "2010-01-12                         2.045269e+06       1.350971e+07   \n",
              "...                                         ...                ...   \n",
              "2015-03-18                         2.640000e+04       1.363799e+06   \n",
              "2015-03-19                         2.400000e+04       1.362575e+06   \n",
              "2015-03-20                         2.160000e+04       1.337302e+06   \n",
              "2015-03-21                         1.920000e+04       1.344981e+06   \n",
              "2015-03-22                         1.680000e+04       1.348689e+06   \n",
              "\n",
              "            BORE_OIL_VOL(t-5)  interaction_effect_onNext_oil1(t-4)  ...  \\\n",
              "DATEPRD                                                             ...   \n",
              "2010-01-08       17799.912406                         2.091405e+06  ...   \n",
              "2010-01-09       17002.616014                         2.104483e+06  ...   \n",
              "2010-01-10       17270.939334                         2.043370e+06  ...   \n",
              "2010-01-11       17331.761803                         2.045269e+06  ...   \n",
              "2010-01-12       17138.601719                         1.724474e+06  ...   \n",
              "...                       ...                                  ...  ...   \n",
              "2015-03-18        1618.116675                         2.400000e+04  ...   \n",
              "2015-03-19        1627.740085                         2.160000e+04  ...   \n",
              "2015-03-20        1613.713808                         1.920000e+04  ...   \n",
              "2015-03-21        1630.318908                         1.680000e+04  ...   \n",
              "2015-03-22        1649.125441                         1.440000e+04  ...   \n",
              "\n",
              "            BORE_OIL_VOL(t-3)  interaction_effect_onNext_oil1(t-2)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-08       17270.939334                         2.043370e+06   \n",
              "2010-01-09       17331.761803                         2.045269e+06   \n",
              "2010-01-10       17138.601719                         1.724474e+06   \n",
              "2010-01-11       17127.657449                         2.049433e+06   \n",
              "2010-01-12       14477.823141                         2.090917e+06   \n",
              "...                       ...                                  ...   \n",
              "2015-03-18        1613.713808                         1.920000e+04   \n",
              "2015-03-19        1630.318908                         1.680000e+04   \n",
              "2015-03-20        1649.125441                         1.440000e+04   \n",
              "2015-03-21        1659.440731                         1.200000e+04   \n",
              "2015-03-22        1662.711432                         9.600000e+03   \n",
              "\n",
              "            BORE_GAS_VOL(t-2)  BORE_OIL_VOL(t-2)  \\\n",
              "DATEPRD                                            \n",
              "2010-01-08       1.364834e+07       17331.761803   \n",
              "2010-01-09       1.350971e+07       17138.601719   \n",
              "2010-01-10       1.349732e+07       17127.657449   \n",
              "2010-01-11       1.192758e+07       14477.823141   \n",
              "2010-01-12       1.350295e+07       17149.483091   \n",
              "...                       ...                ...   \n",
              "2015-03-18       1.344981e+06        1630.318908   \n",
              "2015-03-19       1.348689e+06        1649.125441   \n",
              "2015-03-20       1.354500e+06        1659.440731   \n",
              "2015-03-21       1.366424e+06        1662.711432   \n",
              "2015-03-22       1.397308e+06        1707.494884   \n",
              "\n",
              "            interaction_effect_onNext_oil1(t-1)  BORE_GAS_VOL(t-1)  \\\n",
              "DATEPRD                                                              \n",
              "2010-01-08                         2.045269e+06       1.350971e+07   \n",
              "2010-01-09                         1.724474e+06       1.349732e+07   \n",
              "2010-01-10                         2.049433e+06       1.192758e+07   \n",
              "2010-01-11                         2.090917e+06       1.350295e+07   \n",
              "2010-01-12                         2.095638e+06       1.343582e+07   \n",
              "...                                         ...                ...   \n",
              "2015-03-18                         1.680000e+04       1.348689e+06   \n",
              "2015-03-19                         1.440000e+04       1.354500e+06   \n",
              "2015-03-20                         1.200000e+04       1.366424e+06   \n",
              "2015-03-21                         9.600000e+03       1.397308e+06   \n",
              "2015-03-22                         7.200000e+03       1.408435e+06   \n",
              "\n",
              "            BORE_OIL_VOL(t-1)  interaction_effect_onNext_oil1(t)  \\\n",
              "DATEPRD                                                            \n",
              "2010-01-08       17138.601719                       1.724474e+06   \n",
              "2010-01-09       17127.657449                       2.049433e+06   \n",
              "2010-01-10       14477.823141                       2.090917e+06   \n",
              "2010-01-11       17149.483091                       2.095638e+06   \n",
              "2010-01-12       17073.250587                       2.109894e+06   \n",
              "...                       ...                                ...   \n",
              "2015-03-18        1649.125441                       1.440000e+04   \n",
              "2015-03-19        1659.440731                       1.200000e+04   \n",
              "2015-03-20        1662.711432                       9.600000e+03   \n",
              "2015-03-21        1707.494884                       7.200000e+03   \n",
              "2015-03-22        1725.420844                       4.799716e+03   \n",
              "\n",
              "            BORE_GAS_VOL(t)  BORE_OIL_VOL(t)  \n",
              "DATEPRD                                       \n",
              "2010-01-08     1.349732e+07     17127.657449  \n",
              "2010-01-09     1.192758e+07     14477.823141  \n",
              "2010-01-10     1.350295e+07     17149.483091  \n",
              "2010-01-11     1.343582e+07     17073.250587  \n",
              "2010-01-12     1.339754e+07     17056.834181  \n",
              "...                     ...              ...  \n",
              "2015-03-18     1.354500e+06      1659.440731  \n",
              "2015-03-19     1.366424e+06      1662.711432  \n",
              "2015-03-20     1.397308e+06      1707.494884  \n",
              "2015-03-21     1.408435e+06      1725.420844  \n",
              "2015-03-22     1.379366e+06      1620.632599  \n",
              "\n",
              "[1900 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ea61087-50ce-4e25-9838-b018532588e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>interaction_effect_onNext_oil1(t-7)</th>\n",
              "      <th>BORE_GAS_VOL(t-7)</th>\n",
              "      <th>BORE_OIL_VOL(t-7)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t-6)</th>\n",
              "      <th>BORE_GAS_VOL(t-6)</th>\n",
              "      <th>BORE_OIL_VOL(t-6)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t-5)</th>\n",
              "      <th>BORE_GAS_VOL(t-5)</th>\n",
              "      <th>BORE_OIL_VOL(t-5)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t-4)</th>\n",
              "      <th>...</th>\n",
              "      <th>BORE_OIL_VOL(t-3)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t-2)</th>\n",
              "      <th>BORE_GAS_VOL(t-2)</th>\n",
              "      <th>BORE_OIL_VOL(t-2)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t-1)</th>\n",
              "      <th>BORE_GAS_VOL(t-1)</th>\n",
              "      <th>BORE_OIL_VOL(t-1)</th>\n",
              "      <th>interaction_effect_onNext_oil1(t)</th>\n",
              "      <th>BORE_GAS_VOL(t)</th>\n",
              "      <th>BORE_OIL_VOL(t)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATEPRD</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>2.321410e+06</td>\n",
              "      <td>1.462166e+07</td>\n",
              "      <td>18593.749401</td>\n",
              "      <td>2.181563e+06</td>\n",
              "      <td>1.469266e+07</td>\n",
              "      <td>18701.242265</td>\n",
              "      <td>2.045042e+06</td>\n",
              "      <td>1.400904e+07</td>\n",
              "      <td>17799.912406</td>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>17270.939334</td>\n",
              "      <td>2.043370e+06</td>\n",
              "      <td>1.364834e+07</td>\n",
              "      <td>17331.761803</td>\n",
              "      <td>2.045269e+06</td>\n",
              "      <td>1.350971e+07</td>\n",
              "      <td>17138.601719</td>\n",
              "      <td>1.724474e+06</td>\n",
              "      <td>1.349732e+07</td>\n",
              "      <td>17127.657449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-09</th>\n",
              "      <td>2.181563e+06</td>\n",
              "      <td>1.469266e+07</td>\n",
              "      <td>18701.242265</td>\n",
              "      <td>2.045042e+06</td>\n",
              "      <td>1.400904e+07</td>\n",
              "      <td>17799.912406</td>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>1.341015e+07</td>\n",
              "      <td>17002.616014</td>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>17331.761803</td>\n",
              "      <td>2.045269e+06</td>\n",
              "      <td>1.350971e+07</td>\n",
              "      <td>17138.601719</td>\n",
              "      <td>1.724474e+06</td>\n",
              "      <td>1.349732e+07</td>\n",
              "      <td>17127.657449</td>\n",
              "      <td>2.049433e+06</td>\n",
              "      <td>1.192758e+07</td>\n",
              "      <td>14477.823141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-10</th>\n",
              "      <td>2.045042e+06</td>\n",
              "      <td>1.400904e+07</td>\n",
              "      <td>17799.912406</td>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>1.341015e+07</td>\n",
              "      <td>17002.616014</td>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>1.361768e+07</td>\n",
              "      <td>17270.939334</td>\n",
              "      <td>2.043370e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>17138.601719</td>\n",
              "      <td>1.724474e+06</td>\n",
              "      <td>1.349732e+07</td>\n",
              "      <td>17127.657449</td>\n",
              "      <td>2.049433e+06</td>\n",
              "      <td>1.192758e+07</td>\n",
              "      <td>14477.823141</td>\n",
              "      <td>2.090917e+06</td>\n",
              "      <td>1.350295e+07</td>\n",
              "      <td>17149.483091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-11</th>\n",
              "      <td>2.091405e+06</td>\n",
              "      <td>1.341015e+07</td>\n",
              "      <td>17002.616014</td>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>1.361768e+07</td>\n",
              "      <td>17270.939334</td>\n",
              "      <td>2.043370e+06</td>\n",
              "      <td>1.364834e+07</td>\n",
              "      <td>17331.761803</td>\n",
              "      <td>2.045269e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>17127.657449</td>\n",
              "      <td>2.049433e+06</td>\n",
              "      <td>1.192758e+07</td>\n",
              "      <td>14477.823141</td>\n",
              "      <td>2.090917e+06</td>\n",
              "      <td>1.350295e+07</td>\n",
              "      <td>17149.483091</td>\n",
              "      <td>2.095638e+06</td>\n",
              "      <td>1.343582e+07</td>\n",
              "      <td>17073.250587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-12</th>\n",
              "      <td>2.104483e+06</td>\n",
              "      <td>1.361768e+07</td>\n",
              "      <td>17270.939334</td>\n",
              "      <td>2.043370e+06</td>\n",
              "      <td>1.364834e+07</td>\n",
              "      <td>17331.761803</td>\n",
              "      <td>2.045269e+06</td>\n",
              "      <td>1.350971e+07</td>\n",
              "      <td>17138.601719</td>\n",
              "      <td>1.724474e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>14477.823141</td>\n",
              "      <td>2.090917e+06</td>\n",
              "      <td>1.350295e+07</td>\n",
              "      <td>17149.483091</td>\n",
              "      <td>2.095638e+06</td>\n",
              "      <td>1.343582e+07</td>\n",
              "      <td>17073.250587</td>\n",
              "      <td>2.109894e+06</td>\n",
              "      <td>1.339754e+07</td>\n",
              "      <td>17056.834181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-18</th>\n",
              "      <td>3.120000e+04</td>\n",
              "      <td>1.366879e+06</td>\n",
              "      <td>1643.024325</td>\n",
              "      <td>2.880000e+04</td>\n",
              "      <td>1.365665e+06</td>\n",
              "      <td>1639.439133</td>\n",
              "      <td>2.640000e+04</td>\n",
              "      <td>1.363799e+06</td>\n",
              "      <td>1618.116675</td>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>1613.713808</td>\n",
              "      <td>1.920000e+04</td>\n",
              "      <td>1.344981e+06</td>\n",
              "      <td>1630.318908</td>\n",
              "      <td>1.680000e+04</td>\n",
              "      <td>1.348689e+06</td>\n",
              "      <td>1649.125441</td>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>1.354500e+06</td>\n",
              "      <td>1659.440731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-19</th>\n",
              "      <td>2.880000e+04</td>\n",
              "      <td>1.365665e+06</td>\n",
              "      <td>1639.439133</td>\n",
              "      <td>2.640000e+04</td>\n",
              "      <td>1.363799e+06</td>\n",
              "      <td>1618.116675</td>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>1.362575e+06</td>\n",
              "      <td>1627.740085</td>\n",
              "      <td>2.160000e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>1630.318908</td>\n",
              "      <td>1.680000e+04</td>\n",
              "      <td>1.348689e+06</td>\n",
              "      <td>1649.125441</td>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>1.354500e+06</td>\n",
              "      <td>1659.440731</td>\n",
              "      <td>1.200000e+04</td>\n",
              "      <td>1.366424e+06</td>\n",
              "      <td>1662.711432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-20</th>\n",
              "      <td>2.640000e+04</td>\n",
              "      <td>1.363799e+06</td>\n",
              "      <td>1618.116675</td>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>1.362575e+06</td>\n",
              "      <td>1627.740085</td>\n",
              "      <td>2.160000e+04</td>\n",
              "      <td>1.337302e+06</td>\n",
              "      <td>1613.713808</td>\n",
              "      <td>1.920000e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>1649.125441</td>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>1.354500e+06</td>\n",
              "      <td>1659.440731</td>\n",
              "      <td>1.200000e+04</td>\n",
              "      <td>1.366424e+06</td>\n",
              "      <td>1662.711432</td>\n",
              "      <td>9.600000e+03</td>\n",
              "      <td>1.397308e+06</td>\n",
              "      <td>1707.494884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-21</th>\n",
              "      <td>2.400000e+04</td>\n",
              "      <td>1.362575e+06</td>\n",
              "      <td>1627.740085</td>\n",
              "      <td>2.160000e+04</td>\n",
              "      <td>1.337302e+06</td>\n",
              "      <td>1613.713808</td>\n",
              "      <td>1.920000e+04</td>\n",
              "      <td>1.344981e+06</td>\n",
              "      <td>1630.318908</td>\n",
              "      <td>1.680000e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>1659.440731</td>\n",
              "      <td>1.200000e+04</td>\n",
              "      <td>1.366424e+06</td>\n",
              "      <td>1662.711432</td>\n",
              "      <td>9.600000e+03</td>\n",
              "      <td>1.397308e+06</td>\n",
              "      <td>1707.494884</td>\n",
              "      <td>7.200000e+03</td>\n",
              "      <td>1.408435e+06</td>\n",
              "      <td>1725.420844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-22</th>\n",
              "      <td>2.160000e+04</td>\n",
              "      <td>1.337302e+06</td>\n",
              "      <td>1613.713808</td>\n",
              "      <td>1.920000e+04</td>\n",
              "      <td>1.344981e+06</td>\n",
              "      <td>1630.318908</td>\n",
              "      <td>1.680000e+04</td>\n",
              "      <td>1.348689e+06</td>\n",
              "      <td>1649.125441</td>\n",
              "      <td>1.440000e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>1662.711432</td>\n",
              "      <td>9.600000e+03</td>\n",
              "      <td>1.397308e+06</td>\n",
              "      <td>1707.494884</td>\n",
              "      <td>7.200000e+03</td>\n",
              "      <td>1.408435e+06</td>\n",
              "      <td>1725.420844</td>\n",
              "      <td>4.799716e+03</td>\n",
              "      <td>1.379366e+06</td>\n",
              "      <td>1620.632599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1900 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ea61087-50ce-4e25-9838-b018532588e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ea61087-50ce-4e25-9838-b018532588e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ea61087-50ce-4e25-9838-b018532588e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_LSTM(individual, steps_ahead=1):\n",
        "    n_epochs = individual[0]\n",
        "    num_hidden_layers = individual[1]\n",
        "    num_neurons = individual[2]\n",
        "    batch_size = individual[3]\n",
        "    window_size = individual[4]\n",
        "\n",
        "\n",
        "    # setting the session configurations for reproducibility.\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    np.random.seed(42)\n",
        "    np.random.seed(12345)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                            inter_op_parallelism_threads=1)\n",
        "    from keras import backend as K\n",
        "    tf.random.set_seed(1234)\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), \n",
        "                                config=session_conf)\n",
        "    K.set_session(sess)\n",
        "  \n",
        "\n",
        "    print('n_epochs', n_epochs, 'num_hidden_layers', num_hidden_layers, 'num_neurons',\n",
        "              num_neurons, \"batch_size\", batch_size, 'window_size', window_size)\n",
        "\n",
        "\n",
        "    #feature engineering\n",
        "    # # convert the stationary series to supervise learning\n",
        "    series_supervised = series_to_supervised(series, series.columns, n_in= window_size, n_out= steps_ahead, dropnan=True)   \n",
        "\n",
        "    # drop columns we don't want to predict\n",
        "    pattern = re.compile(r\"(t-)|^BORE_OIL_VOL.*\")\n",
        "    # Extract the column names that match the pattern\n",
        "    matching_columns = [col for col in series_supervised.columns if re.search(pattern, col)]\n",
        "    series_supervised = series_supervised[matching_columns]\n",
        "\n",
        "    # # split into train and test sets\n",
        "    series_supervised = series_supervised.values\n",
        "    train_size = int(series_supervised.shape[0] * 0.8)\n",
        "    test_size = series_supervised.shape[0] - train_size\n",
        "    train, test = series_supervised[0:train_size], series_supervised[train_size:]\n",
        "    print(\"train.shape:\",train.shape, \"test.shape:\",test.shape)\n",
        "\n",
        "    # scale  the data to a feature range(0,1)\n",
        "    scaler, train_scaled, test_scaled = scale(train, test)\n",
        "    print('train_scaled.shape:',train_scaled.shape, \"test_scaled.shape:\",test_scaled.shape)\n",
        "\n",
        "    # reshape input to be 3D [samples, window_size, features]\n",
        "    n_features = len(series.columns)\n",
        "    train_X, train_y = train_scaled[:, 0:-steps_ahead], train_scaled[:, -steps_ahead:]\n",
        "    train_X = train_X.reshape(train_X.shape[0], window_size, n_features)\n",
        "    test_X, test_y = test_scaled[:, 0:-steps_ahead], test_scaled[:, -steps_ahead:]\n",
        "    test_X = test_X.reshape(test_X.shape[0], window_size, n_features )\n",
        "    print(\"train_X.shape:\",train_X.shape, \"train_y.shape:\",train_y.shape,\n",
        "            \"test_X.shape:\", test_X.shape,\"test_y.shape:\",test_y.shape)\n",
        "\n",
        "    # build the model\n",
        "    input_tensor=Input(shape=(window_size, n_features))\n",
        "\n",
        "    if num_hidden_layers != 1:\n",
        "        for num in range(num_hidden_layers-1):\n",
        "            lstm_1 = LSTM(num_neurons,activation='tanh', input_shape=(window_size, n_features), \n",
        "                          return_sequences=True)(input_tensor)\n",
        "        lstm_2 = LSTM(num_neurons)(lstm_1)\n",
        "    else:\n",
        "        lstm_1 = LSTM(num_neurons, activation='tanh', input_shape=(window_size, n_features))(input_tensor)   \n",
        "    lstm_output = LSTM(num_neurons, activation='tanh')(input_tensor)    \n",
        "    outputs = Dense(steps_ahead)(lstm_output)\n",
        "    model = Model(input_tensor, outputs)\n",
        "\n",
        "    model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
        "    #prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=2,\n",
        "                                restore_best_weights=True, mode='min')\n",
        "    \n",
        "    model.fit(train_X, train_y, epochs=n_epochs, batch_size=batch_size, callbacks = [early_stopping],\n",
        "              validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "    \n",
        "    return model.evaluate(test_X, test_y, verbose=0)"
      ],
      "metadata": {
        "id": "qkoLeH9tDTFg"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_TCN(individual, steps_ahead=1):\n",
        "    n_epochs = individual[0]\n",
        "    num_hidden_layers = individual[1]\n",
        "    num_neurons = individual[2]\n",
        "    batch_size = individual[3]\n",
        "    window_size = individual[4]\n",
        "\n",
        "\n",
        "    # setting the session configurations for reproducibility.\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    np.random.seed(42)\n",
        "    np.random.seed(12345)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                            inter_op_parallelism_threads=1)\n",
        "    from keras import backend as K\n",
        "    tf.random.set_seed(1234)\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), \n",
        "                                config=session_conf)\n",
        "    K.set_session(sess)\n",
        "  \n",
        "\n",
        "    # print('n_epochs', n_epochs, 'num_hidden_layers', num_hidden_layers, 'num_neurons',\n",
        "    #           num_neurons, \"batch_size\", batch_size, 'window_size', window_size)\n",
        "\n",
        "\n",
        "    #feature engineering\n",
        "    # # convert the stationary series to supervise learning\n",
        "    series_supervised = series_to_supervised(series, series.columns, n_in= window_size, n_out= steps_ahead, dropnan=True)   \n",
        "\n",
        "    # drop columns we don't want to predict\n",
        "    pattern = re.compile(r\"(t-)|^BORE_OIL_VOL.*\")\n",
        "    # Extract the column names that match the pattern\n",
        "    matching_columns = [col for col in series_supervised.columns if re.search(pattern, col)]\n",
        "    series_supervised = series_supervised[matching_columns]\n",
        "\n",
        "    # # split into train and test sets\n",
        "    series_supervised = series_supervised.values\n",
        "    train_size = int(series_supervised.shape[0] * 0.8)\n",
        "    test_size = series_supervised.shape[0] - train_size\n",
        "    train, test = series_supervised[0:train_size], series_supervised[train_size:]\n",
        "    # print(\"train.shape:\",train.shape, \"test.shape:\",test.shape)\n",
        "\n",
        "    # scale  the data to a feature range(0,1)\n",
        "    scaler, train_scaled, test_scaled = scale(train, test)\n",
        "    # print('train_scaled.shape:',train_scaled.shape, \"test_scaled.shape:\",test_scaled.shape)\n",
        "\n",
        "    # reshape input to be 3D [samples, window_size, features]\n",
        "    n_features = len(series.columns)\n",
        "    train_X, train_y = train_scaled[:, 0:-steps_ahead], train_scaled[:, -steps_ahead:]\n",
        "    train_X = train_X.reshape(train_X.shape[0], window_size, n_features)\n",
        "    test_X, test_y = test_scaled[:, 0:-steps_ahead], test_scaled[:, -steps_ahead:]\n",
        "    test_X = test_X.reshape(test_X.shape[0], window_size, n_features )\n",
        "    print(\"train_X.shape:\",train_X.shape, \"train_y.shape:\",train_y.shape,\n",
        "            \"test_X.shape:\", test_X.shape,\"test_y.shape:\",test_y.shape)\n",
        "\n",
        "    # build the model\n",
        "    input_tensor=Input(shape=(window_size, n_features))\n",
        "\n",
        "    if num_hidden_layers != 1:\n",
        "        for num in range(num_hidden_layers-1):\n",
        "            lstm_1 = LSTM(num_neurons,activation='tanh', input_shape=(window_size, n_features), \n",
        "                          return_sequences=True)(input_tensor)\n",
        "        lstm_2 = LSTM(num_neurons)(lstm_1)\n",
        "    else:\n",
        "        lstm_1 = LSTM(num_neurons, activation='tanh', input_shape=(window_size, n_features))(input_tensor)   \n",
        "    lstm_output = LSTM(num_neurons, activation='tanh')(input_tensor)    \n",
        "    outputs = Dense(steps_ahead)(lstm_output)\n",
        "    model = Model(input_tensor, outputs)\n",
        "\n",
        "    model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
        "    #prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=2,\n",
        "                                restore_best_weights=True, mode='min')\n",
        "    \n",
        "    model.fit(train_X, train_y, epochs=n_epochs, batch_size=batch_size, callbacks = [early_stopping],\n",
        "              validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "    \n",
        "    return model.evaluate(test_X, test_y, verbose=0)"
      ],
      "metadata": {
        "id": "mGQJZ5vr2Bjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the search space\n",
        "n_epochs = [500,500]\n",
        "num_hidden_layers = [1, 2]\n",
        "num_neurons = [4, 200]\n",
        "batch_size = [2, 4]\n",
        "window_size = [2, 12]\n",
        "bounds = [n_epochs,num_hidden_layers, num_neurons, batch_size, window_size]\n",
        "\n",
        "# define the total number of generations\n",
        "n_iter = 3#0\n",
        "# bits per variable\n",
        "n_bits = 16\n",
        "# define the number of combinations (individuals) per generation \n",
        "n_pop = 20\n",
        "# crossover rate\n",
        "r_cross = 0.9\n",
        "# mutation rate\n",
        "r_mut = 1.0 / (float(n_bits) * len(bounds))\n",
        "genetic_algorithm(evaluate, bounds, n_bits, n_iter, n_pop, r_cross, r_mut)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeI5M8oPcgwU",
        "outputId": "d1963294-fb8a-470a-dd0a-0b5edeb7b775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation:1\n",
            "n_epochs 500 num_hidden_layers 2 num_neurons 58 batch_size 2 window_size 2\n",
            "train.shape: (1524, 7) test.shape: (381, 7)\n",
            "train_scaled.shape: (1524, 7) test_scaled.shape: (381, 7)\n",
            "train_X.shape: (1524, 2, 3) train_y.shape: (1524, 1) test_X.shape: (381, 2, 3) test_y.shape: (381, 1)\n",
            "Epoch 1/500\n",
            "762/762 - 4s - loss: 0.1238 - val_loss: 0.0039 - 4s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "762/762 - 2s - loss: 0.0164 - val_loss: 0.0028 - 2s/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "762/762 - 2s - loss: 0.0131 - val_loss: 0.0017 - 2s/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "762/762 - 2s - loss: 0.0116 - val_loss: 0.0011 - 2s/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "762/762 - 2s - loss: 0.0107 - val_loss: 8.4164e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "762/762 - 2s - loss: 0.0101 - val_loss: 6.7812e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "762/762 - 2s - loss: 0.0097 - val_loss: 5.8352e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "762/762 - 2s - loss: 0.0094 - val_loss: 5.2432e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "762/762 - 2s - loss: 0.0091 - val_loss: 4.8384e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "762/762 - 2s - loss: 0.0089 - val_loss: 4.5337e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "762/762 - 2s - loss: 0.0086 - val_loss: 4.2815e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "762/762 - 2s - loss: 0.0084 - val_loss: 4.0543e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "762/762 - 2s - loss: 0.0081 - val_loss: 3.8358e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "762/762 - 2s - loss: 0.0077 - val_loss: 3.6165e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "762/762 - 2s - loss: 0.0074 - val_loss: 3.3913e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "762/762 - 2s - loss: 0.0070 - val_loss: 3.1574e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "762/762 - 2s - loss: 0.0066 - val_loss: 2.9143e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "762/762 - 2s - loss: 0.0061 - val_loss: 2.6655e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "762/762 - 2s - loss: 0.0056 - val_loss: 2.4187e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "762/762 - 2s - loss: 0.0051 - val_loss: 2.1874e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "762/762 - 2s - loss: 0.0046 - val_loss: 1.9905e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "762/762 - 2s - loss: 0.0041 - val_loss: 1.8496e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "762/762 - 2s - loss: 0.0037 - val_loss: 1.7826e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "762/762 - 2s - loss: 0.0034 - val_loss: 1.7941e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "762/762 - 2s - loss: 0.0031 - val_loss: 1.8707e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "762/762 - 2s - loss: 0.0029 - val_loss: 1.9853e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "762/762 - 2s - loss: 0.0028 - val_loss: 2.1086e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "762/762 - 2s - loss: 0.0027 - val_loss: 2.2190e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "762/762 - 2s - loss: 0.0026 - val_loss: 2.3061e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "762/762 - 2s - loss: 0.0025 - val_loss: 2.3675e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 31/500\n",
            "762/762 - 3s - loss: 0.0025 - val_loss: 2.4060e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 32/500\n",
            "762/762 - 2s - loss: 0.0024 - val_loss: 2.4260e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 33/500\n",
            "762/762 - 2s - loss: 0.0024 - val_loss: 2.4320e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 34/500\n",
            "762/762 - 3s - loss: 0.0024 - val_loss: 2.4280e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "762/762 - 2s - loss: 0.0023 - val_loss: 2.4172e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "762/762 - 2s - loss: 0.0023 - val_loss: 2.4018e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "762/762 - 2s - loss: 0.0023 - val_loss: 2.3838e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "762/762 - 2s - loss: 0.0023 - val_loss: 2.3643e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "762/762 - 2s - loss: 0.0023 - val_loss: 2.3443e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "762/762 - 2s - loss: 0.0022 - val_loss: 2.3245e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "762/762 - 2s - loss: 0.0022 - val_loss: 2.3054e-04 - 2s/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "762/762 - 2s - loss: 0.0022 - val_loss: 2.2871e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 43/500\n",
            "Restoring model weights from the end of the best epoch: 23.\n",
            "762/762 - 3s - loss: 0.0022 - val_loss: 2.2700e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 43: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 45 batch_size 2 window_size 7\n",
            "train.shape: (1520, 22) test.shape: (380, 22)\n",
            "train_scaled.shape: (1520, 22) test_scaled.shape: (380, 22)\n",
            "train_X.shape: (1520, 7, 3) train_y.shape: (1520, 1) test_X.shape: (380, 7, 3) test_y.shape: (380, 1)\n",
            "Epoch 1/500\n",
            "760/760 - 4s - loss: 0.0315 - val_loss: 0.0012 - 4s/epoch - 6ms/step\n",
            "Epoch 2/500\n",
            "760/760 - 2s - loss: 0.0132 - val_loss: 0.0010 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "760/760 - 2s - loss: 0.0122 - val_loss: 9.0818e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "760/760 - 2s - loss: 0.0112 - val_loss: 8.2077e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "760/760 - 2s - loss: 0.0102 - val_loss: 7.5914e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "760/760 - 3s - loss: 0.0093 - val_loss: 7.1282e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "760/760 - 2s - loss: 0.0083 - val_loss: 6.7163e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "760/760 - 2s - loss: 0.0074 - val_loss: 6.2545e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "760/760 - 2s - loss: 0.0066 - val_loss: 5.6833e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "760/760 - 2s - loss: 0.0060 - val_loss: 5.0554e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 11/500\n",
            "760/760 - 2s - loss: 0.0055 - val_loss: 4.4694e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 12/500\n",
            "760/760 - 2s - loss: 0.0052 - val_loss: 3.9693e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 13/500\n",
            "760/760 - 2s - loss: 0.0048 - val_loss: 3.5486e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 14/500\n",
            "760/760 - 2s - loss: 0.0045 - val_loss: 3.1870e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 15/500\n",
            "760/760 - 2s - loss: 0.0042 - val_loss: 2.8686e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 16/500\n",
            "760/760 - 2s - loss: 0.0039 - val_loss: 2.5869e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 17/500\n",
            "760/760 - 2s - loss: 0.0037 - val_loss: 2.3412e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 18/500\n",
            "760/760 - 2s - loss: 0.0035 - val_loss: 2.1315e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 19/500\n",
            "760/760 - 2s - loss: 0.0033 - val_loss: 1.9557e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 20/500\n",
            "760/760 - 2s - loss: 0.0031 - val_loss: 1.8099e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 21/500\n",
            "760/760 - 2s - loss: 0.0030 - val_loss: 1.6897e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 22/500\n",
            "760/760 - 2s - loss: 0.0029 - val_loss: 1.5906e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 23/500\n",
            "760/760 - 2s - loss: 0.0028 - val_loss: 1.5090e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 24/500\n",
            "760/760 - 2s - loss: 0.0027 - val_loss: 1.4419e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 25/500\n",
            "760/760 - 3s - loss: 0.0026 - val_loss: 1.3873e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "760/760 - 2s - loss: 0.0025 - val_loss: 1.3440e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 27/500\n",
            "760/760 - 2s - loss: 0.0024 - val_loss: 1.3114e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 28/500\n",
            "760/760 - 2s - loss: 0.0023 - val_loss: 1.2895e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 29/500\n",
            "760/760 - 2s - loss: 0.0022 - val_loss: 1.2786e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 30/500\n",
            "760/760 - 2s - loss: 0.0022 - val_loss: 1.2794e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 31/500\n",
            "760/760 - 2s - loss: 0.0021 - val_loss: 1.2929e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 32/500\n",
            "760/760 - 2s - loss: 0.0020 - val_loss: 1.3195e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 33/500\n",
            "760/760 - 3s - loss: 0.0020 - val_loss: 1.3588e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 34/500\n",
            "760/760 - 2s - loss: 0.0019 - val_loss: 1.4089e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 35/500\n",
            "760/760 - 2s - loss: 0.0019 - val_loss: 1.4664e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 36/500\n",
            "760/760 - 2s - loss: 0.0018 - val_loss: 1.5265e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 37/500\n",
            "760/760 - 2s - loss: 0.0018 - val_loss: 1.5844e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 38/500\n",
            "760/760 - 2s - loss: 0.0017 - val_loss: 1.6357e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 39/500\n",
            "760/760 - 2s - loss: 0.0017 - val_loss: 1.6776e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 40/500\n",
            "760/760 - 2s - loss: 0.0016 - val_loss: 1.7086e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 41/500\n",
            "760/760 - 2s - loss: 0.0016 - val_loss: 1.7289e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 42/500\n",
            "760/760 - 2s - loss: 0.0015 - val_loss: 1.7395e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 43/500\n",
            "760/760 - 2s - loss: 0.0015 - val_loss: 1.7423e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 44/500\n",
            "760/760 - 2s - loss: 0.0015 - val_loss: 1.7392e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 45/500\n",
            "760/760 - 2s - loss: 0.0014 - val_loss: 1.7321e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 46/500\n",
            "760/760 - 2s - loss: 0.0014 - val_loss: 1.7228e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 47/500\n",
            "760/760 - 2s - loss: 0.0014 - val_loss: 1.7125e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 48/500\n",
            "760/760 - 2s - loss: 0.0014 - val_loss: 1.7022e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 49/500\n",
            "Restoring model weights from the end of the best epoch: 29.\n",
            "760/760 - 2s - loss: 0.0014 - val_loss: 1.6927e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 49: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 37 batch_size 3 window_size 8\n",
            "train.shape: (1519, 25) test.shape: (380, 25)\n",
            "train_scaled.shape: (1519, 25) test_scaled.shape: (380, 25)\n",
            "train_X.shape: (1519, 8, 3) train_y.shape: (1519, 1) test_X.shape: (380, 8, 3) test_y.shape: (380, 1)\n",
            "Epoch 1/500\n",
            "507/507 - 4s - loss: 0.1078 - val_loss: 0.0022 - 4s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "507/507 - 2s - loss: 0.0195 - val_loss: 0.0020 - 2s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "507/507 - 2s - loss: 0.0157 - val_loss: 0.0017 - 2s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "507/507 - 2s - loss: 0.0135 - val_loss: 0.0015 - 2s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "507/507 - 2s - loss: 0.0121 - val_loss: 0.0013 - 2s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "507/507 - 2s - loss: 0.0111 - val_loss: 0.0012 - 2s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "507/507 - 2s - loss: 0.0102 - val_loss: 0.0010 - 2s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "507/507 - 2s - loss: 0.0094 - val_loss: 9.1259e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "507/507 - 2s - loss: 0.0087 - val_loss: 8.1981e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "507/507 - 2s - loss: 0.0079 - val_loss: 7.4189e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "507/507 - 2s - loss: 0.0073 - val_loss: 6.7229e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 12/500\n",
            "507/507 - 2s - loss: 0.0068 - val_loss: 6.0835e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 13/500\n",
            "507/507 - 2s - loss: 0.0063 - val_loss: 5.5068e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 14/500\n",
            "507/507 - 2s - loss: 0.0060 - val_loss: 5.0056e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 15/500\n",
            "507/507 - 2s - loss: 0.0057 - val_loss: 4.5809e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 16/500\n",
            "507/507 - 2s - loss: 0.0054 - val_loss: 4.2200e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 17/500\n",
            "507/507 - 2s - loss: 0.0052 - val_loss: 3.9070e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "507/507 - 2s - loss: 0.0050 - val_loss: 3.6292e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 19/500\n",
            "507/507 - 2s - loss: 0.0047 - val_loss: 3.3777e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 20/500\n",
            "507/507 - 2s - loss: 0.0045 - val_loss: 3.1460e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "507/507 - 2s - loss: 0.0043 - val_loss: 2.9303e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "507/507 - 2s - loss: 0.0041 - val_loss: 2.7288e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "507/507 - 3s - loss: 0.0039 - val_loss: 2.5410e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 24/500\n",
            "507/507 - 3s - loss: 0.0037 - val_loss: 2.3674e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 25/500\n",
            "507/507 - 2s - loss: 0.0036 - val_loss: 2.2084e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "507/507 - 2s - loss: 0.0035 - val_loss: 2.0642e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 27/500\n",
            "507/507 - 2s - loss: 0.0033 - val_loss: 1.9347e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 28/500\n",
            "507/507 - 2s - loss: 0.0032 - val_loss: 1.8194e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 29/500\n",
            "507/507 - 2s - loss: 0.0031 - val_loss: 1.7177e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 30/500\n",
            "507/507 - 2s - loss: 0.0030 - val_loss: 1.6284e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 31/500\n",
            "507/507 - 2s - loss: 0.0029 - val_loss: 1.5506e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 32/500\n",
            "507/507 - 2s - loss: 0.0029 - val_loss: 1.4831e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "507/507 - 2s - loss: 0.0028 - val_loss: 1.4247e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 34/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.3740e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.3301e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.2921e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 37/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.2593e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 38/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.2310e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 39/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.2067e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 40/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.1860e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 41/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.1689e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 42/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.1550e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 43/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.1444e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 44/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.1371e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 45/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.1330e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 46/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.1323e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 47/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.1349e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 48/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.1410e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 49/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.1507e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 50/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.1640e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 51/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.1810e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 52/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.2015e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 53/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.2255e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 54/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.2524e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 55/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.2820e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 56/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3134e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 57/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3459e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 58/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3781e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 59/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.4087e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 60/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.4363e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 61/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.4594e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 62/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.4769e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 63/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.4885e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 64/500\n",
            "507/507 - 2s - loss: 0.0015 - val_loss: 1.4939e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 65/500\n",
            "507/507 - 2s - loss: 0.0015 - val_loss: 1.4939e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 66/500\n",
            "Restoring model weights from the end of the best epoch: 46.\n",
            "507/507 - 2s - loss: 0.0015 - val_loss: 1.4892e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 66: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 12 batch_size 3 window_size 7\n",
            "train.shape: (1520, 22) test.shape: (380, 22)\n",
            "train_scaled.shape: (1520, 22) test_scaled.shape: (380, 22)\n",
            "train_X.shape: (1520, 7, 3) train_y.shape: (1520, 1) test_X.shape: (380, 7, 3) test_y.shape: (380, 1)\n",
            "Epoch 1/500\n",
            "507/507 - 4s - loss: 0.1180 - val_loss: 0.0011 - 4s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "507/507 - 2s - loss: 0.0290 - val_loss: 0.0026 - 2s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "507/507 - 2s - loss: 0.0190 - val_loss: 0.0018 - 2s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "507/507 - 2s - loss: 0.0165 - val_loss: 0.0014 - 2s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "507/507 - 2s - loss: 0.0146 - val_loss: 0.0012 - 2s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "507/507 - 2s - loss: 0.0131 - val_loss: 0.0011 - 2s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "507/507 - 2s - loss: 0.0120 - val_loss: 9.7683e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "507/507 - 2s - loss: 0.0112 - val_loss: 8.9564e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "507/507 - 2s - loss: 0.0105 - val_loss: 8.2150e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "507/507 - 2s - loss: 0.0099 - val_loss: 7.5733e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "507/507 - 2s - loss: 0.0094 - val_loss: 7.0437e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 12/500\n",
            "507/507 - 2s - loss: 0.0088 - val_loss: 6.6171e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 13/500\n",
            "507/507 - 2s - loss: 0.0083 - val_loss: 6.2746e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 14/500\n",
            "507/507 - 2s - loss: 0.0078 - val_loss: 5.9966e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 15/500\n",
            "507/507 - 2s - loss: 0.0074 - val_loss: 5.7663e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 16/500\n",
            "507/507 - 2s - loss: 0.0069 - val_loss: 5.5706e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 17/500\n",
            "507/507 - 2s - loss: 0.0065 - val_loss: 5.3993e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "507/507 - 2s - loss: 0.0062 - val_loss: 5.2448e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 19/500\n",
            "507/507 - 2s - loss: 0.0059 - val_loss: 5.1010e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 20/500\n",
            "507/507 - 2s - loss: 0.0056 - val_loss: 4.9633e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "507/507 - 2s - loss: 0.0054 - val_loss: 4.8284e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "507/507 - 2s - loss: 0.0052 - val_loss: 4.6934e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "507/507 - 2s - loss: 0.0050 - val_loss: 4.5563e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 24/500\n",
            "507/507 - 2s - loss: 0.0048 - val_loss: 4.4159e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 25/500\n",
            "507/507 - 2s - loss: 0.0047 - val_loss: 4.2717e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "507/507 - 2s - loss: 0.0046 - val_loss: 4.1237e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 27/500\n",
            "507/507 - 2s - loss: 0.0045 - val_loss: 3.9729e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 28/500\n",
            "507/507 - 2s - loss: 0.0044 - val_loss: 3.8203e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 29/500\n",
            "507/507 - 2s - loss: 0.0043 - val_loss: 3.6674e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 30/500\n",
            "507/507 - 2s - loss: 0.0042 - val_loss: 3.5158e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 31/500\n",
            "507/507 - 2s - loss: 0.0041 - val_loss: 3.3670e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 32/500\n",
            "507/507 - 2s - loss: 0.0040 - val_loss: 3.2223e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "507/507 - 2s - loss: 0.0039 - val_loss: 3.0828e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 34/500\n",
            "507/507 - 2s - loss: 0.0038 - val_loss: 2.9495e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "507/507 - 2s - loss: 0.0038 - val_loss: 2.8230e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "507/507 - 2s - loss: 0.0037 - val_loss: 2.7036e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 37/500\n",
            "507/507 - 2s - loss: 0.0036 - val_loss: 2.5917e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 38/500\n",
            "507/507 - 2s - loss: 0.0036 - val_loss: 2.4872e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 39/500\n",
            "507/507 - 2s - loss: 0.0035 - val_loss: 2.3902e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 40/500\n",
            "507/507 - 3s - loss: 0.0034 - val_loss: 2.3003e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 41/500\n",
            "507/507 - 3s - loss: 0.0034 - val_loss: 2.2174e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 42/500\n",
            "507/507 - 2s - loss: 0.0033 - val_loss: 2.1411e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 43/500\n",
            "507/507 - 2s - loss: 0.0033 - val_loss: 2.0711e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 44/500\n",
            "507/507 - 3s - loss: 0.0032 - val_loss: 2.0070e-04 - 3s/epoch - 7ms/step\n",
            "Epoch 45/500\n",
            "507/507 - 3s - loss: 0.0032 - val_loss: 1.9485e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 46/500\n",
            "507/507 - 3s - loss: 0.0031 - val_loss: 1.8950e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 47/500\n",
            "507/507 - 2s - loss: 0.0031 - val_loss: 1.8463e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 48/500\n",
            "507/507 - 3s - loss: 0.0030 - val_loss: 1.8020e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 49/500\n",
            "507/507 - 3s - loss: 0.0030 - val_loss: 1.7616e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 50/500\n",
            "507/507 - 3s - loss: 0.0029 - val_loss: 1.7250e-04 - 3s/epoch - 7ms/step\n",
            "Epoch 51/500\n",
            "507/507 - 3s - loss: 0.0029 - val_loss: 1.6916e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 52/500\n",
            "507/507 - 4s - loss: 0.0028 - val_loss: 1.6613e-04 - 4s/epoch - 7ms/step\n",
            "Epoch 53/500\n",
            "507/507 - 3s - loss: 0.0028 - val_loss: 1.6338e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 54/500\n",
            "507/507 - 2s - loss: 0.0028 - val_loss: 1.6087e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 55/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.5859e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 56/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.5652e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 57/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.5463e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 58/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.5291e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 59/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.5134e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 60/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.4991e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 61/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.4861e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 62/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.4742e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 63/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.4634e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 64/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.4536e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 65/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.4446e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 66/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.4364e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 67/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.4289e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 68/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.4221e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 69/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.4159e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 70/500\n",
            "507/507 - 3s - loss: 0.0023 - val_loss: 1.4102e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 71/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.4051e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 72/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.4003e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 73/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.3960e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 74/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.3920e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 75/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.3884e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 76/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.3851e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 77/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.3820e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 78/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.3791e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 79/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.3765e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 80/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.3741e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 81/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.3718e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 82/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.3697e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 83/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.3677e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 84/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.3658e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 85/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3640e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 86/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3624e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 87/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3608e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 88/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3592e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 89/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3578e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 90/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3563e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 91/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3550e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 92/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3536e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 93/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3524e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 94/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3511e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 95/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3499e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 96/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3487e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 97/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3475e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 98/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3464e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 99/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3453e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 100/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3442e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 101/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3431e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 102/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3421e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 103/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3411e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 104/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3401e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 105/500\n",
            "507/507 - 3s - loss: 0.0018 - val_loss: 1.3392e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 106/500\n",
            "507/507 - 3s - loss: 0.0018 - val_loss: 1.3383e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 107/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3374e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 108/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3365e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 109/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3357e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 110/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3349e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 111/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3342e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 112/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3335e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 113/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3328e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 114/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3322e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 115/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3316e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 116/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.3310e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 117/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3305e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 118/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3300e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 119/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3296e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 120/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3292e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 121/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3289e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 122/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3286e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 123/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3284e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 124/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3282e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 125/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3281e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 126/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3280e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 127/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3280e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 128/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3280e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 129/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3280e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 130/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3282e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 131/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3284e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 132/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3286e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 133/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3289e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 134/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3293e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 135/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3298e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 136/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3303e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 137/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3309e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 138/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3316e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 139/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3324e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 140/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3333e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 141/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3343e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 142/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3354e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 143/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3367e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 144/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3381e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 145/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3396e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 146/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3413e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 147/500\n",
            "Restoring model weights from the end of the best epoch: 127.\n",
            "507/507 - 2s - loss: 0.0015 - val_loss: 1.3432e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 147: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 25 batch_size 3 window_size 8\n",
            "train.shape: (1519, 25) test.shape: (380, 25)\n",
            "train_scaled.shape: (1519, 25) test_scaled.shape: (380, 25)\n",
            "train_X.shape: (1519, 8, 3) train_y.shape: (1519, 1) test_X.shape: (380, 8, 3) test_y.shape: (380, 1)\n",
            "Epoch 1/500\n",
            "507/507 - 5s - loss: 0.1847 - val_loss: 0.0030 - 5s/epoch - 10ms/step\n",
            "Epoch 2/500\n",
            "507/507 - 3s - loss: 0.0347 - val_loss: 0.0036 - 3s/epoch - 5ms/step\n",
            "Epoch 3/500\n",
            "507/507 - 2s - loss: 0.0252 - val_loss: 0.0025 - 2s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "507/507 - 2s - loss: 0.0208 - val_loss: 0.0018 - 2s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "507/507 - 2s - loss: 0.0175 - val_loss: 0.0014 - 2s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "507/507 - 2s - loss: 0.0150 - val_loss: 0.0012 - 2s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "507/507 - 2s - loss: 0.0133 - val_loss: 0.0011 - 2s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "507/507 - 2s - loss: 0.0120 - val_loss: 9.2778e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "507/507 - 2s - loss: 0.0110 - val_loss: 8.0491e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "507/507 - 3s - loss: 0.0101 - val_loss: 6.9469e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 11/500\n",
            "507/507 - 3s - loss: 0.0093 - val_loss: 5.9918e-04 - 3s/epoch - 7ms/step\n",
            "Epoch 12/500\n",
            "507/507 - 5s - loss: 0.0085 - val_loss: 5.1826e-04 - 5s/epoch - 9ms/step\n",
            "Epoch 13/500\n",
            "507/507 - 3s - loss: 0.0078 - val_loss: 4.5101e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 14/500\n",
            "507/507 - 2s - loss: 0.0071 - val_loss: 3.9662e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 15/500\n",
            "507/507 - 3s - loss: 0.0066 - val_loss: 3.5409e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 16/500\n",
            "507/507 - 2s - loss: 0.0062 - val_loss: 3.2186e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 17/500\n",
            "507/507 - 2s - loss: 0.0059 - val_loss: 2.9780e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "507/507 - 2s - loss: 0.0056 - val_loss: 2.7967e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 19/500\n",
            "507/507 - 2s - loss: 0.0054 - val_loss: 2.6552e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 20/500\n",
            "507/507 - 2s - loss: 0.0052 - val_loss: 2.5391e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "507/507 - 2s - loss: 0.0050 - val_loss: 2.4386e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "507/507 - 2s - loss: 0.0048 - val_loss: 2.3473e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "507/507 - 2s - loss: 0.0046 - val_loss: 2.2609e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 24/500\n",
            "507/507 - 2s - loss: 0.0045 - val_loss: 2.1766e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 25/500\n",
            "507/507 - 2s - loss: 0.0043 - val_loss: 2.0927e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "507/507 - 2s - loss: 0.0042 - val_loss: 2.0090e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 27/500\n",
            "507/507 - 2s - loss: 0.0040 - val_loss: 1.9270e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 28/500\n",
            "507/507 - 2s - loss: 0.0039 - val_loss: 1.8480e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 29/500\n",
            "507/507 - 2s - loss: 0.0037 - val_loss: 1.7735e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 30/500\n",
            "507/507 - 2s - loss: 0.0036 - val_loss: 1.7042e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 31/500\n",
            "507/507 - 2s - loss: 0.0035 - val_loss: 1.6405e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 32/500\n",
            "507/507 - 2s - loss: 0.0033 - val_loss: 1.5829e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "507/507 - 2s - loss: 0.0032 - val_loss: 1.5312e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 34/500\n",
            "507/507 - 2s - loss: 0.0031 - val_loss: 1.4853e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "507/507 - 2s - loss: 0.0031 - val_loss: 1.4448e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "507/507 - 2s - loss: 0.0030 - val_loss: 1.4095e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 37/500\n",
            "507/507 - 2s - loss: 0.0029 - val_loss: 1.3789e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 38/500\n",
            "507/507 - 2s - loss: 0.0028 - val_loss: 1.3526e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 39/500\n",
            "507/507 - 2s - loss: 0.0028 - val_loss: 1.3302e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 40/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.3112e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 41/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.2954e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 42/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.2823e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 43/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.2716e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 44/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.2631e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 45/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.2565e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 46/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.2516e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 47/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.2482e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 48/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.2462e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 49/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.2454e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 50/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.2459e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 51/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.2474e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 52/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.2501e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 53/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.2537e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 54/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.2583e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 55/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.2638e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 56/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2704e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 57/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2779e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 58/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2863e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 59/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2958e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 60/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3063e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 61/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3178e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 62/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3305e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 63/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.3443e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 64/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3593e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 65/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3755e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 66/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.3930e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 67/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.4118e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 68/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.4320e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 69/500\n",
            "Restoring model weights from the end of the best epoch: 49.\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.4536e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 69: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 43 batch_size 4 window_size 4\n",
            "train.shape: (1522, 13) test.shape: (381, 13)\n",
            "train_scaled.shape: (1522, 13) test_scaled.shape: (381, 13)\n",
            "train_X.shape: (1522, 4, 3) train_y.shape: (1522, 1) test_X.shape: (381, 4, 3) test_y.shape: (381, 1)\n",
            "Epoch 1/500\n",
            "381/381 - 4s - loss: 0.3105 - val_loss: 0.0015 - 4s/epoch - 9ms/step\n",
            "Epoch 2/500\n",
            "381/381 - 1s - loss: 0.0563 - val_loss: 0.0096 - 1s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "381/381 - 1s - loss: 0.0236 - val_loss: 0.0072 - 1s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "381/381 - 1s - loss: 0.0203 - val_loss: 0.0054 - 1s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "381/381 - 1s - loss: 0.0179 - val_loss: 0.0041 - 1s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "381/381 - 1s - loss: 0.0160 - val_loss: 0.0031 - 1s/epoch - 3ms/step\n",
            "Epoch 7/500\n",
            "381/381 - 1s - loss: 0.0145 - val_loss: 0.0025 - 1s/epoch - 3ms/step\n",
            "Epoch 8/500\n",
            "381/381 - 1s - loss: 0.0134 - val_loss: 0.0021 - 1s/epoch - 3ms/step\n",
            "Epoch 9/500\n",
            "381/381 - 1s - loss: 0.0126 - val_loss: 0.0017 - 1s/epoch - 3ms/step\n",
            "Epoch 10/500\n",
            "381/381 - 1s - loss: 0.0120 - val_loss: 0.0015 - 1s/epoch - 3ms/step\n",
            "Epoch 11/500\n",
            "381/381 - 1s - loss: 0.0115 - val_loss: 0.0013 - 1s/epoch - 3ms/step\n",
            "Epoch 12/500\n",
            "381/381 - 1s - loss: 0.0111 - val_loss: 0.0012 - 1s/epoch - 3ms/step\n",
            "Epoch 13/500\n",
            "381/381 - 1s - loss: 0.0106 - val_loss: 0.0011 - 1s/epoch - 3ms/step\n",
            "Epoch 14/500\n",
            "381/381 - 1s - loss: 0.0102 - val_loss: 0.0010 - 1s/epoch - 4ms/step\n",
            "Epoch 15/500\n",
            "381/381 - 1s - loss: 0.0097 - val_loss: 9.3847e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 16/500\n",
            "381/381 - 1s - loss: 0.0093 - val_loss: 8.6925e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 17/500\n",
            "381/381 - 1s - loss: 0.0088 - val_loss: 8.0509e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "381/381 - 1s - loss: 0.0083 - val_loss: 7.4394e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 19/500\n",
            "381/381 - 1s - loss: 0.0077 - val_loss: 6.8441e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 20/500\n",
            "381/381 - 1s - loss: 0.0072 - val_loss: 6.2568e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "381/381 - 1s - loss: 0.0067 - val_loss: 5.6748e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "381/381 - 1s - loss: 0.0062 - val_loss: 5.1009e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "381/381 - 1s - loss: 0.0058 - val_loss: 4.5443e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 24/500\n",
            "381/381 - 1s - loss: 0.0053 - val_loss: 4.0208e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 25/500\n",
            "381/381 - 1s - loss: 0.0050 - val_loss: 3.5491e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "381/381 - 1s - loss: 0.0047 - val_loss: 3.1447e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 27/500\n",
            "381/381 - 1s - loss: 0.0044 - val_loss: 2.8138e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 28/500\n",
            "381/381 - 1s - loss: 0.0042 - val_loss: 2.5530e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 29/500\n",
            "381/381 - 1s - loss: 0.0041 - val_loss: 2.3519e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 30/500\n",
            "381/381 - 1s - loss: 0.0039 - val_loss: 2.1976e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 31/500\n",
            "381/381 - 1s - loss: 0.0038 - val_loss: 2.0776e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 32/500\n",
            "381/381 - 2s - loss: 0.0037 - val_loss: 1.9819e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "381/381 - 2s - loss: 0.0036 - val_loss: 1.9030e-04 - 2s/epoch - 6ms/step\n",
            "Epoch 34/500\n",
            "381/381 - 2s - loss: 0.0035 - val_loss: 1.8357e-04 - 2s/epoch - 6ms/step\n",
            "Epoch 35/500\n",
            "381/381 - 1s - loss: 0.0034 - val_loss: 1.7767e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 36/500\n",
            "381/381 - 1s - loss: 0.0033 - val_loss: 1.7237e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 37/500\n",
            "381/381 - 1s - loss: 0.0032 - val_loss: 1.6753e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 38/500\n",
            "381/381 - 1s - loss: 0.0031 - val_loss: 1.6308e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 39/500\n",
            "381/381 - 1s - loss: 0.0031 - val_loss: 1.5897e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 40/500\n",
            "381/381 - 1s - loss: 0.0030 - val_loss: 1.5517e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 41/500\n",
            "381/381 - 1s - loss: 0.0029 - val_loss: 1.5167e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 42/500\n",
            "381/381 - 1s - loss: 0.0029 - val_loss: 1.4846e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 43/500\n",
            "381/381 - 1s - loss: 0.0028 - val_loss: 1.4554e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 44/500\n",
            "381/381 - 1s - loss: 0.0027 - val_loss: 1.4290e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 45/500\n",
            "381/381 - 1s - loss: 0.0027 - val_loss: 1.4052e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 46/500\n",
            "381/381 - 1s - loss: 0.0026 - val_loss: 1.3838e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 47/500\n",
            "381/381 - 1s - loss: 0.0026 - val_loss: 1.3646e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 48/500\n",
            "381/381 - 1s - loss: 0.0026 - val_loss: 1.3474e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 49/500\n",
            "381/381 - 1s - loss: 0.0025 - val_loss: 1.3321e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 50/500\n",
            "381/381 - 1s - loss: 0.0025 - val_loss: 1.3184e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 51/500\n",
            "381/381 - 1s - loss: 0.0024 - val_loss: 1.3061e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 52/500\n",
            "381/381 - 1s - loss: 0.0024 - val_loss: 1.2951e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 53/500\n",
            "381/381 - 1s - loss: 0.0024 - val_loss: 1.2852e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 54/500\n",
            "381/381 - 1s - loss: 0.0024 - val_loss: 1.2763e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 55/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 1.2683e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 56/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 1.2612e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 57/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 1.2547e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 58/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 1.2490e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 59/500\n",
            "381/381 - 1s - loss: 0.0022 - val_loss: 1.2439e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 60/500\n",
            "381/381 - 1s - loss: 0.0022 - val_loss: 1.2394e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 61/500\n",
            "381/381 - 1s - loss: 0.0022 - val_loss: 1.2355e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 62/500\n",
            "381/381 - 1s - loss: 0.0022 - val_loss: 1.2322e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 63/500\n",
            "381/381 - 1s - loss: 0.0021 - val_loss: 1.2295e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 64/500\n",
            "381/381 - 1s - loss: 0.0021 - val_loss: 1.2273e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 65/500\n",
            "381/381 - 1s - loss: 0.0021 - val_loss: 1.2257e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 66/500\n",
            "381/381 - 1s - loss: 0.0021 - val_loss: 1.2247e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 67/500\n",
            "381/381 - 1s - loss: 0.0021 - val_loss: 1.2243e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 68/500\n",
            "381/381 - 1s - loss: 0.0020 - val_loss: 1.2246e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 69/500\n",
            "381/381 - 1s - loss: 0.0020 - val_loss: 1.2256e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 70/500\n",
            "381/381 - 1s - loss: 0.0020 - val_loss: 1.2272e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 71/500\n",
            "381/381 - 1s - loss: 0.0020 - val_loss: 1.2296e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 72/500\n",
            "381/381 - 1s - loss: 0.0020 - val_loss: 1.2328e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 73/500\n",
            "381/381 - 1s - loss: 0.0019 - val_loss: 1.2368e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 74/500\n",
            "381/381 - 1s - loss: 0.0019 - val_loss: 1.2417e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 75/500\n",
            "381/381 - 1s - loss: 0.0019 - val_loss: 1.2474e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 76/500\n",
            "381/381 - 1s - loss: 0.0019 - val_loss: 1.2542e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 77/500\n",
            "381/381 - 1s - loss: 0.0019 - val_loss: 1.2619e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 78/500\n",
            "381/381 - 1s - loss: 0.0019 - val_loss: 1.2707e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 79/500\n",
            "381/381 - 1s - loss: 0.0018 - val_loss: 1.2806e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 80/500\n",
            "381/381 - 1s - loss: 0.0018 - val_loss: 1.2916e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 81/500\n",
            "381/381 - 1s - loss: 0.0018 - val_loss: 1.3038e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 82/500\n",
            "381/381 - 1s - loss: 0.0018 - val_loss: 1.3172e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 83/500\n",
            "381/381 - 1s - loss: 0.0018 - val_loss: 1.3318e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 84/500\n",
            "381/381 - 1s - loss: 0.0018 - val_loss: 1.3475e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 85/500\n",
            "381/381 - 1s - loss: 0.0018 - val_loss: 1.3646e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 86/500\n",
            "381/381 - 1s - loss: 0.0017 - val_loss: 1.3828e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 87/500\n",
            "Restoring model weights from the end of the best epoch: 67.\n",
            "381/381 - 1s - loss: 0.0017 - val_loss: 1.4023e-04 - 1s/epoch - 3ms/step\n",
            "Epoch 87: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 29 batch_size 3 window_size 8\n",
            "train.shape: (1519, 25) test.shape: (380, 25)\n",
            "train_scaled.shape: (1519, 25) test_scaled.shape: (380, 25)\n",
            "train_X.shape: (1519, 8, 3) train_y.shape: (1519, 1) test_X.shape: (380, 8, 3) test_y.shape: (380, 1)\n",
            "Epoch 1/500\n",
            "507/507 - 4s - loss: 0.1506 - val_loss: 0.0023 - 4s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "507/507 - 2s - loss: 0.0289 - val_loss: 0.0021 - 2s/epoch - 4ms/step\n",
            "Epoch 3/500\n",
            "507/507 - 2s - loss: 0.0223 - val_loss: 0.0017 - 2s/epoch - 4ms/step\n",
            "Epoch 4/500\n",
            "507/507 - 2s - loss: 0.0184 - val_loss: 0.0014 - 2s/epoch - 4ms/step\n",
            "Epoch 5/500\n",
            "507/507 - 2s - loss: 0.0157 - val_loss: 0.0012 - 2s/epoch - 4ms/step\n",
            "Epoch 6/500\n",
            "507/507 - 2s - loss: 0.0138 - val_loss: 0.0011 - 2s/epoch - 4ms/step\n",
            "Epoch 7/500\n",
            "507/507 - 2s - loss: 0.0125 - val_loss: 0.0010 - 2s/epoch - 4ms/step\n",
            "Epoch 8/500\n",
            "507/507 - 2s - loss: 0.0114 - val_loss: 9.4508e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 9/500\n",
            "507/507 - 2s - loss: 0.0105 - val_loss: 8.8112e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 10/500\n",
            "507/507 - 2s - loss: 0.0097 - val_loss: 8.2317e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 11/500\n",
            "507/507 - 2s - loss: 0.0089 - val_loss: 7.6993e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 12/500\n",
            "507/507 - 2s - loss: 0.0082 - val_loss: 7.2018e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 13/500\n",
            "507/507 - 2s - loss: 0.0075 - val_loss: 6.7250e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 14/500\n",
            "507/507 - 2s - loss: 0.0070 - val_loss: 6.2577e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 15/500\n",
            "507/507 - 2s - loss: 0.0066 - val_loss: 5.8044e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 16/500\n",
            "507/507 - 2s - loss: 0.0063 - val_loss: 5.3820e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 17/500\n",
            "507/507 - 2s - loss: 0.0060 - val_loss: 5.0028e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 18/500\n",
            "507/507 - 2s - loss: 0.0058 - val_loss: 4.6672e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 19/500\n",
            "507/507 - 2s - loss: 0.0056 - val_loss: 4.3689e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 20/500\n",
            "507/507 - 2s - loss: 0.0053 - val_loss: 4.0999e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 21/500\n",
            "507/507 - 2s - loss: 0.0051 - val_loss: 3.8538e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 22/500\n",
            "507/507 - 2s - loss: 0.0049 - val_loss: 3.6263e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 23/500\n",
            "507/507 - 2s - loss: 0.0048 - val_loss: 3.4141e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 24/500\n",
            "507/507 - 2s - loss: 0.0046 - val_loss: 3.2150e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 25/500\n",
            "507/507 - 2s - loss: 0.0044 - val_loss: 3.0274e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 26/500\n",
            "507/507 - 2s - loss: 0.0042 - val_loss: 2.8504e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 27/500\n",
            "507/507 - 2s - loss: 0.0040 - val_loss: 2.6837e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 28/500\n",
            "507/507 - 2s - loss: 0.0038 - val_loss: 2.5272e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 29/500\n",
            "507/507 - 2s - loss: 0.0037 - val_loss: 2.3811e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 30/500\n",
            "507/507 - 2s - loss: 0.0035 - val_loss: 2.2450e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 31/500\n",
            "507/507 - 2s - loss: 0.0034 - val_loss: 2.1193e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 32/500\n",
            "507/507 - 2s - loss: 0.0033 - val_loss: 2.0047e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "507/507 - 2s - loss: 0.0032 - val_loss: 1.9020e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 34/500\n",
            "507/507 - 2s - loss: 0.0031 - val_loss: 1.8112e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 35/500\n",
            "507/507 - 2s - loss: 0.0030 - val_loss: 1.7320e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 36/500\n",
            "507/507 - 2s - loss: 0.0029 - val_loss: 1.6631e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 37/500\n",
            "507/507 - 2s - loss: 0.0028 - val_loss: 1.6035e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 38/500\n",
            "507/507 - 2s - loss: 0.0028 - val_loss: 1.5519e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 39/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.5072e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 40/500\n",
            "507/507 - 2s - loss: 0.0027 - val_loss: 1.4685e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 41/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.4350e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 42/500\n",
            "507/507 - 2s - loss: 0.0026 - val_loss: 1.4058e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 43/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.3805e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 44/500\n",
            "507/507 - 2s - loss: 0.0025 - val_loss: 1.3584e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 45/500\n",
            "507/507 - 3s - loss: 0.0024 - val_loss: 1.3392e-04 - 3s/epoch - 6ms/step\n",
            "Epoch 46/500\n",
            "507/507 - 3s - loss: 0.0024 - val_loss: 1.3223e-04 - 3s/epoch - 5ms/step\n",
            "Epoch 47/500\n",
            "507/507 - 2s - loss: 0.0024 - val_loss: 1.3075e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 48/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.2945e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 49/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.2830e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 50/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.2728e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 51/500\n",
            "507/507 - 2s - loss: 0.0023 - val_loss: 1.2638e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 52/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.2559e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 53/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.2488e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 54/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.2426e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 55/500\n",
            "507/507 - 2s - loss: 0.0022 - val_loss: 1.2370e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 56/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2322e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 57/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2279e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 58/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2243e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 59/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2213e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 60/500\n",
            "507/507 - 2s - loss: 0.0021 - val_loss: 1.2188e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 61/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.2169e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 62/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.2156e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 63/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.2150e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 64/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.2150e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 65/500\n",
            "507/507 - 2s - loss: 0.0020 - val_loss: 1.2157e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 66/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.2172e-04 - 2s/epoch - 5ms/step\n",
            "Epoch 67/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.2195e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 68/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.2229e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 69/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.2272e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 70/500\n",
            "507/507 - 2s - loss: 0.0019 - val_loss: 1.2327e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 71/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.2395e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 72/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.2478e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 73/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.2577e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 74/500\n",
            "507/507 - 2s - loss: 0.0018 - val_loss: 1.2694e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 75/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.2831e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 76/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.2991e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 77/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3174e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 78/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3382e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 79/500\n",
            "507/507 - 2s - loss: 0.0017 - val_loss: 1.3617e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 80/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.3876e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 81/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.4160e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 82/500\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.4463e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 83/500\n",
            "Restoring model weights from the end of the best epoch: 63.\n",
            "507/507 - 2s - loss: 0.0016 - val_loss: 1.4782e-04 - 2s/epoch - 4ms/step\n",
            "Epoch 83: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 26 batch_size 4 window_size 2\n",
            "train.shape: (1524, 7) test.shape: (381, 7)\n",
            "train_scaled.shape: (1524, 7) test_scaled.shape: (381, 7)\n",
            "train_X.shape: (1524, 2, 3) train_y.shape: (1524, 1) test_X.shape: (381, 2, 3) test_y.shape: (381, 1)\n",
            "Epoch 1/500\n",
            "381/381 - 3s - loss: 0.1344 - val_loss: 7.2299e-04 - 3s/epoch - 8ms/step\n",
            "Epoch 2/500\n",
            "381/381 - 1s - loss: 0.0424 - val_loss: 0.0061 - 848ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "381/381 - 1s - loss: 0.0178 - val_loss: 0.0062 - 866ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "381/381 - 1s - loss: 0.0144 - val_loss: 0.0043 - 815ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "381/381 - 1s - loss: 0.0130 - val_loss: 0.0029 - 830ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "381/381 - 1s - loss: 0.0120 - val_loss: 0.0020 - 907ms/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "381/381 - 1s - loss: 0.0113 - val_loss: 0.0014 - 874ms/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "381/381 - 1s - loss: 0.0107 - val_loss: 0.0011 - 876ms/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "381/381 - 1s - loss: 0.0103 - val_loss: 8.3946e-04 - 850ms/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "381/381 - 1s - loss: 0.0100 - val_loss: 7.0353e-04 - 880ms/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "381/381 - 1s - loss: 0.0097 - val_loss: 6.1977e-04 - 840ms/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "381/381 - 1s - loss: 0.0096 - val_loss: 5.6688e-04 - 883ms/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "381/381 - 1s - loss: 0.0094 - val_loss: 5.3228e-04 - 831ms/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "381/381 - 1s - loss: 0.0093 - val_loss: 5.0859e-04 - 848ms/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "381/381 - 1s - loss: 0.0092 - val_loss: 4.9145e-04 - 829ms/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "381/381 - 1s - loss: 0.0091 - val_loss: 4.7826e-04 - 852ms/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "381/381 - 1s - loss: 0.0090 - val_loss: 4.6746e-04 - 834ms/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "381/381 - 1s - loss: 0.0089 - val_loss: 4.5803e-04 - 852ms/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "381/381 - 1s - loss: 0.0088 - val_loss: 4.4935e-04 - 911ms/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "381/381 - 1s - loss: 0.0087 - val_loss: 4.4104e-04 - 878ms/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "381/381 - 1s - loss: 0.0086 - val_loss: 4.3286e-04 - 836ms/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "381/381 - 1s - loss: 0.0085 - val_loss: 4.2463e-04 - 854ms/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "381/381 - 1s - loss: 0.0083 - val_loss: 4.1620e-04 - 835ms/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "381/381 - 1s - loss: 0.0082 - val_loss: 4.0746e-04 - 868ms/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "381/381 - 1s - loss: 0.0081 - val_loss: 3.9834e-04 - 812ms/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "381/381 - 1s - loss: 0.0079 - val_loss: 3.8880e-04 - 837ms/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "381/381 - 1s - loss: 0.0078 - val_loss: 3.7881e-04 - 857ms/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "381/381 - 1s - loss: 0.0076 - val_loss: 3.6837e-04 - 856ms/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "381/381 - 1s - loss: 0.0074 - val_loss: 3.5750e-04 - 867ms/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "381/381 - 1s - loss: 0.0072 - val_loss: 3.4623e-04 - 844ms/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "381/381 - 1s - loss: 0.0070 - val_loss: 3.3460e-04 - 806ms/epoch - 2ms/step\n",
            "Epoch 32/500\n",
            "381/381 - 1s - loss: 0.0069 - val_loss: 3.2268e-04 - 1s/epoch - 4ms/step\n",
            "Epoch 33/500\n",
            "381/381 - 1s - loss: 0.0066 - val_loss: 3.1055e-04 - 850ms/epoch - 2ms/step\n",
            "Epoch 34/500\n",
            "381/381 - 1s - loss: 0.0064 - val_loss: 2.9832e-04 - 848ms/epoch - 2ms/step\n",
            "Epoch 35/500\n",
            "381/381 - 1s - loss: 0.0062 - val_loss: 2.8612e-04 - 807ms/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "381/381 - 1s - loss: 0.0060 - val_loss: 2.7411e-04 - 874ms/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "381/381 - 1s - loss: 0.0058 - val_loss: 2.6245e-04 - 875ms/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "381/381 - 1s - loss: 0.0055 - val_loss: 2.5130e-04 - 821ms/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "381/381 - 1s - loss: 0.0053 - val_loss: 2.4082e-04 - 866ms/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "381/381 - 1s - loss: 0.0050 - val_loss: 2.3120e-04 - 848ms/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "381/381 - 1s - loss: 0.0048 - val_loss: 2.2260e-04 - 852ms/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "381/381 - 1s - loss: 0.0046 - val_loss: 2.1520e-04 - 945ms/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "381/381 - 1s - loss: 0.0044 - val_loss: 2.0916e-04 - 815ms/epoch - 2ms/step\n",
            "Epoch 44/500\n",
            "381/381 - 1s - loss: 0.0041 - val_loss: 2.0463e-04 - 849ms/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "381/381 - 1s - loss: 0.0039 - val_loss: 2.0168e-04 - 864ms/epoch - 2ms/step\n",
            "Epoch 46/500\n",
            "381/381 - 1s - loss: 0.0037 - val_loss: 2.0040e-04 - 827ms/epoch - 2ms/step\n",
            "Epoch 47/500\n",
            "381/381 - 1s - loss: 0.0035 - val_loss: 2.0078e-04 - 828ms/epoch - 2ms/step\n",
            "Epoch 48/500\n",
            "381/381 - 1s - loss: 0.0034 - val_loss: 2.0278e-04 - 857ms/epoch - 2ms/step\n",
            "Epoch 49/500\n",
            "381/381 - 1s - loss: 0.0032 - val_loss: 2.0626e-04 - 851ms/epoch - 2ms/step\n",
            "Epoch 50/500\n",
            "381/381 - 1s - loss: 0.0031 - val_loss: 2.1103e-04 - 855ms/epoch - 2ms/step\n",
            "Epoch 51/500\n",
            "381/381 - 1s - loss: 0.0029 - val_loss: 2.1680e-04 - 828ms/epoch - 2ms/step\n",
            "Epoch 52/500\n",
            "381/381 - 1s - loss: 0.0028 - val_loss: 2.2324e-04 - 871ms/epoch - 2ms/step\n",
            "Epoch 53/500\n",
            "381/381 - 1s - loss: 0.0027 - val_loss: 2.3000e-04 - 831ms/epoch - 2ms/step\n",
            "Epoch 54/500\n",
            "381/381 - 1s - loss: 0.0026 - val_loss: 2.3669e-04 - 862ms/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "381/381 - 1s - loss: 0.0026 - val_loss: 2.4301e-04 - 846ms/epoch - 2ms/step\n",
            "Epoch 56/500\n",
            "381/381 - 1s - loss: 0.0025 - val_loss: 2.4867e-04 - 837ms/epoch - 2ms/step\n",
            "Epoch 57/500\n",
            "381/381 - 1s - loss: 0.0025 - val_loss: 2.5350e-04 - 835ms/epoch - 2ms/step\n",
            "Epoch 58/500\n",
            "381/381 - 1s - loss: 0.0024 - val_loss: 2.5738e-04 - 854ms/epoch - 2ms/step\n",
            "Epoch 59/500\n",
            "381/381 - 1s - loss: 0.0024 - val_loss: 2.6028e-04 - 863ms/epoch - 2ms/step\n",
            "Epoch 60/500\n",
            "381/381 - 1s - loss: 0.0024 - val_loss: 2.6222e-04 - 877ms/epoch - 2ms/step\n",
            "Epoch 61/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 2.6327e-04 - 877ms/epoch - 2ms/step\n",
            "Epoch 62/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 2.6353e-04 - 843ms/epoch - 2ms/step\n",
            "Epoch 63/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 2.6309e-04 - 838ms/epoch - 2ms/step\n",
            "Epoch 64/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 2.6208e-04 - 819ms/epoch - 2ms/step\n",
            "Epoch 65/500\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 2.6060e-04 - 858ms/epoch - 2ms/step\n",
            "Epoch 66/500\n",
            "Restoring model weights from the end of the best epoch: 46.\n",
            "381/381 - 1s - loss: 0.0023 - val_loss: 2.5876e-04 - 902ms/epoch - 2ms/step\n",
            "Epoch 66: early stopping\n",
            "n_epochs 500 num_hidden_layers 1 num_neurons 8 batch_size 2 window_size 7\n",
            "train.shape: (1520, 22) test.shape: (380, 22)\n",
            "train_scaled.shape: (1520, 22) test_scaled.shape: (380, 22)\n",
            "train_X.shape: (1520, 7, 3) train_y.shape: (1520, 1) test_X.shape: (380, 7, 3) test_y.shape: (380, 1)\n",
            "Epoch 1/500\n",
            "760/760 - 4s - loss: 0.0384 - val_loss: 5.8159e-04 - 4s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "760/760 - 2s - loss: 0.0197 - val_loss: 5.4005e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/500\n",
            "760/760 - 2s - loss: 0.0171 - val_loss: 5.0886e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/500\n",
            "760/760 - 2s - loss: 0.0151 - val_loss: 4.8654e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 5/500\n",
            "760/760 - 2s - loss: 0.0136 - val_loss: 4.7034e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/500\n",
            "760/760 - 2s - loss: 0.0124 - val_loss: 4.5851e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode bitstring to numbers\n",
        "def decode(bounds, n_bits, bitstring):\n",
        "    decoded = list()\n",
        "    largest = 2**n_bits\n",
        "    for i in range(len(bounds)):\n",
        "        # extract the substring\n",
        "        start, end = i * n_bits, (i * n_bits)+n_bits\n",
        "        substring = bitstring[start:end]\n",
        "        # convert bitstring to a string of chars\n",
        "        chars = ''.join([str(s) for s in substring])\n",
        "        # convert string to integer\n",
        "        integer = int(chars, 2)\n",
        "        # scale integer to desired range\n",
        "        value = bounds[i][0] + (integer/largest) * (bounds[i][1] - bounds[i][0])\n",
        "        value = np.round(value)\n",
        "        value = int(value)\n",
        "        # store\n",
        "        decoded.append(value)\n",
        "    return decoded\n",
        "\n",
        "# tournament selection\n",
        "def selection(pop, scores, k=3):\n",
        "\t# first random selection\n",
        "\tselection_ix = randint(len(pop))\n",
        "\tfor ix in randint(0, len(pop), k-1):\n",
        "\t\t# check if better (e.g. perform a tournament)\n",
        "\t\tif scores[ix] < scores[selection_ix] : # which individual has the lowest loss\n",
        "\n",
        "\t\t\tselection_ix = ix\n",
        "\treturn pop[selection_ix]\n",
        "\n",
        "# crossover two parents to create two children\n",
        "def crossover(p1, p2, r_cross):\n",
        "\t# children are copies of parents by default\n",
        "\tc1, c2 = p1.copy(), p2.copy()\n",
        "\t# check for recombination\n",
        "\tif rand() < r_cross:\n",
        "\t\t# select crossover point that is not on the end of the string\n",
        "\t\tpt = randint(1, len(p1)-2)\n",
        "\t\t# perform crossover\n",
        "\t\tc1 = np.append(p1[:pt] , p2[pt:])\n",
        "\t\tc2 = np.append(p2[:pt] , p1[pt:])\n",
        "\treturn [c1, c2]\n",
        "\n",
        "# mutation operator\n",
        "def mutation(bitstring, r_mut):\n",
        "\tfor i in range(len(bitstring)):\n",
        "\t\t# check for a mutation\n",
        "\t\tif rand() < r_mut:\n",
        "\t\t\t# flip the bit\n",
        "\t\t\tbitstring[i] = 1 - bitstring[i]\n",
        "\n",
        "# genetic algorithm\n",
        "def genetic_algorithm(evaluate, bounds, n_bits, n_iter, n_pop, r_cross, r_mut):\n",
        "    # initial population of random bitstring\n",
        "    pop = [randint(0, 2, n_bits*len(bounds)).tolist() for _ in range(n_pop)]\n",
        "    # keep track of best solution\n",
        "    best, best_eval = 0, math.inf\n",
        "    # enumerate generations\n",
        "    for gen in range(1, n_iter+1):\n",
        "        print(f\"Generation:{gen}\")\n",
        "        # decode population\n",
        "        decoded = [decode(bounds, n_bits, p) for p in pop]\n",
        "        # evaluate all candidates in the population\n",
        "        scores = [evaluate(ind) for ind in decoded]\n",
        "        # check for new best solution\n",
        "        for i in range(n_pop):\n",
        "            if scores[i] < best_eval: # find the lowest validation loss\n",
        "                best, best_eval = pop[i], scores[i]\n",
        "                # print\n",
        "                print(\">%d, new best combination, Epoch: %d, num_hidden_layers: %d num_neurons:%d batch_size:\\\n",
        "                 %d window_size: %d  = %.8f\" % \\\n",
        "                        (gen,  decoded[i][0],decoded[i][1], decoded[i][2], decoded[i][3], decoded[i][4],\n",
        "                                                            scores[i]))\n",
        "\t\t# select parents\n",
        "        selected = [selection(pop, scores) for _ in range(n_pop)]\n",
        "\t\t# create the next generation\n",
        "        children = list()\n",
        "        for i in range(0, n_pop, 2):\n",
        "\t\t\t# get selected parents in pairs\n",
        "            p1, p2 = selected[i], selected[i+1]\n",
        "\t\t\t# crossover and mutation\n",
        "            for c in crossover(p1, p2, r_cross):\n",
        "\t\t\t\t# mutation\n",
        "                mutation(c, r_mut)\n",
        "\t\t\t\t# store for next generation\n",
        "                children.append(c)\n",
        "\t\t# replace population\n",
        "        pop = children\n",
        "    return [best, best_eval]\n"
      ],
      "metadata": {
        "id": "gECMYhl8he7m"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Add, Reshape, BatchNormalization, Concatenate, MaxPooling1D, RepeatVector, TimeDistributed, Flatten\n",
        "\n",
        "from keras.models import Model\n",
        "import keras.backend as k\n",
        "from tcn import TCN\n",
        "from keras.layers import GRU, RNN, Bidirectional\n",
        "from keras.layers import Layer"
      ],
      "metadata": {
        "id": "j-r_aXekI0bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22FjeiolElsX",
        "outputId": "f35cfe11-675a-4072-c26f-8d544a83fdb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 4, 3) dtype=float32 (created by layer 'input_87')>"
            ]
          },
          "metadata": {},
          "execution_count": 392
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1EvEswvoTXy5",
        "outputId": "31764781-32c0-400a-eb9d-bcba74d4a494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "381/381 - 2s - loss: 0.2452 - val_loss: 0.0139 - 2s/epoch - 5ms/step\n",
            "Epoch 2/500\n",
            "381/381 - 1s - loss: 0.2337 - val_loss: 0.0130 - 704ms/epoch - 2ms/step\n",
            "Epoch 3/500\n",
            "381/381 - 1s - loss: 0.2260 - val_loss: 0.0122 - 662ms/epoch - 2ms/step\n",
            "Epoch 4/500\n",
            "381/381 - 1s - loss: 0.2198 - val_loss: 0.0116 - 702ms/epoch - 2ms/step\n",
            "Epoch 5/500\n",
            "381/381 - 1s - loss: 0.2144 - val_loss: 0.0111 - 658ms/epoch - 2ms/step\n",
            "Epoch 6/500\n",
            "381/381 - 1s - loss: 0.2095 - val_loss: 0.0106 - 670ms/epoch - 2ms/step\n",
            "Epoch 7/500\n",
            "381/381 - 1s - loss: 0.2050 - val_loss: 0.0101 - 711ms/epoch - 2ms/step\n",
            "Epoch 8/500\n",
            "381/381 - 1s - loss: 0.2008 - val_loss: 0.0097 - 669ms/epoch - 2ms/step\n",
            "Epoch 9/500\n",
            "381/381 - 1s - loss: 0.1968 - val_loss: 0.0093 - 677ms/epoch - 2ms/step\n",
            "Epoch 10/500\n",
            "381/381 - 1s - loss: 0.1930 - val_loss: 0.0089 - 703ms/epoch - 2ms/step\n",
            "Epoch 11/500\n",
            "381/381 - 1s - loss: 0.1895 - val_loss: 0.0085 - 692ms/epoch - 2ms/step\n",
            "Epoch 12/500\n",
            "381/381 - 1s - loss: 0.1860 - val_loss: 0.0082 - 714ms/epoch - 2ms/step\n",
            "Epoch 13/500\n",
            "381/381 - 1s - loss: 0.1827 - val_loss: 0.0079 - 708ms/epoch - 2ms/step\n",
            "Epoch 14/500\n",
            "381/381 - 1s - loss: 0.1796 - val_loss: 0.0076 - 699ms/epoch - 2ms/step\n",
            "Epoch 15/500\n",
            "381/381 - 1s - loss: 0.1765 - val_loss: 0.0073 - 691ms/epoch - 2ms/step\n",
            "Epoch 16/500\n",
            "381/381 - 1s - loss: 0.1735 - val_loss: 0.0070 - 702ms/epoch - 2ms/step\n",
            "Epoch 17/500\n",
            "381/381 - 1s - loss: 0.1706 - val_loss: 0.0067 - 675ms/epoch - 2ms/step\n",
            "Epoch 18/500\n",
            "381/381 - 1s - loss: 0.1679 - val_loss: 0.0064 - 710ms/epoch - 2ms/step\n",
            "Epoch 19/500\n",
            "381/381 - 1s - loss: 0.1651 - val_loss: 0.0062 - 695ms/epoch - 2ms/step\n",
            "Epoch 20/500\n",
            "381/381 - 1s - loss: 0.1625 - val_loss: 0.0059 - 704ms/epoch - 2ms/step\n",
            "Epoch 21/500\n",
            "381/381 - 1s - loss: 0.1599 - val_loss: 0.0057 - 681ms/epoch - 2ms/step\n",
            "Epoch 22/500\n",
            "381/381 - 1s - loss: 0.1574 - val_loss: 0.0055 - 693ms/epoch - 2ms/step\n",
            "Epoch 23/500\n",
            "381/381 - 1s - loss: 0.1549 - val_loss: 0.0053 - 654ms/epoch - 2ms/step\n",
            "Epoch 24/500\n",
            "381/381 - 1s - loss: 0.1525 - val_loss: 0.0050 - 696ms/epoch - 2ms/step\n",
            "Epoch 25/500\n",
            "381/381 - 1s - loss: 0.1502 - val_loss: 0.0048 - 704ms/epoch - 2ms/step\n",
            "Epoch 26/500\n",
            "381/381 - 1s - loss: 0.1479 - val_loss: 0.0046 - 698ms/epoch - 2ms/step\n",
            "Epoch 27/500\n",
            "381/381 - 1s - loss: 0.1457 - val_loss: 0.0045 - 716ms/epoch - 2ms/step\n",
            "Epoch 28/500\n",
            "381/381 - 1s - loss: 0.1435 - val_loss: 0.0043 - 662ms/epoch - 2ms/step\n",
            "Epoch 29/500\n",
            "381/381 - 1s - loss: 0.1414 - val_loss: 0.0041 - 680ms/epoch - 2ms/step\n",
            "Epoch 30/500\n",
            "381/381 - 1s - loss: 0.1393 - val_loss: 0.0039 - 707ms/epoch - 2ms/step\n",
            "Epoch 31/500\n",
            "381/381 - 1s - loss: 0.1372 - val_loss: 0.0038 - 647ms/epoch - 2ms/step\n",
            "Epoch 32/500\n",
            "381/381 - 1s - loss: 0.1352 - val_loss: 0.0036 - 705ms/epoch - 2ms/step\n",
            "Epoch 33/500\n",
            "381/381 - 1s - loss: 0.1332 - val_loss: 0.0035 - 655ms/epoch - 2ms/step\n",
            "Epoch 34/500\n",
            "381/381 - 1s - loss: 0.1313 - val_loss: 0.0033 - 671ms/epoch - 2ms/step\n",
            "Epoch 35/500\n",
            "381/381 - 1s - loss: 0.1294 - val_loss: 0.0032 - 668ms/epoch - 2ms/step\n",
            "Epoch 36/500\n",
            "381/381 - 1s - loss: 0.1275 - val_loss: 0.0030 - 722ms/epoch - 2ms/step\n",
            "Epoch 37/500\n",
            "381/381 - 1s - loss: 0.1257 - val_loss: 0.0029 - 714ms/epoch - 2ms/step\n",
            "Epoch 38/500\n",
            "381/381 - 1s - loss: 0.1239 - val_loss: 0.0028 - 669ms/epoch - 2ms/step\n",
            "Epoch 39/500\n",
            "381/381 - 1s - loss: 0.1221 - val_loss: 0.0027 - 702ms/epoch - 2ms/step\n",
            "Epoch 40/500\n",
            "381/381 - 1s - loss: 0.1204 - val_loss: 0.0026 - 667ms/epoch - 2ms/step\n",
            "Epoch 41/500\n",
            "381/381 - 1s - loss: 0.1187 - val_loss: 0.0025 - 710ms/epoch - 2ms/step\n",
            "Epoch 42/500\n",
            "381/381 - 1s - loss: 0.1171 - val_loss: 0.0023 - 678ms/epoch - 2ms/step\n",
            "Epoch 43/500\n",
            "381/381 - 1s - loss: 0.1154 - val_loss: 0.0022 - 681ms/epoch - 2ms/step\n",
            "Epoch 44/500\n",
            "381/381 - 1s - loss: 0.1138 - val_loss: 0.0022 - 652ms/epoch - 2ms/step\n",
            "Epoch 45/500\n",
            "381/381 - 1s - loss: 0.1123 - val_loss: 0.0021 - 666ms/epoch - 2ms/step\n",
            "Epoch 46/500\n",
            "381/381 - 1s - loss: 0.1107 - val_loss: 0.0020 - 695ms/epoch - 2ms/step\n",
            "Epoch 47/500\n",
            "381/381 - 1s - loss: 0.1092 - val_loss: 0.0019 - 705ms/epoch - 2ms/step\n",
            "Epoch 48/500\n",
            "381/381 - 1s - loss: 0.1077 - val_loss: 0.0018 - 664ms/epoch - 2ms/step\n",
            "Epoch 49/500\n",
            "381/381 - 1s - loss: 0.1062 - val_loss: 0.0017 - 716ms/epoch - 2ms/step\n",
            "Epoch 50/500\n",
            "381/381 - 1s - loss: 0.1048 - val_loss: 0.0017 - 695ms/epoch - 2ms/step\n",
            "Epoch 51/500\n",
            "381/381 - 1s - loss: 0.1034 - val_loss: 0.0016 - 676ms/epoch - 2ms/step\n",
            "Epoch 52/500\n",
            "381/381 - 1s - loss: 0.1020 - val_loss: 0.0015 - 662ms/epoch - 2ms/step\n",
            "Epoch 53/500\n",
            "381/381 - 1s - loss: 0.1006 - val_loss: 0.0015 - 691ms/epoch - 2ms/step\n",
            "Epoch 54/500\n",
            "381/381 - 1s - loss: 0.0993 - val_loss: 0.0014 - 682ms/epoch - 2ms/step\n",
            "Epoch 55/500\n",
            "381/381 - 1s - loss: 0.0980 - val_loss: 0.0014 - 724ms/epoch - 2ms/step\n",
            "Epoch 56/500\n",
            "381/381 - 1s - loss: 0.0967 - val_loss: 0.0013 - 640ms/epoch - 2ms/step\n",
            "Epoch 57/500\n",
            "381/381 - 1s - loss: 0.0954 - val_loss: 0.0013 - 677ms/epoch - 2ms/step\n",
            "Epoch 58/500\n",
            "381/381 - 1s - loss: 0.0941 - val_loss: 0.0012 - 653ms/epoch - 2ms/step\n",
            "Epoch 59/500\n",
            "381/381 - 1s - loss: 0.0929 - val_loss: 0.0012 - 659ms/epoch - 2ms/step\n",
            "Epoch 60/500\n",
            "381/381 - 1s - loss: 0.0917 - val_loss: 0.0012 - 712ms/epoch - 2ms/step\n",
            "Epoch 61/500\n",
            "381/381 - 1s - loss: 0.0905 - val_loss: 0.0011 - 663ms/epoch - 2ms/step\n",
            "Epoch 62/500\n",
            "381/381 - 1s - loss: 0.0893 - val_loss: 0.0011 - 669ms/epoch - 2ms/step\n",
            "Epoch 63/500\n",
            "381/381 - 1s - loss: 0.0882 - val_loss: 0.0011 - 690ms/epoch - 2ms/step\n",
            "Epoch 64/500\n",
            "381/381 - 1s - loss: 0.0871 - val_loss: 0.0010 - 715ms/epoch - 2ms/step\n",
            "Epoch 65/500\n",
            "381/381 - 1s - loss: 0.0860 - val_loss: 0.0010 - 650ms/epoch - 2ms/step\n",
            "Epoch 66/500\n",
            "381/381 - 1s - loss: 0.0849 - val_loss: 9.9800e-04 - 753ms/epoch - 2ms/step\n",
            "Epoch 67/500\n",
            "381/381 - 1s - loss: 0.0838 - val_loss: 9.8126e-04 - 657ms/epoch - 2ms/step\n",
            "Epoch 68/500\n",
            "381/381 - 1s - loss: 0.0828 - val_loss: 9.6711e-04 - 656ms/epoch - 2ms/step\n",
            "Epoch 69/500\n",
            "381/381 - 1s - loss: 0.0817 - val_loss: 9.5549e-04 - 677ms/epoch - 2ms/step\n",
            "Epoch 70/500\n",
            "381/381 - 1s - loss: 0.0807 - val_loss: 9.4635e-04 - 683ms/epoch - 2ms/step\n",
            "Epoch 71/500\n",
            "381/381 - 1s - loss: 0.0797 - val_loss: 9.3962e-04 - 652ms/epoch - 2ms/step\n",
            "Epoch 72/500\n",
            "381/381 - 1s - loss: 0.0787 - val_loss: 9.3526e-04 - 711ms/epoch - 2ms/step\n",
            "Epoch 73/500\n",
            "381/381 - 1s - loss: 0.0778 - val_loss: 9.3321e-04 - 704ms/epoch - 2ms/step\n",
            "Epoch 74/500\n",
            "381/381 - 1s - loss: 0.0768 - val_loss: 9.3342e-04 - 674ms/epoch - 2ms/step\n",
            "Epoch 75/500\n",
            "381/381 - 1s - loss: 0.0759 - val_loss: 9.3583e-04 - 678ms/epoch - 2ms/step\n",
            "Epoch 76/500\n",
            "381/381 - 1s - loss: 0.0750 - val_loss: 9.4039e-04 - 632ms/epoch - 2ms/step\n",
            "Epoch 77/500\n",
            "381/381 - 1s - loss: 0.0741 - val_loss: 9.4705e-04 - 647ms/epoch - 2ms/step\n",
            "Epoch 78/500\n",
            "381/381 - 1s - loss: 0.0732 - val_loss: 9.5576e-04 - 636ms/epoch - 2ms/step\n",
            "Epoch 79/500\n",
            "381/381 - 1s - loss: 0.0723 - val_loss: 9.6647e-04 - 690ms/epoch - 2ms/step\n",
            "Epoch 80/500\n",
            "381/381 - 1s - loss: 0.0715 - val_loss: 9.7913e-04 - 637ms/epoch - 2ms/step\n",
            "Epoch 81/500\n",
            "381/381 - 1s - loss: 0.0706 - val_loss: 9.9370e-04 - 684ms/epoch - 2ms/step\n",
            "Epoch 82/500\n",
            "381/381 - 1s - loss: 0.0698 - val_loss: 0.0010 - 655ms/epoch - 2ms/step\n",
            "Epoch 83/500\n",
            "381/381 - 1s - loss: 0.0690 - val_loss: 0.0010 - 663ms/epoch - 2ms/step\n",
            "Epoch 84/500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-418-6a79b23d9e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}/{n_epochs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Train the model for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     history = model.fit(train_X, train_y, callbacks=[early_stopping, mcp_save],\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_build_call_outputs\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m   2219\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2221\u001b[0;31m         \u001b[0mhandle_data_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2222\u001b[0m         \u001b[0moutputs_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2223\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/handle_data_util.py\u001b[0m in \u001b[0;36mcopy_handle_data\u001b[0;34m(source_t, target_t)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtarget_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mHandleData\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m   if (target_t.dtype == dtypes.resource or\n\u001b[0m\u001b[1;32m     41\u001b[0m       target_t.dtype == dtypes.variant):\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;31m# Note: using the intern table directly here as this is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "np.random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                        inter_op_parallelism_threads=1)\n",
        "from keras import backend as K\n",
        "tf.random.set_seed(1234)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), \n",
        "                            config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "# fit the model\n",
        "neurons=195#22\n",
        "batch_size=4\n",
        "window_size=4\n",
        "input_tensor=Input(shape=(window_size, n_features))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "conv1 = Conv1D(filters = int(n_features*2), kernel_size=1, dilation_rate=1 ,padding='same')(input_tensor)\n",
        "conv2 = Conv1D(filters = int(n_features*1), kernel_size=1, dilation_rate=1, padding='same')(conv1)\n",
        "conv4 = Conv1D(filters = int(n_features), kernel_size=4, dilation_rate=1, padding='causal')(conv2)\n",
        "\n",
        "\n",
        "# conv2 = Conv1D(filters = int(n_features/2), kernel_size= int(window_size/2), dilation_rate=1, padding='causal')(conv1)\n",
        "# tcn1 = TCN( nb_filters=int(22), return_sequences=True, kernel_size=2, dilations=[1])(input_tensor)\n",
        "# tcn2 = TCN( nb_filters=22*1, return_sequences=True, kernel_size=2, dilations=[1])(tcn1)\n",
        "# tcn1 = TCN( nb_filters=22*1, return_sequences=True, kernel_size=1, dilations=[1])(input_tensor)\n",
        "# tcn2 = TCN( nb_filters=22/2, return_sequences=False, kernel_size=8, dilations=[1])(tcn1)\n",
        "# conv3 = Conv1D(filters = (n_features*1), kernel_size=4, dilation_rate=1, padding='causal') (conv1)\n",
        "# model= Sequential()\n",
        "# model.add(LSTM(neurons,activation = 'tanh',input_shape=(4, n_features)) )\n",
        "\n",
        "\n",
        "\n",
        "conv4 = Flatten()(conv4)\n",
        "outputs = Dense(steps_ahead)(conv4) # output layer trainable=True\n",
        "model = Model(input_tensor, outputs)\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.0001))\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=2,\n",
        "                                restore_best_weights=True, mode='min')\n",
        "# save the best weights if training is interrupted\n",
        "mcp_save = ModelCheckpoint(os.path.join(directory, 'mdl_wts.hdf5'),\n",
        "                            save_best_only=True,\n",
        "                            monitor='val_loss', mode='min') \n",
        "\n",
        "\n",
        "\n",
        "# Set the initial and total number of epochs\n",
        "initial_epoch = 1\n",
        "n_epochs = 500\n",
        "loss_tracking = list()\n",
        "\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(initial_epoch , n_epochs+1):\n",
        "    print(f'Epoch {epoch}/{n_epochs}')\n",
        "    # Train the model for one epoch\n",
        "    history = model.fit(train_X, train_y, callbacks=[early_stopping, mcp_save],\n",
        "                    epochs=1, batch_size=4, validation_data=(test_X, test_y),\n",
        "                     verbose=2,\n",
        "                     shuffle=False)\n",
        "    # to find for which epoch each loss belongs\n",
        "    validation_loss= model.evaluate(test_X, test_y, verbose=0)\n",
        "    loss_tracking.append(validation_loss)\n",
        "    # Save the model every 10 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        # Save the model  in HDF5 foramt with a filename that includes the epoch number\n",
        "        #model.save(f'model_{epoch}Eps.h5')\n",
        "        model.save(os.path.join(directory, f'model_{epoch}Eps.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEoLtgezmQJg",
        "outputId": "f16ba02c-b4d3-44ce-c6ff-a8f4f3577dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_48\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_57 (InputLayer)       [(None, 4, 22)]           0         \n",
            "                                                                 \n",
            " conv1d_21 (Conv1D)          (None, 4, 1)              23        \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 4, 1)              2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25\n",
            "Trainable params: 25\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tcn --no-dependencies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZrYOYH45_XT",
        "outputId": "3866e1f1-afa4-4458-8647-3a9e19f67aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tcn\n",
            "  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: keras-tcn\n",
            "Successfully installed keras-tcn-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tcn import TCN"
      ],
      "metadata": {
        "id": "BMnR7SV06YzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# conv1 = Conv1D(filters = 46, kernel_size=1, dilation_rate=1, padding='same')(input_tensor)\n",
        "\n",
        "\n",
        "# conv2 = Conv1D(filters = 25, kernel_size=1, dilation_rate=1, padding='same')(conv1)\n",
        "\n",
        "\n",
        "# conv3 = Conv1D(filters = 46, kernel_size=1, dilation_rate=1, padding='same')(conv2)\n",
        "model = Sequential([\n",
        "    model.add(Conv1D(filters = 146, kernel_size=1, dilation_rate=1, padding='same')),\n",
        "    model.add(Conv1D(filters = 125, kernel_size=1, dilation_rate=1, padding='same')),\n",
        "    model.add(Conv1D(filters = 110, kernel_size=1, dilation_rate=1, padding='same')),\n",
        "    model.add(Conv1D(filters = 146, kernel_size=4, dilation_rate=16, padding='causal')),\n",
        "    LSTM(64),\n",
        "    Dense(3)\n",
        "])\n",
        "\n",
        "# model.add(Conv1D(filters = 46, kernel_size=1, dilation_rate=1, padding='same'))\n",
        "# model.add(Conv1D(filters = 25, kernel_size=1, dilation_rate=1, padding='same'))\n",
        "# model.add(Conv1D(filters = 10, kernel_size=1, dilation_rate=1, padding='same'))\n",
        "# model.add(Conv1D(filters = 46, kernel_size=2, dilation_rate=8, padding='causal'))\n",
        "# model.add(Conv1D(filters = 46, kernel_size=4, dilation_rate=16, padding='causal'))\n",
        "\n",
        "# conv4 = Conv1D(filters = 10, kernel_size=1, dilation_rate=1, padding='same')(conv3)\n",
        "# conv5 = Conv1D(filters = 10, kernel_size=1, dilation_rate=1, padding='same')(conv4)\n",
        "\n",
        "\n",
        "# lstm_output = LSTM(neurons, activation=\"tanh\")(conv3)\n",
        " \n",
        "# outputs = Dense(steps_ahead)(lstm_output) # output layer\n",
        "# model = Model(input_tensor, outputs)\n",
        "# model.add(Dense(steps_ahead))\n",
        "\n",
        "# Define the TCN layers\n",
        "# model.add(TCN(nb_filters=46, kernel_size=1, dilations=[1,1,1], nb_stacks=3, input_shape =(window_size, n_features),\n",
        "#           padding='same',# use_skip_connections=True,\n",
        "#            #return_sequences=True,\n",
        "#           activation='relu'#, kernel_initializer='he_normal'#,dropout_rate=0.1\n",
        "#           ))\n",
        "# model.add(LSTM(65))\n",
        "# output layer\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
        "\n",
        "\n",
        "# Set the initial and total number of epochs\n",
        "initial_epoch = 1\n",
        "n_epochs = 500\n",
        "loss_tracking = list()\n",
        "\n",
        "history = model.fit(train_X, train_y,\n",
        "                    epochs=500, batch_size=4, validation_data=(test_X, test_y),\n",
        "                     verbose=2,\n",
        "                     shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "owXCqrE_e_rn",
        "outputId": "ae25c4bb-b2ce-413a-f870-a4a7946844c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d0c8c5681b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# conv3 = Conv1D(filters = 46, kernel_size=1, dilation_rate=1, padding='same')(conv2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m model = Sequential([\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m146\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m125\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'add'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEKVnLYPD47l",
        "outputId": "e6a47993-c994-4b46-d6aa-85cf6ab053ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 4, 46) dtype=float32 (created by layer 'conv1d_4')>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "np.random.seed(12345)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                                        inter_op_parallelism_threads=1)\n",
        "from keras import backend as K\n",
        "tf.random.set_seed(1234)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), \n",
        "                            config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    TCN(input_shape=(window_size, n_features), nb_filters=256, return_sequences=True, dilations=[1, 2, 4, 8, 16, 32]),\n",
        "    keras.layers.Dense(steps_ahead)\n",
        "    ])\n",
        "\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=2,\n",
        "                                restore_best_weights=True, mode='min')\n",
        "# save the best weights if training is interrupted\n",
        "mcp_save = ModelCheckpoint(os.path.join(directory, 'mdl_wts.hdf5'),\n",
        "                            save_best_only=True,\n",
        "                            monitor='val_loss', mode='min') \n",
        "\n",
        "\n",
        "# Set the initial and total number of epochs\n",
        "initial_epoch = 1\n",
        "n_epochs = 700\n",
        "loss_tracking = list()\n",
        "\n",
        "# Run the training loop\n",
        "for epoch in range(initial_epoch , n_epochs+1):\n",
        "    print(f'Epoch {epoch}/{n_epochs}')\n",
        "    # Train the model for one epoch\n",
        "    history = model.fit(train_X, train_y, callbacks=[early_stopping, mcp_save],\n",
        "                    epochs=1, batch_size=4, validation_data=(test_X, test_y),\n",
        "                     verbose=2,\n",
        "                     shuffle=False)\n",
        "    # to find for which epoch each loss belongs\n",
        "    validation_loss= model.evaluate(test_X, test_y, verbose=0)\n",
        "    loss_tracking.append(validation_loss)\n",
        "    # Save the model every 10 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        # Save the model  in HDF5 foramt with a filename that includes the epoch number\n",
        "        #model.save(f'model_{epoch}Eps.h5')\n",
        "        model.save(os.path.join(directory, f'model_{epoch}Eps.h5'))"
      ],
      "metadata": {
        "id": "lZ7I0rvzsWSO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "c6b97dc2-f579-436a-eff2-de62880a5f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-bd94d439ade0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m model = keras.models.Sequential([\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mTCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_ahead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tcn import TCN\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "import keras"
      ],
      "metadata": {
        "id": "s7OzpLLfDDRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnKJQzBtEoFu",
        "outputId": "631f09f7-8020-486f-ad90-745531b8bf65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 4, 4) dtype=float32 (created by layer 'input_4')>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  best tcn architecture is the one used in the papaer"
      ],
      "metadata": {
        "id": "Vi-k3p80zLmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyrxlI_zh4_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2c2411-9fb7-4c32-8658-b05f940841c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00019997876370325685"
            ]
          },
          "metadata": {},
          "execution_count": 421
        }
      ],
      "source": [
        "# load the trained saved model\n",
        "model_saved = tf.keras.models.load_model('/content/drive/MyDrive/my_trained_models/[500, 1, 4, 2, 3]_model.h5')\n",
        "# Load the best weights\n",
        "model_saved.load_weights('/content/drive/MyDrive/my_trained_models/[500, 1, 4, 2, 3]_weights.hdf5')\n",
        "#print the loss of the best weights\n",
        "model_saved.evaluate(test_X, test_y, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_saved = tf.keras.models.load_model('/content/drive/MyDrive/my_trained_models/optimal_lstm_model_200Eps.h5')"
      ],
      "metadata": {
        "id": "ZbIZlT0gMPSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/my_trained_models/mdl_wts.hdf5')\n",
        "model.evaluate(test_X, test_y, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDcUFDrcLz-N",
        "outputId": "66af580e-d3fd-41c0-8e54-4a620bc6a436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.000168306622072123"
            ]
          },
          "metadata": {},
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUQ9MQO4e0uQ",
        "outputId": "601859fd-c6f2-473b-be0a-e4c80fd46c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:121\n",
            "Validation loss: 0.0007662050193175673\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(test_X, test_y, verbose=0)\n",
        "best_epoch = loss_tracking.index(score) + 1\n",
        "# validation loss and corresponding epoch for the saved model\n",
        "print(f'Epoch:{best_epoch}\\nValidation loss: {score}') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfmgjN3OhMYp"
      },
      "outputs": [],
      "source": [
        "# # continue if training is interrupted \n",
        "# model.compile(loss='mean_squared_error',\n",
        "#               optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5))\n",
        "# early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=2,\n",
        "#                                 restore_best_weights=True, mode='min')\n",
        "# # save the best weights if training is interrupted\n",
        "# mcp_save = ModelCheckpoint(os.path.join(directory, 'mdl_wts.hdf5'),\n",
        "#                             save_best_only=True,\n",
        "#                             monitor='val_loss', mode='min') \n",
        "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n",
        "\n",
        "# # Set the initial and total number of epochs\n",
        "# initial_epoch = 1\n",
        "# n_epochs = 2000\n",
        "\n",
        "# # Run the training loop\n",
        "# for epoch in range(initial_epoch , n_epochs+1):\n",
        "#     print(f'Epoch {epoch}/{n_epochs}')\n",
        "#     # Train the model for one epoch\n",
        "#     history = model.fit(train_X, train_y, callbacks=[early_stopping, mcp_save, reduce_lr_loss],\n",
        "#                     epochs=1, batch_size=2, validation_data=(test_X, test_y),\n",
        "#                      verbose=2,\n",
        "#                      shuffle=False)\n",
        "#     # to find for which epoch each loss belongs\n",
        "#     validation_loss= model.evaluate(test_X, test_y, verbose=0)\n",
        "#     loss_tracking.append(validation_loss)\n",
        "#     # Save the model every 10 epochs\n",
        "#     if epoch % 50 == 0:\n",
        "#         # Save the model  in HDF5 foramt with a filename that includes the epoch number\n",
        "#         #model.save(f'model_{epoch}Eps.h5')\n",
        "#         model.save(os.path.join(directory, f'model_{epoch}Eps.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "Om2PkqgiVjri",
        "outputId": "3539c89c-b7df-4061-ff34-94cf2f4ae408"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABB4AAAFlCAYAAABMeyuWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5Qcd33n/c+3rzMjaUZYFzuSTCRLxiAgtkHxygQIwXliiRgr2YDXPgmGhYVsYhJg9wkPWh6ww645ywbH8OzByTFgbAjxJeZikfXGDg+35DlgWb6BZVtGkm1dfJF8kceW5tKX7/NHVU/XtLpHM9PVXTNd79c5c9RVXf3tb5eqp2c+86tfmbsLAAAAAACgEzJJNwAAAAAAAHoXwQMAAAAAAOgYggcAAAAAANAxBA8AAAAAAKBjCB4AAAAAAEDHEDwAAAAAAICOySXdwEwsXbrUV69enXQbAAAAAAAg4p577nnW3Zc1u29eBQ+rV6/Wjh07km4DAAAAAABEmNkTre7jVAsAAAAAANAxBA8AAAAAAKBjCB4AAAAAAEDHEDwAAAAAAICOIXgAAAAAAAAdM6+uagEAAAAAwEwNDw/r0KFDKpVKSbcy7+TzeS1fvlyDg4OzrkHwAAAAAADoWcPDw3rmmWe0cuVK9ff3y8ySbmnecHeNjIzo4MGDkjTr8IFTLQAAAAAAPevQoUNauXKlBgYGCB1myMw0MDCglStX6tChQ7OuQ/AAAAAAAOhZpVJJ/f39Sbcxr/X397d1mgrBAwAAAACgpzHSoT3t7j+CBwAAAAAA0DEEDx209/DL+uGu2Z8HAwAAAADAfEfw0EG33nNAH7xhR9JtAAAAAADmsVtuuUXXX3/9vKnbiOChg4q5rMpVV7lSTboVAAAAAMA8RfCAlor5YPeOEzwAAAAAAFKK4KGDirkweCgTPAAAAAAAZu5973ufvvWtb+nHP/6xzExmpiuuuEKSdNttt2nDhg3q6+vTKaecoo9//OOTLnt54MABXXTRRVq+fLn6+/u1du1afepTnzph3bjlOlIVkoJTLSRpjOABAAAAADALn/rUp7Rv3z4dOXJE11xzjSRp1apVuuWWW3TJJZfoj//4j/XZz35We/bs0datW1WtVvX5z39eknTppZdqZGRE1157rRYvXqy9e/fqkUcembJuJxA8dFBtxMNYieABAAAAAOaKv/zeTj305HAiz71+xaAuf+drp7392rVrddJJJ6larWrjxo2SJHfXX/zFX+jSSy+dCA0kqVgs6rLLLtPWrVu1ZMkSbd++XTfeeKPe+c53SpLe9ra3TVm3UzjVooMKteChXEm4EwAAAABAr3j00Ue1b98+XXTRRSqXyxNfb3/72zU6OqoHH3xQknTWWWdp69atuv7667Vv377E+mXEQwdNjHjgVAsAAAAAmDNmMuJgLnr22WclSe94xzua3r9//35J0s0336xPfvKT+tjHPqYjR47ozDPP1FVXXaXzzjuva71KBA8dVczX5nhgxAMAAAAAIB4nnXSSJOnaa6/V2Weffdz9a9askSStXLlS119/varVqrZv364rrrhCF154ofbt26clS5Z0rV+Chw5ixAMAAAAAoF2FQkGjo6MTy2eccYZWrlypxx9/XB/84AdP+PhMJqONGzfq8ssv15ve9CY98cQTWrJkyXF1O4XgoYMIHgAAAAAA7Xr1q1+t2267Td/97ne1atUqrVixQldddZXe8573aHh4WJs3b1ahUNDevXv13e9+V7feeqtKpZLOP/98XXrppXrVq16lsbExXXXVVTrllFP0mte8pmXdFStWxN4/wUMHTVxOk6taAAAAAABm6U//9E9133336f3vf79eeOEFXX755briiis0ODioz372s7ruuuuUzWZ12mmn6YILLlChUFA2m9XrX/96ffGLX9T+/fs1MDCgjRs36s4771R/f/+UdeNG8NBBXNUCAAAAANCupUuX6jvf+c5x6zdv3qzNmzc3fUwul9OXv/zlWdWNG5fT7CBOtQAAAAAApB3BQwcV8wQPAAAAAIB0I3jooNocD+MEDwAAAACAlCJ46KAiczwAAAAAAFKO4KGDJoIHrmoBAAAAAIlx96RbmNfa3X8EDx1kZipkM8zxAAAAAAAJyefzGhkZSbqNeW1kZET5fH7Wjyd46LBiLsOpFgAAAACQkOXLl+vgwYM6duwYIx9myN117NgxHTx4UMuXL591nVyMPaGJYp4RDwAAAACQlMHBQUnSk08+qVKplHA3808+n9fJJ588sR9ng+Chw4q5LFe1AAAAAIAEDQ4OtvWLM9rDqRYdFpxqQfAAAAAAAEgngocOK+QyGisxxwMAAAAAIJ0IHjqMEQ8AAAAAgDQjeOiwYi7LVS0AAAAAAKlF8NBhxXyGySUBAAAAAKlF8NBhnGoBAAAAAEgzgocOC061IHgAAAAAAKQTwUOHBSMemOMBAAAAAJBOBA8dFlxOkxEPAAAAAIB0InjoMOZ4AAAAAACkGcFDhxXzWa5qAQAAAABILYKHDqvN8eDuSbcCAAAAAEDXETx0WDGXUdWlcpXgAQAAAACQPgQPHVbMZSWJeR4AAAAAAKlE8NBhhVywi8dKXFITAAAAAJA+BA8dVqwFD4x4AAAAAACk0LSCBzPbZGa7zGy3mX2iyf1FM7s5vP8uM1sduW9ruH6XmZ0fWf8xM9tpZg+a2Y1m1hfHC5privlgF3NlCwAAAABAGp0weDCzrKQvSdosab2kS8xsfcNmH5D0gruvk3S1pM+Fj10v6WJJr5W0SdI1ZpY1s5WS/lzSBnd/naRsuF3PYY4HAAAAAECaTWfEwzmSdrv7Xncfl3STpC0N22yRdEN4+1ZJ55mZhetvcvcxd39M0u6wniTlJPWbWU7SgKQn23spc1P9VAvmeAAAAAAApM90goeVkvZHlg+E65pu4+5lSS9KWtLqse5+UNLnJe2T9JSkF939zmZPbmYfMrMdZrbj8OHD02h3bmHEAwAAAAAgzRKZXNLMXqFgNMQaSSskLTCzP2q2rbtf6+4b3H3DsmXLutlmLOpXtSB4AAAAAACkz3SCh4OSTo0srwrXNd0mPHViSNJzUzz2tyU95u6H3b0k6duS3jSbFzDXcaoFAAAAACDNphM83C3pdDNbY2YFBZNAbmvYZpuk94a33yXpB+7u4fqLw6terJF0uqTtCk6x2GhmA+FcEOdJerj9lzP3cFULAAAAAECa5U60gbuXzezDku5QcPWJ69x9p5l9RtIOd98m6auSvmFmuyU9r/AKFeF2t0h6SFJZ0mXuXpF0l5ndKunecP19kq6N/+UljzkeAAAAAABpdsLgQZLc/XZJtzes+3Tk9qikd7d47JWSrmyy/nJJl8+k2fmIUy0AAAAAAGmWyOSSaVIPHhjxAAAAAABIH4KHDuOqFgAAAACANCN46LD6HA+cagEAAAAASB+Chw7LZ01mXNUCAAAAAJBOBA8dZmYq5jLM8QAAAAAASCWChy4o5rIEDwAAAACAVCJ46IJgxANzPAAAAAAA0ofgoQsKuQxXtQAAAAAApBLBQxcwxwMAAAAAIK0IHrqAOR4AAAAAAGlF8NAFxTxzPAAAAAAA0ongoQs41QIAAAAAkFYED13AqRYAAAAAgLQieOiC4KoWnGoBAAAAAEgfgocuKOYyGmfEAwAAAAAghQgeuoBTLQAAAAAAaUXw0AXBVS0IHgAAAAAA6UPw0AXBVS2Y4wEAAAAAkD4ED13AqRYAAAAAgLQieOiCQji5pLsn3QoAAAAAAF1F8NAFxVywm8crjHoAAAAAAKQLwUMX1IIHTrcAAAAAAKQNwUMXFPNZSdJYieABAAAAAJAuBA9dUB/xwJUtAAAAAADpQvDQBZxqAQAAAABIK4KHLpgIHjjVAgAAAACQMgQPXVDMBXM8cFULAAAAAEDaEDx0QX3EA3M8AAAAAADSheChC4p55ngAAAAAAKQTwUMX1E61IHgAAAAAAKQNwUMXcDlNAAAAAEBaETx0QYGrWgAAAAAAUorgoQu4qgUAAAAAIK0IHrqAq1oAAAAAANKK4KELuKoFAAAAACCtCB66oJAleAAAAAAApBPBQxfkshnlMsZVLQAAAAAAqUPw0CXFXIarWgAAAAAAUofgoUsKuQxXtQAAAAAApA7BQ5cUc1lGPAAAAAAAUofgoUuK+QxzPAAAAAAAUofgoUuKuQxXtQAAAAAApA7BQ5cUc1mCBwAAAABA6hA8dEkw4oFTLQAAAAAA6ULw0CWFXEbjjHgAAAAAAKQMwUOXMMcDAAAAACCNCB66hMtpAgAAAADSiOChS7icJgAAAAAgjQgeuoRTLQAAAAAAaUTw0CVcThMAAAAAkEYED13CVS0AAAAAAGlE8NAlwakWzPEAAAAAAEgXgocuKeayKlVclaon3QoAAAAAAF1D8NAlxXywqzndAgAAAACQJgQPXVLMBbua0y0AAAAAAGlC8NAlxVxWkriyBQAAAAAgVaYVPJjZJjPbZWa7zewTTe4vmtnN4f13mdnqyH1bw/W7zOz8yPrFZnarmT1iZg+b2blxvKC5qpDjVAsAAAAAQPqcMHgws6ykL0naLGm9pEvMbH3DZh+Q9IK7r5N0taTPhY9dL+liSa+VtEnSNWE9SfqipH9y91dLOlPSw+2/nLmLUy0AAAAAAGk0nREP50ja7e573X1c0k2StjRss0XSDeHtWyWdZ2YWrr/J3cfc/TFJuyWdY2ZDkt4q6auS5O7j7n6k/Zczd9WCh9ESIx4AAAAAAOkxneBhpaT9keUD4bqm27h7WdKLkpZM8dg1kg5L+pqZ3WdmXzGzBbN6BfNEMc8cDwAAAACA9ElqcsmcpDdI+ht3P1vSUUnHzR0hSWb2ITPbYWY7Dh8+3M0eY8WpFgAAAACANJpO8HBQ0qmR5VXhuqbbmFlO0pCk56Z47AFJB9z9rnD9rQqCiOO4+7XuvsHdNyxbtmwa7c5NRSaXBAAAAACk0HSCh7slnW5ma8ysoGCyyG0N22yT9N7w9rsk/cDdPVx/cXjVizWSTpe03d2flrTfzM4IH3OepIfafC1zWmFixAPBAwAAAAAgPXIn2sDdy2b2YUl3SMpKus7dd5rZZyTtcPdtCiaJ/IaZ7Zb0vIJwQuF2tygIFcqSLnP32rkGfybpm2GYsVfSv4/5tc0pxRxzPAAAAAAA0ueEwYMkufvtkm5vWPfpyO1RSe9u8dgrJV3ZZP39kjbMpNn5bGKOhxJzPAAAAAAA0iOpySVTp5jnVAsAAAAAQPoQPHQJp1oAAAAAANKI4KFLuKoFAAAAACCNCB66pJCtnWrBHA8AAAAAgPQgeOiSTMZUyGY41QIAAAAAkCoED11UzGU0ViJ4AAAAAACkB8FDFxXzGU61AAAAAACkCsFDFxVzWU61AAAAAACkCsFDFxVzGa5qAQAAAABIFYKHLirkONUCAAAAAJAuBA9dVMxxVQsAAAAAQLoQPHRRMZflqhYAAAAAgFQheOgirmoBAAAAAEgbgocu4lQLAAAAAEDaEDx0UTGX5aoWAAAAAIBUIXjoogIjHgAAAAAAKUPw0EVFLqcJAAAAAEgZgocuYo4HAAAAAEDaEDx0UTHP5TQBAAAAAOlC8NBFnGoBAAAAAEgbgocuKuYyqrpUrjDqAQAAAACQDgQPXVTIBbubeR4AAAAAAGlB8NBFxVxWEsEDAAAAACA9CB66qDgx4oF5HgAAAAAA6UDw0EXFfBg8cGULAAAAAEBKEDx0EadaAAAAAADShuChi2qnWowTPAAAAAAAUoLgoYsKzPEAAAAAAEgZgocu4lQLAAAAAEDaEDx0EVe1AAAAAACkDcFDF3FVCwAAAABA2hA8dBGnWgAAAAAA0obgoYu4qgUAAAAAIG0IHrqIq1oAAAAAANKG4KGL6pNLMuIBAAAAAJAOBA9dxBwPAAAAAIC0IXjoonzWZCaNlTjVAgAAAACQDgQPXWRmKuYyGqsw4gEAAAAAkA4ED13Wl89qdJwRDwAAAACAdCB46LJFfTkNj5aTbgMAAAAAgK4geOiyof68hkdKSbcBAAAAAEBXEDx02WBfXi8SPAAAAAAAUoLgocuG+vMaHiV4AAAAAACkA8FDlzHiAQAAAACQJgQPXTY0kNfwCJNLAgAAAADSgeChywb7chopVTReribdCgAAAAAAHUfw0GVD/XlJYp4HAAAAAEAqEDx02WAYPDDPAwAAAAAgDQgeuqwWPAwTPAAAAAAAUoDgocsG+xjxAAAAAABID4KHLhviVAsAAAAAQIoQPHTZYH9OkjQ8yiU1AQAAAAC9j+Chy2qnWjDHAwAAAAAgDQgeuqwvn1UxlyF4AAAAAACkAsFDAob688zxAAAAAABIBYKHBAz25zU8SvAAAAAAAOh9BA8JYMQDAAAAACAtphU8mNkmM9tlZrvN7BNN7i+a2c3h/XeZ2erIfVvD9bvM7PyGx2XN7D4z+8d2X8h8MtiX0/AIV7UAAAAAAPS+EwYPZpaV9CVJmyWtl3SJma1v2OwDkl5w93WSrpb0ufCx6yVdLOm1kjZJuiasV/MRSQ+3+yLmG0Y8AAAAAADSYjojHs6RtNvd97r7uKSbJG1p2GaLpBvC27dKOs/MLFx/k7uPuftjknaH9WRmqyT9rqSvtP8y5hfmeAAAAAAApMV0goeVkvZHlg+E65pu4+5lSS9KWnKCx35B0sclVWfc9Tw31J/X8EhJ1aon3QoAAAAAAB2VyOSSZnaBpEPufs80tv2Qme0wsx2HDx/uQnedN9iXV9Wlo+PM8wAAAAAA6G3TCR4OSjo1srwqXNd0GzPLSRqS9NwUj/0NSRea2eMKTt14u5n9XbMnd/dr3X2Du29YtmzZNNqd+4b685LEPA8AAAAAgJ43neDhbkmnm9kaMysomCxyW8M22yS9N7z9Lkk/cHcP118cXvVijaTTJW13963uvsrdV4f1fuDufxTD65kXBvtzksSVLQAAAAAAPS93og3cvWxmH5Z0h6SspOvcfaeZfUbSDnffJumrkr5hZrslPa8gTFC43S2SHpJUlnSZu1c69FrmjUFGPAAAAAAAUuKEwYMkufvtkm5vWPfpyO1RSe9u8dgrJV05Re0fSfrRdProFYN9QfDAlS0AAAAAAL0ukckl0445HgAAAAAAaUHwkIDaqRbDBA8AAAAAgB5H8JCARcWczAgeAAAAAAC9j+AhAZmMaVExp+FRrmoBAAAAAOhtBA8JGRrIM8cDAAAAAKDnETwkZLAvz6kWAAAAAICeR/CQkKF+RjwAAAAAAHofwUNCBvvyGh4leAAAAAAA9DaCh4Qw4gEAAAAAkAYEDwkZ7M9peISrWgAAAAAAehvBQ0KG+vMaKVU0Xq4m3QoAAAAAAB1D8JCQwf68JDHPAwAAAACgpxE8JGQoDB6Y5wEAAAAA0MsIHhIy2EfwAAAAAADofQQPCZk41YLgAQAAAADQwwgeEjLUn5PEiAcAAAAAQG8jeEhIfXJJLqkJAAAAAOhdBA8Jqc3xwKkWAAAAAIBeRvCQkL58VsVchuABAAAAANDTCB4SNNifZ44HAAAAAEBPI3hI0FB/XsOjBA8AAAAAgN5F8JCgwb4cIx4AAAAAAD2N4CFBQ/15DY9wVQsAAAAAQO8ieEgQczwAAAAAAHodwUOCmOMBAAAAANDrCB4SNNiX1/BISdWqJ90KAAAAAAAdQfCQoKH+vKouHR1nngcAAAAAQG8ieEjQYH9OkpjnAQAAAADQswgeEjTUn5ckrmwBAAAAAOhZBA8JGuwLggdGPAAAAAAAehXBQ4IGayMeuLIFAAAAAKBHETwkqHaqBSMeAAAAAAC9iuAhQRMjHggeAAAAAAA9iuAhQYuKOZkRPAAAAAAAehfBQ4IyGdOiYk7Do1zVAgAAAADQmwgeEjbYn2eOBwAAAABAzyJ4SNhQf55TLQAAAAAAPYvgIWGDfYx4AAAAAAD0LoKHhA315zU8SvAAAAAAAOhNBA8JG+zPMeIBAAAAANCzCB4SFszxwFUtAAAAAAC9ieAhYYN9eY2UKhovV5NuBQAAAACA2BE8JGxoIC9JzPMAAAAAAOhJBA8JG+oPgocXjo4n3AkAAAAAAPEjeEjYry5ZIEna++zRhDsBAAAAACB+BA8JW7ssCB52H3o54U4AAAAAAIgfwUPCFvXldcpgn/YcJngAAAAAAPQegoc5YO3yBdrDiAcAAAAAQA8ieJgD1i1bqD2Hj8rdk24FAAAAAIBYETzMAeuWL9TLY2U9MzyWdCsAAAAAAMSK4GEOWLtsoSQmmAQAAAAA9B6Chzlg3fJa8PBSwp0AAAAAABAvgoc5YNmiohb15bTn8NGkWwEAAAAAIFYED3OAmWntsoWcagEAAAAA6DkED3PEuuULtfswwQMAAAAAoLcQPMwR65Yv1OGXxvTiSCnpVgAAAAAAiA3BwxxRu7LFHkY9AAAAAAB6yLSCBzPbZGa7zGy3mX2iyf1FM7s5vP8uM1sduW9ruH6XmZ0frjvVzH5oZg+Z2U4z+0hcL2i+ql/ZguABAAAAANA7Thg8mFlW0pckbZa0XtIlZra+YbMPSHrB3ddJulrS58LHrpd0saTXStok6ZqwXlnSf3b39ZI2SrqsSc1UOfUV/SpkM4x4AAAAAAD0lOmMeDhH0m533+vu45JukrSlYZstkm4Ib98q6Twzs3D9Te4+5u6PSdot6Rx3f8rd75Ukd39J0sOSVrb/cuavXDaj1UsHtIcRDwAAAACAHjKd4GGlpP2R5QM6PiSY2Mbdy5JelLRkOo8NT8s4W9Jd02+7N61bziU1AQAAAAC9JdHJJc1soaRvSfqouw+32OZDZrbDzHYcPny4uw122bplC7Xv+WMaK1eSbgUAAAAAgFhMJ3g4KOnUyPKqcF3TbcwsJ2lI0nNTPdbM8gpCh2+6+7dbPbm7X+vuG9x9w7Jly6bR7vy1dvlCVV16/NljSbcCAAAAAEAsphM83C3pdDNbY2YFBZNFbmvYZpuk94a33yXpB+7u4fqLw6terJF0uqTt4fwPX5X0sLv/dRwvpBfULqnJ6RYAAAAAgF6RO9EG7l42sw9LukNSVtJ17r7TzD4jaYe7b1MQInzDzHZLel5BOKFwu1skPaTgShaXuXvFzN4s6T2SfmFm94dP9V/c/fa4X+B8snbZQpmJK1sAAAAAAHrGCYMHSQoDgdsb1n06cntU0rtbPPZKSVc2rPtXSTbTZntdfyGrlYv7GfEAAAAAAOgZiU4uieOtXcaVLQAAAAAAvYPgYY5Zt3yh9j77sqpVT7oVAAAAAADaRvAwx6xbvlCjpaoOHhlJuhUAAAAAANpG8DDHTFzZggkmAQAAAAA9gOBhjlm3PAge9jDPAwAAAACgBxA8zDEnLSjopAUFLqkJAAAAAOgJBA9z0KtOXqh7nzgidyaYBAAAAADMbwQPc9Dv/toK7XrmJe18cjjpVgAAAAAAaAvBwxx04ZkrVMxldPPd+5NuBQAAAACAthA8zEFD/Xltet0puu3+gxotVZJuBwAAAACAWSN4mKMu2nCqhkfLumPn00m3AgAAAADArBE8zFHnnrZEq17Rr3/YcSDpVgAAAAAAmDWChzkqkzG9+42n6v/b86z2P38s6XYAAAAAAJgVgoc57A/euFKS9K17GfUAAAAAAJifCB7msFWvGNCb1y3VP+w4oGrVk24HAAAAAIAZI3iY49694VQdPDKin+59LulWAAAAAACYMYKHOe531p+sof68br57f9KtAAAAAAAwYwQPc1xfPqvfO2uF/mnn03rxWCnpdgAAAAAAmBGCh3ngol8/VePlqv76n3cl3QoAAAAAADNC8DAPvHbFkD7w5jW64adP6I6dTyfdDgAAAAAA00bwME98fNMZev3KIX381p/r4JGRpNsBAAAAAGBaCB7miWIuq/95ydmqVF0fufE+lSvVpFsCAAAAAOCECB7mkdVLF+jK33+ddjzxgq7+/qNJtwMAAAAAwAkRPMwzW85aqX+34VRd86M9+tdfPpt0OwAAAAAATIngYR66/ML1Wrtsof7sxnv14MEXk24HAAAAAICWCB7moYFCTl+5dIMGCjldcu3PtP2x55NuCQAAAACApgge5qnVSxfoH/7juVo2WNSl192lH+06lHRLAAAAAAAch+BhHluxuF+3/PG5Om3pQn3w6zv0v37+VNItAQAAAAAwCcHDPLd0YVE3fmijzly1WH924736yr/slbsn3RYAAAAAAJIIHnrCUH9eX//AOTrvNSfrv/2vh/XBr+/QC0fHk24LAAAAAACCh14xUMjp2ve8UZ++YL1+/Ohhbf7iv+iuvc8l3RYAAAAAIOUIHnqImen9b16jb//Jb6gvn9ElX/6ZvvD9RzVeribdGgAAAAAgpQgeetDrVw3pH//8LbrwzBX6wvd/qU1f/AlXvQAAAAAAJILgoUctLOb0hYvP1tfe9+tyl973tbv1H264W48/ezTp1gAAAAAAKULw0ON+69XLdcdH36qtm1+tn+55Tr9z9U/0l9/bqYNHRpJuDQAAAACQAjafLr24YcMG37FjR9JtzFuHhkf1V3fs0nfuOyiXdMGv/Yo++JbT9LqVQ0m3BgAAAACYx8zsHnff0PQ+gof0efLIiK7718d04/Z9Ojpe0W+sW6J3vXGVfvs1J2tRXz7p9gAAAAAA8wzBA5p6caSkm7bv09d/+oQOHhlRIZfRb52xTBf82gqd95rlGijkkm6xa6pV11i5qtFSRaPlisZKVVXc5e6qVKWquypVl7tUcVc1vC9jpmym/pXLmDJmymUyymRUvy+yLp/NKJ/NKJuxpF82AAAAAMSC4AFTqlZd9+1/Qd974Cnd/oundOilMeWzptevHNI5a5bo36w5SW9c/QoNztHREJWqa3ikpBdHSjoyUtKRY+PB7WPh10iw/OKx4P6XRksaLYUhQ6mi0XI1kUuOZiwIIQq5jAphGJHP2cTtQi78N5tRPpdRIWuT1gf32eTliVqmfLi+GLk/nw3qT36MqZDNKp+ziXW1xxCOAAAAAJgOggdMW6Xq2v7Y8/rJLw9r+2PP6+cHjqhUcZlJrzxpQGuXLdTaZQu0bvlCrV6yQMsWFbV0UVGLijmZzanGgYsAABgtSURBVP6X1PFyVUfHyno58hUEB/UQoXmwMK7h0fKUtRcVcxoayGvxQF6L+wtaWMypv5BVXz6jYi6rvnxwuy+fVV8u+LeYzyhjNjGiIWOaWM5kgttmFoyEqLgq4YiI477cVa66qtX6v6VqVeWKq1QJAo/xSlWlSlWlsmu8Ei6Xg3XjkfW17UuVqkqVcNuJ5WBd3BILR6LPF31MziYtF7IZZQhHAAAAgMRNFTykZyw9piWbMZ27donOXbtEkjQyXtF9+1/Q3Y+9oEcPvaQ9h17Wv+5+9rgRAoVcRksXFLSoLz/xC30xH/xCWT89QXJ3jZWqemmsrKPh10tj5ROOOMiYtHigoMX9eQ0N5HXSgoJOW7pAiwcKGuoPQ4UwWBgayAfr+vMa7M8rn03HxVvcfSKQiAYXQTjhDcu1IGPy9sE2Pnmb6GPKQZ2xSY8Jth8ZKU0KQSaHKrXb8YcjffmMBgo59eezGigEX/2F7MS6/ui6fG7idm372rb129mJxxWymbYCNQAAAAAEDziB/kJWb1q7VG9au3RiXaXqOvjCiB5/7qieOzqmZ18a17Mvj+nwy2M6OlaemCvhpdGynitXJ0YIBF/BX9BXLu7TwmJOC4o5LezLaWEh+HdBMadF4brF/QUtHgiChoWFHH/ZPgEzC06byGWkYtLdNFcLR6KjN8YjQUV9lEdtVEdF42HYMfkxwfajpYpGShUdGy9rZLyqkVJZx8YrOjZe0eGXxsL1FR0rVTQyXtHYDE+pyWZMA5PCi5wWRAKKBYVg9MyCYiT4KOY0EL1dqAUik29zGgsAAADSguABM5bNmF65ZECvXDKQdCuYZ6LhyIIEwpFK1SNBRWUipBgt1W7X1zcGGkfHauuDU4EODY/pWKmsY2P17WeimMs0DSRqIy9qocZAJNhYUAzCj4F8VgPFYPsFDaM2ijlGaQAAAGBuIXgAkBrZjGlhMaeFxfi/9VUnQo0gsDg23ng7+HdkvBKEGJNCiyDYGBmv6Onh0Ynw42j4mEp1+qeoZExThhnBSIz6qSi1MKM2v0l0zpP+fPO5UNJy+hIAAADiQfAAADHIZEwLwtOH4jzXxT2Yh2NkvKKj4xWNhGHE0bHJgcWx8XJ4fxBYjDQEH8OjZT0zPDopEBktze5qLtmMTQoqivmM+nJBMNFfyIa3w/X57MR9fZEgo5gLJwmtTRQa3i7mMipks/XbDdsxogMAAGD+IXgAgDnMzMJf1LNaHPPZTROnnowFIcRoObzEbMPlZkdLFY1F15eD2yPhNmMN648cK03UGYtsO5ORG1OpXRa2mM9OCi0aA4xacJHL1K+YksvapCuoBPcF63LhVVhy2YxyGYs8tna/TTwunz3+vkJYY+J2JrgqDkEJAABIO4IHAEipTp560kypEoYY5fBSsuX6VVPGIstjpcrE+mbb1LettNymNrnteKWqcqV+ZZdy1YPJS8PL2pZjCkNaMZPymcmhRS5crgUTzZazGYusC0KMxuVs1pSvLWeDxzYu58KvbCQIyWcjNZosB8819XK2xTpCFgAA0AzBAwCgK2ojDRYl3UhE7Uor5Wp4udgwkIheGnYisAjX1e+v3Re5HbmvXKmqVK2tn7xNueqqVIO6zZYrVddYORglUgqXy9Vqw3L4mInHB9t0OEuZUmMQkc8GIUq2YX02EqBkomHGibY1mwhdmi1nbXLwEiwHoc3EtpOWM02fd3JPGWUyarnt5NdX3zZjIogBACBE8AAASK2JK60oIxWS7iYe1erkIKIxmGhcbhZuTCccmU54UqpUVfVw2SOBSePyRB9VjZS84TU0LldVqWriuSqRr06PYJmpqYOV40OZjNmk0Sr15eODl2zTIKUevEwKWhr6qPfSPPxp3Lbeb3B57Gy0Nwu2r4U8mUjvtfAnk1G9ljE6BgDSiOABAIAeksmYCpnaL3XZRHtJQmNIUa2qHqg0hBQTocjEaJHochiORIKaICwJwpuqR56nyfLEtscFJ62Wm/c4VqqqXK203DZaq3m/cyuMqTHTRGiRy0QCjInQQhMjSI4LN6wethwfbmSUNUXqNHmOieeqjU4JbkefI9vQU7TX+nb1IOa4xzSsr2+nidE2Ez2f6DFWD5tq+4PwBsB8Q/AAAAB6RtqDl0buDUFLtR5ORJejQUnTICUMMqrVybdrdWphSXBbE6NYKh6EQZXINpXoY6J1JrZr8pjIc0d7q9WpVhWe+lSZtF2loc6k5/DglKiq67he5gMzNRldUgstJo9OmQhwIiGHRQKaYJ0m3Z9pGOGSiYQtE9tY8J7LWP35M9G6kVDnuG0mbkfqWqT/ieeoB0W1bZo+Zhp1a685eP0WuV0flVMLxWrhTtP9yalUwIwRPAAAAPQoC0cH5MhgZiQafETDjXK1GgYUmhSGNA9gfNKpRlM9ptoQzEwOeTRFyHP8Y6quyO16yBJ9rmgvVQ+ewxvqjYeBk3s9rPFImONhYFP1eqBTjQRNE73Utpl4/qT/d+ORaQxqamFJJHSZFMJkJocitcdbJCCxyPpauFFfbgiLWtwfrZ3JTH6e42tPDo+muv+42pPua/3Y7Anuj/adnfQ8J3iN4f5sen+mSe3a/Rk1fW4Cpc4jeAAAAAAiMhlTRqY8gU1H1AOPIASp324RVkxnG1fzcGXSaJkThTCNoU3tOVQPYcLtm4UwrYKaaG8Vd2lSH8F2teephvWqHn3u6qQe6q+j9WODbSfXmfTYyP2VJvd7jwREs1ELI0z1cCIaUFhtm0x0m1qIocg29ZFDk2vVgw+pHqSY6uGHmfT195+jRX355HZEzAgeAAAAAHRNLdjhF5G5yycFIPUwohbmeHWKUKNxREyL+6sNQchU93skpJr0uGj4Um0MbuohUbR+s+d21dcp+ryqP6YxmKktu+p9+KS+JgdD9Vqttwu2CZZ7bfQF73cAAAAAwISJv+Crt375RXIySTcAAAAAAAB6F8EDAAAAAADomGkFD2a2ycx2mdluM/tEk/uLZnZzeP9dZrY6ct/WcP0uMzt/ujUBAAAAAMD8d8Lgwcyykr4kabOk9ZIuMbP1DZt9QNIL7r5O0tWSPhc+dr2kiyW9VtImSdeYWXaaNQEAAAAAwDw3nREP50ja7e573X1c0k2StjRss0XSDeHtWyWdZ8E0nFsk3eTuY+7+mKTdYb3p1AQAAAAAAPPcdIKHlZL2R5YPhOuabuPuZUkvSloyxWOnUxMAAAAAAMxzc35ySTP7kJntMLMdhw8fTrodAAAAAAAwA9MJHg5KOjWyvCpc13QbM8tJGpL03BSPnU5NSZK7X+vuG9x9w7Jly6bRLgAAAAAAmCumEzzcLel0M1tjZgUFk0Vua9hmm6T3hrffJekH7u7h+ovDq16skXS6pO3TrAkAAAAAAOa53Ik2cPeymX1Y0h2SspKuc/edZvYZSTvcfZukr0r6hpntlvS8giBB4Xa3SHpIUlnSZe5ekaRmNeN/eQAAAAAAIEkWDEyYHzZs2OA7duxIug0AAAAAABBhZve4+4Zm9835ySUBAAAAAMD8RfAAAAAAAAA6Zl6damFmhyU90YHSSyU924G61KY2tak9H2rPx56pTW1qU5vafC5Qm9rUnlu1f9Xdm16Kcl4FD51iZjtanYtCbWpTm9q9Xns+9kxtalOb2tTmc4Ha1Kb23K9dw6kWAAAAAACgYwgeAAAAAABAxxA8BK6lNrWpTe0U156PPVOb2tSmNrX5XKA2tak992tLYo4HAAAAAADQQYx4AAAAAAAAHUPwEDKzK8zsoJndH369I4aam8xsl5ntNrNPxNFnpPbjZvaLsNcdbda6zswOmdmDkXUnmdk/m9kvw39fEWPttve1mZ1qZj80s4fMbKeZfSSuvqeoHUfffWa23cweCGv/Zbh+jZndFR4rN5tZIcba15vZY5G+z5pp7chzZM3sPjP7x7j6nqJ2LH03e6/EeHw3qx3L9xIzW2xmt5rZI2b2sJmdG2PfzWrHcXyfEXn8/WY2bGYfjel92ap2XPv7Y+H75kEzuzF8P8VyfLeo3fbxbWYfCWvuNLOPhuviOkaa1Z71vrYZfM5Y4P8J9/vPzewNMdZ+m5m9GHkNn55F7XeH+6VqZhsatt8a9r3LzM6Po66ZrTazkUjPfzuLnv8qfL//3My+Y2aLZ9rzTGvH1Pd/Deveb2Z3mtmKcH0cx0ir2m0fI5H7/rOZuZktjavvKWrHcWy3fI/HcJw0rR3HcRKu/7PwONxpZv8jrr5b1Y7p+L458vjHzez+uPpuVTumvs8ys5+Fj99hZueE6+N4X7aqHcfxfaaZ/dSCn9u+Z2aDkfva3d9Na89kf9sMf7eZyf6eRe0Z7e9pc3e+gtNNrpD0f8ZYLytpj6TTJBUkPSBpfYz1H5e0NKZab5X0BkkPRtb9D0mfCG9/QtLnYqzd9r6W9CuS3hDeXiTpUUnr4+h7itpx9G2SFoa385LukrRR0i2SLg7X/62kP4mx9vWS3hXTsfKfJP29pH8Ml9vue4rasfTd7L0S4/HdrHYs30sk3SDpP4S3C5IWx9h3s9qx9B15jqykpyX9alx9t6gdx/typaTHJPWHy7dIel9M78tWtds6viW9TtKDkgYk5SR9X9K6OPb1FLVnva81g88ZSe+Q9L8VfE/bKOmuGGu/TeH3mDb6fo2kMyT9SNKGyPr1Cj7ri5LWKPgZIBtD3dXR7WbZ8+9IyoW3PxfZH9PueRa14+h7MHL7zyX9bYzHSKvabR8j4fpTJd0h6QmFnxNx9D1F7TiO7SvU5D0e03HSqnYcx8lvKfg+VQyXl8fYd6vabffdcP9Vkj4dV99T1I5jf98paXPkmP5RXMf3FLXjOL7vlvSb4e33S/qvMR4nrWpPe39rhr/bzGR/z6L2jPb3dL8Y8dA550ja7e573X1c0k2StiTcU1Pu/hNJzzes3qLglxOF//5ejLXb5u5Pufu94e2XJD2s4Af8tvueonYcfbu7vxwu5sMvl/R2SbeG62fbd6vasTCzVZJ+V9JXwmVTDH03q90FsRzfnWJmQwo+2L4qSe4+7u5HFEPfU9SO23mS9rj7E4p/f0drxyUnqd/Mcgp+4X5KMR3fTWo/2WavUvBL6l3ufszdy5J+LOnfKp593ar2rM3wc2aLpK+H39N+Jmmxmf1KTLXb7tvdH3b3XU023yLpJncfc/fHJO1W8LNAu3Xj6PnO8P9Skn4madVMe55F7Tj6Ho4sLlD9M63tY2SK2m33Hbpa0scb6sZxbLeqHVffzbR9nMSlRe0/kfTf3X0s3OZQuD6OvlvVjqNvSRM/T10k6cYY+25VO46+XVJttMCQ6p9pcRzfrWrH0ferJP0kvP3Pkv4g0ne7+7tV7Zn0PNPfbaa9vzv5e9NMEDxM9uFwqMp1NsvhqRErJe2PLB9QTL+8hlzSnWZ2j5l9KMa6NSe7+1Ph7aclnRxz/dj2tZmtlnS2gr/wx9p3Q20phr4tOKXgfkmHFHxz2iPpSOQHt1kfK4213b3W95Vh31ebWXE2tSV9QcEPOtVweUlcfTepXRNH383eK3EdJ63eh+0eJ2skHZb0NQtOP/mKmS2Iqe9WtePoO+pi1X/Qifv7SbS21Gbf7n5Q0ucl7VMQOLwo6R7FcHw3q+3ud4Z3t3N8PyjpLWa2xMwGFPzl41TFs69b1ZbiPUZa9RrH5+dU++FcC05J+99m9tqZNj2FTn7urwnfrz82s7e0Wev9Cv5KJsXfc7S2FEPfZnalme2X9IeSasN9Y+m7RW2pzWPEzLZIOujuDzTc1XbfU9SW4jm2m73H4zpOWn3/aPc4eZWC71l3hTV+Pca+W9WOo++at0h6xt1/GS7H+b5srC213/dHJf1V+N75vKSt4fo4+m5VW2r/+N6p+h+B363651ocfbeqLc1if0/zd5tZ9T2D35ti/6xMVfBgZt+34JzVxq8tkv5G0lpJZyn44fCqRJs9sTe7+xskbZZ0mZm9tVNP5O6uGP9yrhj3tZktlPQtSR9t+OtF2303qR1L3+5ecfezFPxV6BxJr55tjyeqbWavU/BN+9WSfl3SSZL+r5nWNbMLJB1y93vi6nUatdvuOzTle6XN46RZ7TiOk5yCYXx/4+5nSzqqYAhcHH23qh3n+7Ig6UJJ/9B4Xwzvy8babfcd/gC8RUEos0LBXz83zbbHE9U2sz9Sm8e3uz+sYFj7nZL+SdL9kioN28xqX09Ru2Ofkx34nGlV+15Jv+ruZ0r6n5K+24nnjNlTkl4Zvl//k6S/t8i5yTNhZp+UVJb0zRj7a1U7lr7d/ZPufmpY98Nx9TtF7baOkTCs+y+aHGTE4gS14zi2O/mzcKvacRwnOQXfRzdK+gtJt5iZxdJ169qxvS8lXaJZjkiYRe04+v4TSR8L3zsfUziCMiatasdxfL9f0p+a2T0KTjcYj6HfE9We8f7u8u82rWp35LMyVcGDu/+2u7+uyddt7v5M+EtbVdKXNcUQm2k6qMlp16pwXSzCv6LVhnx9R+332+gZC4frhP/OamhZM3HtazPLK3jzfNPdvx2ujqXvZrXjPkY8GN7+Q0nnKhgelQvvavtYidTeFA6vcg+GCX5Ns+v7NyRdaGaPKzht6O2SvhhT38fVNrO/i6nvVu+VWI6TZrVjOk4OSDoQGbFyq4KwII6+m9aO+fjeLOled38mXI7z+8mk2jH1/duSHnP3w+5ekvRtBcdlHMd3s9pviuP4dvevuvsb3f2tkl5QcM5mXMf2cbU78DnZqtc4Pj+b1nb3YQ9PSXP32yXlLZycLwYd+dz3YPjvc+HtexSMknvVTOuY2fskXSDpD8MfMKWYem5WO66+I76p+vDluPf1RO0YjpG1CoLGB8LPtVWS7jWzU2Lou2XtOI7tKd7jbe/vVrVjOk4OSPp2+D11u4LRk0vj6LtV7RjflzkFp7LdHFkd1/vyuNox9f1eBZ9lUvBHgNiOk1a1Yzq+H3H333H3NyoIY/bE1Xer2jPd3zP83WZGfc+kdqc+K1MVPEzFJp8T8/sKhpq2425Jp1swK3pBwdDgbW3WlCSZ2QIzW1S7rWBip3b7bbRNwZtf4b+3xVU4jn0dps1flfSwu/915K62+25VO6a+l1l9xu9+Sf+HgvOsfijpXW323az2I5FvKKbg3K0Z9+3uW919lbuvVnAs/8Dd/zCOvlvU/qM4+p7ivRLHcdK0dhzHibs/LWm/mZ0RrjpP0kNx9N2qdszfAxv/whLn95NJtWPqe5+kjWY2EB5vtf3d9vHdovbDMR3fy8N/X6ngh8u/V0z7ulntDnxOtup1m6RLLbBRwekpTzUrMNPaZnZK7S+hFsyWnpH03OxfwnHPebGZFc1sjaTTJW1vt2j4vT0b3j4trLt3hjU2KTid7UJ3PxZnz61qx9T36ZHFLZIeifTd1jHSqna7x4i7/8Ldl7v76vBz7YCCcPfpdvueqnYcx/YU7/E4jpOmteM4ThT8Nfa3whqvUjBp8rNx9N2qdkx9S0E4/Yi7H4isi+t7yXG1Y+r7SUm/Gd5+u6TaaRxxfO9uWjum47v2uZaR9H8rmDS61ne7x3fT2jPZ37P43Wba+3umtTv2Wekxz1Y5X78kfUPSLyT9PPxP+JUYar5DwV+g9kj6ZIy9nqZg9tUHFJxT1FZtBT/EPyWppOBD7AMKzt//fxW84b8v6aQYa7e9ryW9WcFwoJ8rGAZ8f7i/2+57itpx9P1rku4Lazyo+izDpyn4JrdbQcJbjLH2D8K+H5T0dwqvfNHG8fI21a880XbfU9Ruu+9W75WYjpNWtWP5XqJgOOqOsM53Jb0ixvdls9px9b1AwYfTUGRdXH03qx1X33+p4BePB8OaxbiO7xa14zi+/0VBQPKApPNi3tfNas96X2sGnzMKZuj+koLPzl8ocoWHGGp/OHy/PqBgIsQ3zaL274e3xyQ9I+mOyPafDPvepXBm9nbrKvhL/E4Fn0X3SnrnLHrereBc4Npn2t/OtOeZ1o6p728peI/8XNL3JK2M8RhpVbvtY6Th/sdVv/JE231PUTuOY7vlezyG46Rp7ZiOk4KC76MPhjXeHmPfTWvH0Xe4/npJ/7HJ9m313ap2TPv7zQrmQXpAwTwBb4zxfdmqdhzH90cU/F72qKT/LsliPE6a1p7J/tYMf7eZyf6eRe0Z7e/pftV2CgAAAAAAQOw41QIAAAAAAHQMwQMAAAAAAOgYggcAAAAAANAxBA8AAAAAAKBjCB4AAAAAAEDHEDwAAAAAAICOIXgAAAAAAAAdQ/AAAAAAAAA65v8HVbYldTlZLgEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(18,6)) \n",
        "ax.xaxis.set_major_locator(plt.MultipleLocator(5))\n",
        "#plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(loss_tracking, label='test')\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHohbk8uYRIh"
      },
      "source": [
        "**MinMax Scaler equation**\n",
        "$$x' = \\frac{(x - min)}{(max - min)} \\times (new\\ max\\ value - new\\ min\\  value) + new\\ min\\ value$$<br/>\n",
        "\n",
        "\n",
        "$$x = \\frac{(max - min)\\times (new\\ min\\ value + x') + (new\\ max\\ value - new\\ min\\ value)\\times min}{new\\ max\\ value - new\\ min\\ value}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$x$ is the inverse scaled value\n",
        "$x'$ is the scaled value\n",
        "$min$ is the minimum value of the original data\n",
        "$max$ is the maximum value of the original data.<br/>\n",
        "$new\\ max\\ value$ and $new\\ min\\ value$ is the new range that we want to scale the data to. For example: $(1,0)\\ or\\ (1,-1)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ3IIvIhvHaz",
        "outputId": "188b78a1-c74b-46db-a471-eea012e4190f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 1s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "# make a prediction \n",
        "# select the number of obersvtions for prediction\n",
        "n_obs = len(test)\n",
        "yhat = model.predict(test_X[-n_obs:], verbose=1)\n",
        "\n",
        "\n",
        "# invert scaling \n",
        "scaled_y = pd.DataFrame(test_y)\n",
        "scaled_yhat = pd.DataFrame(yhat) ## ravel () converting into 1D array\n",
        "#obtain the min and max from the training set\n",
        "unscaled_train = pd.DataFrame(series_supervised[:len(train)])\n",
        "#new feature range\n",
        "new_max_value = 1 \n",
        "new_min_value= 0\n",
        "feature_range = new_max_value - new_min_value\n",
        "\n",
        "def transform_column(column):\n",
        "    min_value = min(unscaled_train.iloc[:, -steps_ahead + column.name])\n",
        "    max_value = max(unscaled_train.iloc[:, -steps_ahead + column.name])\n",
        "    return ((max_value - min_value) * (new_min_value + column) + (feature_range  * min_value)) / feature_range \n",
        "    \n",
        "# invert scaling for actual\n",
        "inv_scale_y = scaled_y.apply(transform_column, axis=0)\n",
        "inv_scale_y = inv_scale_y.values\n",
        "# invert scaling for forecast\n",
        "inv_scale_yhat = scaled_yhat.apply(transform_column, axis=0)\n",
        "inv_scale_yhat = inv_scale_yhat.values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(inv_scale_y, inv_scale_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXw77ALK8x8C",
        "outputId": "2eb5573c-904e-4397-ae63-a73b7e603705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.877456584883505"
            ]
          },
          "metadata": {},
          "execution_count": 384
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru = inv_scale_yhat"
      ],
      "metadata": {
        "id": "0hzq8rUh6_IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_inv_scale_yhat = inv_scale_yhat"
      ],
      "metadata": {
        "id": "S--9zsmU_nZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_inv_scale_yhat= lstm_inv_scale_yhat"
      ],
      "metadata": {
        "id": "fxIpLx2HNeTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tcn = inv_scale_yhat"
      ],
      "metadata": {
        "id": "srv4Fdis9naM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_inv_scale_yhat."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6uPe7lrNjeV",
        "outputId": "addad3ed-b19f-4e87-9c9b-ddc03ef9a8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(380, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_gru =np.mean([tcn, gru],axis=0)"
      ],
      "metadata": {
        "id": "GcYmRdyr7ERC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_gru =np.mean([tcn_inv_scale_yhat,gru],axis=0)"
      ],
      "metadata": {
        "id": "19ACVW_vAl5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(inv_scale_y, tcn_gru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDHm3jeJZHfW",
        "outputId": "ed7de502-5f70-47e2-ec7e-5073c09141c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9141430781964726"
            ]
          },
          "metadata": {},
          "execution_count": 377
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wMAPE(inv_scale_y,tcn_gru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikTWOm7c7Tsv",
        "outputId": "3238b096-6dcf-4e82-9656-1ae3ee9cbd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.50488926])"
            ]
          },
          "metadata": {},
          "execution_count": 378
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wMAPE(inv_scale_y,tcn_gru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylXW8-y-59ra",
        "outputId": "10c2d5b3-626d-4fcd-cced-b9138f22e591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.50488926])"
            ]
          },
          "metadata": {},
          "execution_count": 379
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wMAPE(inv_scale_y,lstm_inv_scale_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11pAopnwFIih",
        "outputId": "f30b8807-c1ea-4b9c-b499-20b0543ee29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.29508392])"
            ]
          },
          "metadata": {},
          "execution_count": 356
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.sqrt(mean_squared_error(inv_scale_y, tcn_lstm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-fOZxFL5n4P",
        "outputId": "162812b4-2c84-46df-b723-533daf65e6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130.94257446393243"
            ]
          },
          "metadata": {},
          "execution_count": 357
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.sqrt(mean_squared_error(inv_scale_y, tcn_lstm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6GD7BxhAOly",
        "outputId": "8cf2fcd8-dbbf-4ea5-b118-81a96d2f3074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130.94257446393243"
            ]
          },
          "metadata": {},
          "execution_count": 354
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tcn_inv_scale_yhat[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9gVPRbLBxXW",
        "outputId": "6fc3ff2f-a4e1-4196-fe91-8ba01f189a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3764.9487],\n",
              "       [3711.6826],\n",
              "       [3651.9321]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_scale_y[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqtMFT5Q_5_u",
        "outputId": "06ce7032-5337-4f77-b799-ee4f851c8fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3745.45641609],\n",
              "       [3718.22153619],\n",
              "       [3741.55673352]])"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_inv_scale_yhat[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHEI4RQo_3m2",
        "outputId": "aaab1e16-1232-445f-e348-cf12a21f7685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3729.4663],\n",
              "       [3689.3171],\n",
              "       [3653.2861]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(inv_scale_y, inv_scale_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEb0CJNETZt-",
        "outputId": "be109300-9982-427a-89a0-73cd557b020e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8807582168983916"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wMAPE(inv_scale_y, inv_scale_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf2YzQOW80Cm",
        "outputId": "b1714311-18b1-4073-b51c-15df415230fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6.28332583])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wMAPE(inv_scale_y, inv_scale_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nurcW0ExWKMh",
        "outputId": "f50473c3-8348-4f76-af34-f4908b2423ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6.83447497])"
            ]
          },
          "metadata": {},
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.sqrt(mean_squared_error(inv_scale_y[:,0], inv_scale_yhat[:,0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSZsNLSU82gn",
        "outputId": "ea8cb48a-aa73-49fb-8618-826a6b62f344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "223.33350402933982"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.sqrt(mean_squared_error(inv_scale_y[:,0], inv_scale_yhat[:,0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqsUebr1EW6w",
        "outputId": "ffcf122d-9bca-4021-e8bd-274dca90b6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "258.48293790148364"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqftbPYxvEE5"
      },
      "outputs": [],
      "source": [
        "# Invert the Differencing for actual \n",
        "df = pd.DataFrame(series.iloc[-len(test)-steps_ahead:,-1])\n",
        "n_vars = df.shape[1]\n",
        "columns = df.columns\n",
        "cols, names = list(), list()\n",
        "for i in range(0, steps_ahead):\n",
        "    cols.append(df.shift(-i))\n",
        "    if i == 0:\n",
        "        names += [('%s(t)' % (columns[j])) for j in range(n_vars)]\n",
        "    else:\n",
        "        names += [('%s(t+%d)' % (columns[j], i)) for j in range(n_vars)]\n",
        "# put it all together\n",
        "agg = pd.concat(cols, axis=1)\n",
        "agg.columns = names\n",
        "agg.dropna(inplace=True)\n",
        "agg = agg.iloc[:-1]\n",
        "#drop all the variables that we don't want to predict\n",
        "#agg.drop(columns=vars_to_drop, inplace=True)\n",
        "agg = agg.to_numpy()\n",
        "inv_y = np.add(inv_scale_y,agg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWAqc4iTkxvn"
      },
      "source": [
        "* To invert the differencing of time series for multistep prediction:<br/>\n",
        "The equation is given by $$\n",
        "\\hat x_{t+h|t}=x_t+(\\widehat{\\Delta x_{t+1}}+\\dots+\\widehat{\\Delta x_{t+h}}).\n",
        "$$ <br/>\n",
        "where: <br/>\n",
        "$\\hat x_{t+h|t}$ is the predicted value of the time series x at time $t$+h, given the value of the time series at time $t$.<br/>\n",
        "$x_t$ is the value of the time series $x$ at time t.<br/>\n",
        "${\\Delta x_{t+1}}$ is the difference between the value of the time series $x$ at time $t+1$ and the value of the time series at time t.<br/>\n",
        "${\\Delta x_{t+2}}$ is the difference between the value of the time series $x$ at time $t+2$ and the value of the time series at time $t+1$.<br/>\n",
        "${\\Delta x_{t+h}}$ is the difference between the value of the time series $x$ at time $t+h$ and the value of the time series at time $t+h-1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSJErK0WDkX4"
      },
      "outputs": [],
      "source": [
        "# Invert the Differencing for forecast\n",
        "# to invert the diffrenced predicted values,the the predicted differenced value is added\n",
        "# to previous predicted diffenced values and last available observation in test set(Xt) as explained above\n",
        "originalSeries_supervised = series_to_supervised(series, series.columns, n_in=window_size, n_out=steps_ahead, dropnan=True)\n",
        "current_timestep = 1\n",
        "originalSeries_xt = originalSeries_supervised[['BORE_OIL_VOL(t)']]\n",
        "# A predicted value at any given step ahead is a result of the previous cumulative differnced predicted values and current time step\n",
        "col = []\n",
        "inv_yhat_cum = np.cumsum(inv_scale_yhat, axis=1)\n",
        "\n",
        "for i in range(n_obs):\n",
        "    #.ravel() flattens the series into a one-dimensional array\n",
        "    inverted_diff_yhat = originalSeries_xt[-n_obs:].values[i] + inv_yhat_cum[i]\n",
        "    col.append(inverted_diff_yhat)\n",
        "col = pd.DataFrame(col)\n",
        "inv_yhat = col.values # convert df to NumpyArray"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "originalSeries_xt[-380:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "ln7s1EfolcgM",
        "outputId": "6baa3341-3139-49a4-a005-15435a6e2af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            BORE_OIL_VOL(t)\n",
              "DATEPRD                    \n",
              "2014-03-06      3858.232720\n",
              "2014-03-07      3745.456416\n",
              "2014-03-08      3718.221536\n",
              "2014-03-09      3741.556734\n",
              "2014-03-10      3364.042301\n",
              "...                     ...\n",
              "2015-03-16      1630.318908\n",
              "2015-03-17      1649.125441\n",
              "2015-03-18      1659.440731\n",
              "2015-03-19      1662.711432\n",
              "2015-03-20      1707.494884\n",
              "\n",
              "[380 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd44f8a2-addb-422f-92c6-643c6ce932d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BORE_OIL_VOL(t)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DATEPRD</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2014-03-06</th>\n",
              "      <td>3858.232720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-03-07</th>\n",
              "      <td>3745.456416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-03-08</th>\n",
              "      <td>3718.221536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-03-09</th>\n",
              "      <td>3741.556734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014-03-10</th>\n",
              "      <td>3364.042301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-16</th>\n",
              "      <td>1630.318908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-17</th>\n",
              "      <td>1649.125441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-18</th>\n",
              "      <td>1659.440731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-19</th>\n",
              "      <td>1662.711432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-03-20</th>\n",
              "      <td>1707.494884</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>380 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd44f8a2-addb-422f-92c6-643c6ce932d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd44f8a2-addb-422f-92c6-643c6ce932d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd44f8a2-addb-422f-92c6-643c6ce932d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(agg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hCbSWG5l2Gy-",
        "outputId": "bb309cb0-5376-414c-bc6e-284e5421bbc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0\n",
              "0    1\n",
              "1    1\n",
              "2    1\n",
              "3    1\n",
              "4    1\n",
              "..  ..\n",
              "375  1\n",
              "376  1\n",
              "377  1\n",
              "378  1\n",
              "379  1\n",
              "\n",
              "[380 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a85fc75a-0197-473f-92e5-b5cb7d287056\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>380 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a85fc75a-0197-473f-92e5-b5cb7d287056')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a85fc75a-0197-473f-92e5-b5cb7d287056 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a85fc75a-0197-473f-92e5-b5cb7d287056');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(inv_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "RhJxS0DT2Ptf",
        "outputId": "ea54eb36-801f-4cc3-df6f-57b9274c3408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               0\n",
              "0    3917.495415\n",
              "1    3764.820674\n",
              "2    3757.244974\n",
              "3    3763.211030\n",
              "4    3368.609684\n",
              "..           ...\n",
              "375  1501.568908\n",
              "376  1521.498488\n",
              "377  1530.981746\n",
              "378  1536.731940\n",
              "379  1581.629649\n",
              "\n",
              "[380 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08923488-ea37-4901-9421-2dba415fd2d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3917.495415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3764.820674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3757.244974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3763.211030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3368.609684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>1501.568908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>1521.498488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>1530.981746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>1536.731940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>1581.629649</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>380 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08923488-ea37-4901-9421-2dba415fd2d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-08923488-ea37-4901-9421-2dba415fd2d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-08923488-ea37-4901-9421-2dba415fd2d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_yhat[np.where(inv_y == 0)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd9swbmqf9f_",
        "outputId": "82f24563-65e1-40c2-9377-56e1bcefe5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([408.79837036, 303.49575806, 139.05915833, 143.31625366])"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.sqrt(mean_squared_error(originalSeries_xt[-380:].values, inv_yhat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsKsb0qwWlId",
        "outputId": "b6ac9833-323c-4306-854e-1135764e4046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480.2511567023128"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.sqrt(mean_squared_error(originalSeries_xt[-380:].values, inv_yhat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRKHoxIjgYvQ",
        "outputId": "76641371-52fa-4c5c-bf63-73ce585c9051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480.2511567023128"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wMAPE(inv_y, inv_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLjObLBVfz8X",
        "outputId": "5b522373-af04-4028-9b60-2badea48018f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6.18229087])"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_yhat[30:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kr_4VSO2iYO",
        "outputId": "d21e9114-a8e0-4e3c-f8bc-a27c02e80b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3452.50645563],\n",
              "       [3360.8467349 ],\n",
              "       [1697.79908635],\n",
              "       [2840.89462331],\n",
              "       [4211.87119429],\n",
              "       [3704.14514823],\n",
              "       [ 272.9634047 ],\n",
              "       [5336.72805161],\n",
              "       [3678.59843467],\n",
              "       [3600.82300678]])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_y[30:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVV-mCjQ2cyG",
        "outputId": "7aa9ba3a-ef0e-4da9-a51a-0c645b83bcc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3391.08848688],\n",
              "       [3391.46587552],\n",
              "       [2522.84303166],\n",
              "       [2692.22763112],\n",
              "       [3421.40537398],\n",
              "       [3608.02405448],\n",
              "       [1851.72024064],\n",
              "       [3682.6841063 ],\n",
              "       [3676.70878623],\n",
              "       [3630.41578021]])"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(inv_y, inv_yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9QW-H4qWt5w",
        "outputId": "006eb251-717a-462c-bb91-73579d4db458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9086773951224011"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "51SnY7n5e0nx",
        "outputId": "0678e5d8-7ee9-45e3-e8ac-1c8dcf4e43f1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-dc2c46888d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mresult_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_yhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresult_RMSPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_yhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mresult_MAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_yhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mresult_MAPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_yhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-32add8e09828>\u001b[0m in \u001b[0;36mRMSPE\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sqrt' is not defined"
          ]
        }
      ],
      "source": [
        "# Performance evaluation\n",
        "rmse_test, RMSPE_test, MAE_test, MAPE_test, r2_test, wMAPE_test, SMAPE_test  = [], [], [], [], [], [], []\n",
        "# calculate the score for each day\n",
        "\n",
        "for i in range(test_y.shape[1]):\n",
        "    result_rmse = math.sqrt(mean_squared_error(inv_y[:,i], inv_yhat[:,i]))\n",
        "    result_RMSPE = RMSPE(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_MAE = mean_absolute_error(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_MAPE = MAPE(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_r2 = r2_score(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_wMAPE = wMAPE(inv_y[:,i], inv_yhat[:,i])\n",
        "    result_SMAPE = SMAPE(inv_y[:,i], inv_yhat[:,i])\n",
        "\n",
        "    rmse_test.append(result_rmse)\n",
        "    RMSPE_test.append(result_RMSPE)\n",
        "    MAE_test.append(result_MAE)\n",
        "    MAPE_test.append(result_MAPE)\n",
        "    r2_test.append(result_r2)\n",
        "    wMAPE_test.append(result_wMAPE)\n",
        "    SMAPE_test.append(result_SMAPE)\n",
        "    \n",
        "## calculate overall score\n",
        "print(\"The Average scores for the vector output {} steps ahead:\\n\".format(steps_ahead))\n",
        "print('Test RMSE: %.5f' % np.mean(rmse_test))\n",
        "#print('Test RMSPE: %.5f' % np.mean(RMSPE_test)) because of that the denominator (actual) has some zero values\n",
        "print('Test MAE: %.5f' % np.mean(MAE_test))\n",
        "#print('Test MAPE: %.5f' % np.mean(MAPE_test)) #because of that the denominator (actual) has some zero values\n",
        "print('Test r2: %.5f' % np.mean(r2_test))\n",
        "print('Test wMAPE: %.5f ' % np.mean(wMAPE_test))\n",
        "print('Test SMAPE: %.5f ' % np.mean(SMAPE_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4KkEyXsJvDW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "outputId": "870b7b56-869a-4336-cab9-05d16b5c5a06"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1224x1080 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot the scores for each time step of the multi-step forecast\n",
        "scores = pd.DataFrame({\"rmse_test\":rmse_test, \"MAE_test\":MAE_test, \"R-squared_test\":r2_test, \"wMAPE_test\":wMAPE_test, \"SMAPE_test\":SMAPE_test})\n",
        "\n",
        "# Reset the index, keeping the old index as a column\n",
        "scores = scores.reset_index(drop=False)\n",
        "\n",
        "# Set the 'index' column as the new index\n",
        "scores.index = scores['index'] + 1\n",
        "\n",
        "# Drop the old 'index' column\n",
        "scores = scores.drop(columns='index')\n",
        "\n",
        "data = scores.columns\n",
        "\n",
        "# Creating figure with two rows and one column\n",
        "fig, axs = plt.subplots(nrows=len(data), figsize=(17, 15))\n",
        "\n",
        "axs = axs.ravel()\n",
        "\n",
        "for id, column in enumerate(data):\n",
        "    # Set the x-axis limits\n",
        "    #axs[id].set_xlim(xmin=1, xmax= steps_ahead)\n",
        "    #print the name of the test on plot\n",
        "    axs[id].plot(scores[column])\n",
        "    # Add a title to the x-axis\n",
        "    axs[id].set_xlabel('Steps ahead',fontsize=10, labelpad=0.1)\n",
        "    axs[id].grid(True)\n",
        "    # Remove the horizontal grid lines\n",
        "    axs[id].grid(which='both', axis='y')\n",
        "    axs[id].xaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "    axs[id].legend([column], loc='upper left', fontsize=15, handlelength=0, handletextpad=0, frameon=False)\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doFyZoCCZ-ht",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "7e9c5fc3-c49f-4bb3-c64c-ab373a761259"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot the last forecasted values on test set\n",
        "fig, ax = plt.subplots(1, 1, figsize=(18,6)) \n",
        "# Set the major locator for the x-axis\n",
        "x = list(range(1, len(inv_yhat[-1])+1))\n",
        "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "ax.plot(x, inv_yhat[-1], label = \"predicted\")\n",
        "ax.plot(x, inv_y[-1], label = \"actual\")\n",
        "ax.set_ylabel('Oil Rate', fontsize=15)\n",
        "ax.set_xlabel('Steps Ahead (Days)',fontsize=15)\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1PcMnvDfCdcQlg3ikKWSjWFO9GT6HGhdO",
      "authorship_tag": "ABX9TyPqqpOcYhbBT/2fyUjpbrYF",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}